<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<title>[1301.3781] Efficient Estimation of Word Representations in Vector Space</title><meta property="og:description" content="We propose two novel model architectures for computing continuous vector representations of words from very large data sets.
The quality of these representations is measured in a word similarity task, and the results aâ€¦">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Efficient Estimation of Word Representations in Vector Space">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="Efficient Estimation of Word Representations in Vector Space">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/1301.3781">

<!--Generated on Fri Jan 28 20:46:44 2022 by LaTeXML (version 0.8.6) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }
  
  detectColorScheme();
  
  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv.0.7.4.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.0.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">Efficient Estimation of Word Representations in Vector Space</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Tomas Mikolov 
<br class="ltx_break">Google Inc.,
Mountain View, CA 
<br class="ltx_break"><span id="id1.1.id1" class="ltx_text ltx_font_typewriter">tmikolov@google.com</span> 
<br class="ltx_break"><span id="id2.2.id2" class="ltx_ERROR undefined">\And</span>Kai Chen 
<br class="ltx_break">Google Inc.,
Mountain View, CA 
<br class="ltx_break"><span id="id3.3.id3" class="ltx_text ltx_font_typewriter">kaichen@google.com</span> 
<br class="ltx_break"><span id="id4.4.id4" class="ltx_ERROR undefined">\AND</span>Greg Corrado 
<br class="ltx_break">Google Inc.,
Mountain View, CA 
<br class="ltx_break"><span id="id5.5.id5" class="ltx_text ltx_font_typewriter">gcorrado@google.com</span> 
<br class="ltx_break"><span id="id6.6.id6" class="ltx_ERROR undefined">\And</span>Jeffrey Dean 
<br class="ltx_break">Google Inc.,
Mountain View, CA 
<br class="ltx_break"><span id="id7.7.id7" class="ltx_text ltx_font_typewriter">jeff@google.com</span> 
<br class="ltx_break">
</span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id8.id1" class="ltx_p">We propose two novel model architectures for computing continuous vector representations of words from very large data sets.
The quality of these representations is measured in a word similarity task, and the results are compared to the previously best performing techniques based on different types of neural networks.
We observe large improvements in accuracy at much lower computational cost, i.e. it takes less than a day to learn high quality word vectors
from a 1.6 billion words data set. Furthermore, we show that these
vectors provide state-of-the-art performance on our test set for measuring syntactic and semantic word similarities.</p>
</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>

<div id="S1.p1" class="ltx_para ltx_noindent">
<p id="S1.p1.1" class="ltx_p">Many current NLP systems and techniques treat words as atomic units - there is no notion of similarity between words, as these are represented as indices in a vocabulary.
This choice has several good reasons - simplicity, robustness and the observation that simple models trained on huge amounts of data
outperform complex systems trained on less data. An example is the popular N-gram model used for statistical language modeling - today, it is possible to
train N-grams on virtually all available data (trillions of wordsÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite>).</p>
</div>
<div id="S1.p2" class="ltx_para ltx_noindent">
<p id="S1.p2.1" class="ltx_p">However, the simple techniques are at their limits in many tasks. For example, the amount of relevant in-domain data for automatic speech recognition is limited - the performance is usually dominated by
the size of high quality transcribed speech data (often just millions of words). In machine translation, the existing corpora for many languages contain only a few billions of words or less.
Thus, there are situations where simple scaling up of the basic techniques will not result in any significant progress, and we have to focus on more advanced techniques.</p>
</div>
<div id="S1.p3" class="ltx_para ltx_noindent">
<p id="S1.p3.1" class="ltx_p">With progress of machine learning techniques in recent years, it has become possible to train more complex models on much larger data set, and they typically outperform the simple models.
Probably the most successful concept is to use distributed representations of wordsÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite>.
For example, neural network based language models significantly outperform N-gram modelsÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>, <a href="#bib.bib27" title="" class="ltx_ref">27</a>, <a href="#bib.bib17" title="" class="ltx_ref">17</a>]</cite>.</p>
</div>
<section id="S1.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">1.1 </span>Goals of the Paper</h3>

<div id="S1.SS1.p1" class="ltx_para ltx_noindent">
<p id="S1.SS1.p1.1" class="ltx_p">The main goal of this paper is to introduce techniques that can be used for learning high-quality word vectors from huge data sets with billions of words, and with millions of words in the vocabulary.
As far as we know, none of the previously proposed architectures has been successfully trained on more than a few hundred of millions of words, with a modest dimensionality of the word vectors between 50 - 100.</p>
</div>
<div id="S1.SS1.p2" class="ltx_para ltx_noindent">
<p id="S1.SS1.p2.1" class="ltx_p">We use recently proposed techniques for measuring the quality of the resulting vector representations, with the expectation that not only
will similar words tend to be close to each other, but that words can have <span id="S1.SS1.p2.1.1" class="ltx_text ltx_font_bold">multiple degrees of similarity</span>Â <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib20" title="" class="ltx_ref">20</a>]</cite>. This has been observed earlier in the context of inflectional
languages - for example, nouns can have multiple word endings, and if we search for similar words in a subspace of the original vector space, it is possible to find words that have similar endingsÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>, <a href="#bib.bib14" title="" class="ltx_ref">14</a>]</cite>.</p>
</div>
<div id="S1.SS1.p3" class="ltx_para ltx_noindent">
<p id="S1.SS1.p3.1" class="ltx_p">Somewhat surprisingly, it was found that similarity of word representations goes beyond simple syntactic regularities. Using a word offset technique where simple algebraic operations are performed on the
word vectors, it was shown for example that <span id="S1.SS1.p3.1.1" class="ltx_text ltx_font_italic">vector(â€Kingâ€) - vector(â€Manâ€) + vector(â€Womanâ€)</span> results in a vector that is closest to the vector representation of the word <span id="S1.SS1.p3.1.2" class="ltx_text ltx_font_italic">Queen</span>Â <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib20" title="" class="ltx_ref">20</a>]</cite>.</p>
</div>
<div id="S1.SS1.p4" class="ltx_para ltx_noindent">
<p id="S1.SS1.p4.1" class="ltx_p">In this paper, we try to maximize accuracy of these vector operations by
developing new model architectures that preserve the linear regularities among words. We design a new comprehensive test set for measuring both
syntactic and semantic regularities<span id="footnote1" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span>The test set is available at <a href="www.fit.vutbr.cz/~imikolov/rnnlm/word-test.v1.txt" title="" class="ltx_ref ltx_url ltx_font_typewriter">www.fit.vutbr.cz/~imikolov/rnnlm/word-test.v1.txt</a></span></span></span>,
and show that many such regularities can be learned
with high accuracy. Moreover, we discuss how training time and accuracy depends on the dimensionality of the word vectors and on the amount of the training data.</p>
</div>
</section>
<section id="S1.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">1.2 </span>Previous Work</h3>

<div id="S1.SS2.p1" class="ltx_para ltx_noindent">
<p id="S1.SS2.p1.1" class="ltx_p">Representation of words as continuous vectors has a long historyÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>, <a href="#bib.bib26" title="" class="ltx_ref">26</a>, <a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite>. A very popular model architecture for estimating neural network language model (NNLM) was proposed inÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite>,
where a feedforward neural network with a linear projection layer and a non-linear hidden layer was used to learn jointly the word vector representation and a statistical language model. This work has been
followed by many others.</p>
</div>
<div id="S1.SS2.p2" class="ltx_para ltx_noindent">
<p id="S1.SS2.p2.1" class="ltx_p">Another interesting architecture of NNLM was presented inÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>, <a href="#bib.bib14" title="" class="ltx_ref">14</a>]</cite>, where the word vectors are first learned using neural network with a single hidden layer. The
word vectors are then used to train the NNLM. Thus, the word vectors are learned even without constructing the full NNLM.
In this work, we directly extend this architecture, and focus just on the first step where the word vectors are learned using a simple model.</p>
</div>
<div id="S1.SS2.p3" class="ltx_para ltx_noindent">
<p id="S1.SS2.p3.1" class="ltx_p">It was later shown that the word vectors can be used to significantly improve and simplify many NLP applicationsÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>, <a href="#bib.bib5" title="" class="ltx_ref">5</a>, <a href="#bib.bib29" title="" class="ltx_ref">29</a>]</cite>. Estimation of the word vectors itself was
performed using different model architectures and trained on various corporaÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>, <a href="#bib.bib29" title="" class="ltx_ref">29</a>, <a href="#bib.bib23" title="" class="ltx_ref">23</a>, <a href="#bib.bib19" title="" class="ltx_ref">19</a>, <a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite>, and some of the resulting word vectors were made
available for future research and comparison<span id="footnote2" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_tag ltx_tag_note">2</span><a target="_blank" href="http://ronan.collobert.com/senna/" title="" class="ltx_ref ltx_url ltx_font_typewriter">http://ronan.collobert.com/senna/</a> 
<br class="ltx_break"><a target="_blank" href="http://metaoptimize.com/projects/wordreprs/" title="" class="ltx_ref ltx_url ltx_font_typewriter">http://metaoptimize.com/projects/wordreprs/</a> 
<br class="ltx_break"><a target="_blank" href="http://www.fit.vutbr.cz/~imikolov/rnnlm/" title="" class="ltx_ref ltx_url ltx_font_typewriter">http://www.fit.vutbr.cz/~imikolov/rnnlm/</a> 
<br class="ltx_break"><a target="_blank" href="http://ai.stanford.edu/~ehhuang/" title="" class="ltx_ref ltx_url ltx_font_typewriter">http://ai.stanford.edu/~ehhuang/</a> </span></span></span>.
However, as far as we know, these architectures were significantly more computationally expensive for training
than the one proposed inÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite>, with the exception of certain version of log-bilinear model where diagonal weight matrices are usedÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib23" title="" class="ltx_ref">23</a>]</cite>.</p>
</div>
</section>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Model Architectures</h2>

<div id="S2.p1" class="ltx_para ltx_noindent">
<p id="S2.p1.1" class="ltx_p">Many different types of models were proposed for estimating continuous representations of words, including the well-known Latent Semantic Analysis (LSA) and Latent Dirichlet Allocation (LDA). In this paper, we focus
on distributed representations of words learned by neural networks, as it was previously shown that they perform significantly better than LSA for preserving linear regularities among wordsÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib20" title="" class="ltx_ref">20</a>, <a href="#bib.bib31" title="" class="ltx_ref">31</a>]</cite>; LDA moreover becomes computationally
very expensive on large data sets.</p>
</div>
<div id="S2.p2" class="ltx_para ltx_noindent">
<p id="S2.p2.1" class="ltx_p">Similar toÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref">18</a>]</cite>, to compare different model architectures we define first the computational complexity of a model as the number of parameters that need to be accessed to fully train the model. Next, we will
try to maximize the accuracy, while minimizing the computational complexity.</p>
</div>
<div id="S2.p3" class="ltx_para ltx_noindent">
<p id="S2.p3.6" class="ltx_p">For all the following models, the training complexity is proportional to</p>
<table id="S2.E1" class="ltx_equation ltx_eqn_table">

<tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S2.E1.m1.1" class="ltx_Math" alttext="O=E\times T\times Q," display="block"><semantics id="S2.E1.m1.1a"><mrow id="S2.E1.m1.1.1.1" xref="S2.E1.m1.1.1.1.1.cmml"><mrow id="S2.E1.m1.1.1.1.1" xref="S2.E1.m1.1.1.1.1.cmml"><mi id="S2.E1.m1.1.1.1.1.2" xref="S2.E1.m1.1.1.1.1.2.cmml">O</mi><mo id="S2.E1.m1.1.1.1.1.1" xref="S2.E1.m1.1.1.1.1.1.cmml">=</mo><mrow id="S2.E1.m1.1.1.1.1.3" xref="S2.E1.m1.1.1.1.1.3.cmml"><mi id="S2.E1.m1.1.1.1.1.3.2" xref="S2.E1.m1.1.1.1.1.3.2.cmml">E</mi><mo id="S2.E1.m1.1.1.1.1.3.1" xref="S2.E1.m1.1.1.1.1.3.1.cmml">Ã—</mo><mi id="S2.E1.m1.1.1.1.1.3.3" xref="S2.E1.m1.1.1.1.1.3.3.cmml">T</mi><mo id="S2.E1.m1.1.1.1.1.3.1a" xref="S2.E1.m1.1.1.1.1.3.1.cmml">Ã—</mo><mi id="S2.E1.m1.1.1.1.1.3.4" xref="S2.E1.m1.1.1.1.1.3.4.cmml">Q</mi></mrow></mrow><mo id="S2.E1.m1.1.1.1.2" xref="S2.E1.m1.1.1.1.1.cmml">,</mo></mrow><annotation-xml encoding="MathML-Content" id="S2.E1.m1.1b"><apply id="S2.E1.m1.1.1.1.1.cmml" xref="S2.E1.m1.1.1.1"><eq id="S2.E1.m1.1.1.1.1.1.cmml" xref="S2.E1.m1.1.1.1.1.1"></eq><ci id="S2.E1.m1.1.1.1.1.2.cmml" xref="S2.E1.m1.1.1.1.1.2">ğ‘‚</ci><apply id="S2.E1.m1.1.1.1.1.3.cmml" xref="S2.E1.m1.1.1.1.1.3"><times id="S2.E1.m1.1.1.1.1.3.1.cmml" xref="S2.E1.m1.1.1.1.1.3.1"></times><ci id="S2.E1.m1.1.1.1.1.3.2.cmml" xref="S2.E1.m1.1.1.1.1.3.2">ğ¸</ci><ci id="S2.E1.m1.1.1.1.1.3.3.cmml" xref="S2.E1.m1.1.1.1.1.3.3">ğ‘‡</ci><ci id="S2.E1.m1.1.1.1.1.3.4.cmml" xref="S2.E1.m1.1.1.1.1.3.4">ğ‘„</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.E1.m1.1c">O=E\times T\times Q,</annotation><annotation encoding="application/x-llamapun" id="S2.E1.m1.1d">italic_O = italic_E Ã— italic_T Ã— italic_Q ,</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(1)</span></td>
</tr>
</table>
<p id="S2.p3.5" class="ltx_p">where <math id="S2.p3.1.m1.1" class="ltx_Math" alttext="E" display="inline"><semantics id="S2.p3.1.m1.1a"><mi id="S2.p3.1.m1.1.1" xref="S2.p3.1.m1.1.1.cmml">E</mi><annotation-xml encoding="MathML-Content" id="S2.p3.1.m1.1b"><ci id="S2.p3.1.m1.1.1.cmml" xref="S2.p3.1.m1.1.1">ğ¸</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.p3.1.m1.1c">E</annotation><annotation encoding="application/x-llamapun" id="S2.p3.1.m1.1d">italic_E</annotation></semantics></math> is number of the training epochs, <math id="S2.p3.2.m2.1" class="ltx_Math" alttext="T" display="inline"><semantics id="S2.p3.2.m2.1a"><mi id="S2.p3.2.m2.1.1" xref="S2.p3.2.m2.1.1.cmml">T</mi><annotation-xml encoding="MathML-Content" id="S2.p3.2.m2.1b"><ci id="S2.p3.2.m2.1.1.cmml" xref="S2.p3.2.m2.1.1">ğ‘‡</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.p3.2.m2.1c">T</annotation><annotation encoding="application/x-llamapun" id="S2.p3.2.m2.1d">italic_T</annotation></semantics></math> is the number of the words in the training set and <math id="S2.p3.3.m3.1" class="ltx_Math" alttext="Q" display="inline"><semantics id="S2.p3.3.m3.1a"><mi id="S2.p3.3.m3.1.1" xref="S2.p3.3.m3.1.1.cmml">Q</mi><annotation-xml encoding="MathML-Content" id="S2.p3.3.m3.1b"><ci id="S2.p3.3.m3.1.1.cmml" xref="S2.p3.3.m3.1.1">ğ‘„</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.p3.3.m3.1c">Q</annotation><annotation encoding="application/x-llamapun" id="S2.p3.3.m3.1d">italic_Q</annotation></semantics></math> is defined further for each model architecture. Common choice is <math id="S2.p3.4.m4.1" class="ltx_Math" alttext="E=3-50" display="inline"><semantics id="S2.p3.4.m4.1a"><mrow id="S2.p3.4.m4.1.1" xref="S2.p3.4.m4.1.1.cmml"><mi id="S2.p3.4.m4.1.1.2" xref="S2.p3.4.m4.1.1.2.cmml">E</mi><mo id="S2.p3.4.m4.1.1.1" xref="S2.p3.4.m4.1.1.1.cmml">=</mo><mrow id="S2.p3.4.m4.1.1.3" xref="S2.p3.4.m4.1.1.3.cmml"><mn id="S2.p3.4.m4.1.1.3.2" xref="S2.p3.4.m4.1.1.3.2.cmml">3</mn><mo id="S2.p3.4.m4.1.1.3.1" xref="S2.p3.4.m4.1.1.3.1.cmml">-</mo><mn id="S2.p3.4.m4.1.1.3.3" xref="S2.p3.4.m4.1.1.3.3.cmml">50</mn></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.p3.4.m4.1b"><apply id="S2.p3.4.m4.1.1.cmml" xref="S2.p3.4.m4.1.1"><eq id="S2.p3.4.m4.1.1.1.cmml" xref="S2.p3.4.m4.1.1.1"></eq><ci id="S2.p3.4.m4.1.1.2.cmml" xref="S2.p3.4.m4.1.1.2">ğ¸</ci><apply id="S2.p3.4.m4.1.1.3.cmml" xref="S2.p3.4.m4.1.1.3"><minus id="S2.p3.4.m4.1.1.3.1.cmml" xref="S2.p3.4.m4.1.1.3.1"></minus><cn type="integer" id="S2.p3.4.m4.1.1.3.2.cmml" xref="S2.p3.4.m4.1.1.3.2">3</cn><cn type="integer" id="S2.p3.4.m4.1.1.3.3.cmml" xref="S2.p3.4.m4.1.1.3.3">50</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.p3.4.m4.1c">E=3-50</annotation><annotation encoding="application/x-llamapun" id="S2.p3.4.m4.1d">italic_E = 3 - 50</annotation></semantics></math> and <math id="S2.p3.5.m5.1" class="ltx_Math" alttext="T" display="inline"><semantics id="S2.p3.5.m5.1a"><mi id="S2.p3.5.m5.1.1" xref="S2.p3.5.m5.1.1.cmml">T</mi><annotation-xml encoding="MathML-Content" id="S2.p3.5.m5.1b"><ci id="S2.p3.5.m5.1.1.cmml" xref="S2.p3.5.m5.1.1">ğ‘‡</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.p3.5.m5.1c">T</annotation><annotation encoding="application/x-llamapun" id="S2.p3.5.m5.1d">italic_T</annotation></semantics></math> up to one billion.
All models are trained using stochastic gradient descent and backpropagationÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib26" title="" class="ltx_ref">26</a>]</cite>.</p>
</div>
<section id="S2.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.1 </span>Feedforward Neural Net Language Model (NNLM)</h3>

<div id="S2.SS1.p1" class="ltx_para ltx_noindent">
<p id="S2.SS1.p1.6" class="ltx_p">The probabilistic feedforward neural network language model has been proposed inÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite>. It consists of input, projection, hidden and output layers. At the input layer, <math id="S2.SS1.p1.1.m1.1" class="ltx_Math" alttext="N" display="inline"><semantics id="S2.SS1.p1.1.m1.1a"><mi id="S2.SS1.p1.1.m1.1.1" xref="S2.SS1.p1.1.m1.1.1.cmml">N</mi><annotation-xml encoding="MathML-Content" id="S2.SS1.p1.1.m1.1b"><ci id="S2.SS1.p1.1.m1.1.1.cmml" xref="S2.SS1.p1.1.m1.1.1">ğ‘</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p1.1.m1.1c">N</annotation><annotation encoding="application/x-llamapun" id="S2.SS1.p1.1.m1.1d">italic_N</annotation></semantics></math> previous words are encoded
using 1-of-<math id="S2.SS1.p1.2.m2.1" class="ltx_Math" alttext="V" display="inline"><semantics id="S2.SS1.p1.2.m2.1a"><mi id="S2.SS1.p1.2.m2.1.1" xref="S2.SS1.p1.2.m2.1.1.cmml">V</mi><annotation-xml encoding="MathML-Content" id="S2.SS1.p1.2.m2.1b"><ci id="S2.SS1.p1.2.m2.1.1.cmml" xref="S2.SS1.p1.2.m2.1.1">ğ‘‰</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p1.2.m2.1c">V</annotation><annotation encoding="application/x-llamapun" id="S2.SS1.p1.2.m2.1d">italic_V</annotation></semantics></math> coding, where <math id="S2.SS1.p1.3.m3.1" class="ltx_Math" alttext="V" display="inline"><semantics id="S2.SS1.p1.3.m3.1a"><mi id="S2.SS1.p1.3.m3.1.1" xref="S2.SS1.p1.3.m3.1.1.cmml">V</mi><annotation-xml encoding="MathML-Content" id="S2.SS1.p1.3.m3.1b"><ci id="S2.SS1.p1.3.m3.1.1.cmml" xref="S2.SS1.p1.3.m3.1.1">ğ‘‰</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p1.3.m3.1c">V</annotation><annotation encoding="application/x-llamapun" id="S2.SS1.p1.3.m3.1d">italic_V</annotation></semantics></math> is size of the vocabulary. The input layer is then projected to a projection layer <math id="S2.SS1.p1.4.m4.1" class="ltx_Math" alttext="P" display="inline"><semantics id="S2.SS1.p1.4.m4.1a"><mi id="S2.SS1.p1.4.m4.1.1" xref="S2.SS1.p1.4.m4.1.1.cmml">P</mi><annotation-xml encoding="MathML-Content" id="S2.SS1.p1.4.m4.1b"><ci id="S2.SS1.p1.4.m4.1.1.cmml" xref="S2.SS1.p1.4.m4.1.1">ğ‘ƒ</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p1.4.m4.1c">P</annotation><annotation encoding="application/x-llamapun" id="S2.SS1.p1.4.m4.1d">italic_P</annotation></semantics></math> that has dimensionality <math id="S2.SS1.p1.5.m5.1" class="ltx_Math" alttext="N\times D" display="inline"><semantics id="S2.SS1.p1.5.m5.1a"><mrow id="S2.SS1.p1.5.m5.1.1" xref="S2.SS1.p1.5.m5.1.1.cmml"><mi id="S2.SS1.p1.5.m5.1.1.2" xref="S2.SS1.p1.5.m5.1.1.2.cmml">N</mi><mo id="S2.SS1.p1.5.m5.1.1.1" xref="S2.SS1.p1.5.m5.1.1.1.cmml">Ã—</mo><mi id="S2.SS1.p1.5.m5.1.1.3" xref="S2.SS1.p1.5.m5.1.1.3.cmml">D</mi></mrow><annotation-xml encoding="MathML-Content" id="S2.SS1.p1.5.m5.1b"><apply id="S2.SS1.p1.5.m5.1.1.cmml" xref="S2.SS1.p1.5.m5.1.1"><times id="S2.SS1.p1.5.m5.1.1.1.cmml" xref="S2.SS1.p1.5.m5.1.1.1"></times><ci id="S2.SS1.p1.5.m5.1.1.2.cmml" xref="S2.SS1.p1.5.m5.1.1.2">ğ‘</ci><ci id="S2.SS1.p1.5.m5.1.1.3.cmml" xref="S2.SS1.p1.5.m5.1.1.3">ğ·</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p1.5.m5.1c">N\times D</annotation><annotation encoding="application/x-llamapun" id="S2.SS1.p1.5.m5.1d">italic_N Ã— italic_D</annotation></semantics></math>, using a shared projection matrix. As only <math id="S2.SS1.p1.6.m6.1" class="ltx_Math" alttext="N" display="inline"><semantics id="S2.SS1.p1.6.m6.1a"><mi id="S2.SS1.p1.6.m6.1.1" xref="S2.SS1.p1.6.m6.1.1.cmml">N</mi><annotation-xml encoding="MathML-Content" id="S2.SS1.p1.6.m6.1b"><ci id="S2.SS1.p1.6.m6.1.1.cmml" xref="S2.SS1.p1.6.m6.1.1">ğ‘</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p1.6.m6.1c">N</annotation><annotation encoding="application/x-llamapun" id="S2.SS1.p1.6.m6.1d">italic_N</annotation></semantics></math> inputs are
active at any given time, composition of the projection layer is a relatively cheap operation.</p>
</div>
<div id="S2.SS1.p2" class="ltx_para ltx_noindent">
<p id="S2.SS1.p2.4" class="ltx_p">The NNLM architecture becomes complex for computation between the projection and the hidden layer, as values in the projection layer are dense.
For a common choice of <math id="S2.SS1.p2.1.m1.1" class="ltx_Math" alttext="N=10" display="inline"><semantics id="S2.SS1.p2.1.m1.1a"><mrow id="S2.SS1.p2.1.m1.1.1" xref="S2.SS1.p2.1.m1.1.1.cmml"><mi id="S2.SS1.p2.1.m1.1.1.2" xref="S2.SS1.p2.1.m1.1.1.2.cmml">N</mi><mo id="S2.SS1.p2.1.m1.1.1.1" xref="S2.SS1.p2.1.m1.1.1.1.cmml">=</mo><mn id="S2.SS1.p2.1.m1.1.1.3" xref="S2.SS1.p2.1.m1.1.1.3.cmml">10</mn></mrow><annotation-xml encoding="MathML-Content" id="S2.SS1.p2.1.m1.1b"><apply id="S2.SS1.p2.1.m1.1.1.cmml" xref="S2.SS1.p2.1.m1.1.1"><eq id="S2.SS1.p2.1.m1.1.1.1.cmml" xref="S2.SS1.p2.1.m1.1.1.1"></eq><ci id="S2.SS1.p2.1.m1.1.1.2.cmml" xref="S2.SS1.p2.1.m1.1.1.2">ğ‘</ci><cn type="integer" id="S2.SS1.p2.1.m1.1.1.3.cmml" xref="S2.SS1.p2.1.m1.1.1.3">10</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p2.1.m1.1c">N=10</annotation><annotation encoding="application/x-llamapun" id="S2.SS1.p2.1.m1.1d">italic_N = 10</annotation></semantics></math>, the size of the projection layer (<math id="S2.SS1.p2.2.m2.1" class="ltx_Math" alttext="P" display="inline"><semantics id="S2.SS1.p2.2.m2.1a"><mi id="S2.SS1.p2.2.m2.1.1" xref="S2.SS1.p2.2.m2.1.1.cmml">P</mi><annotation-xml encoding="MathML-Content" id="S2.SS1.p2.2.m2.1b"><ci id="S2.SS1.p2.2.m2.1.1.cmml" xref="S2.SS1.p2.2.m2.1.1">ğ‘ƒ</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p2.2.m2.1c">P</annotation><annotation encoding="application/x-llamapun" id="S2.SS1.p2.2.m2.1d">italic_P</annotation></semantics></math>) might be 500 to 2000, while the hidden layer size <math id="S2.SS1.p2.3.m3.1" class="ltx_Math" alttext="H" display="inline"><semantics id="S2.SS1.p2.3.m3.1a"><mi id="S2.SS1.p2.3.m3.1.1" xref="S2.SS1.p2.3.m3.1.1.cmml">H</mi><annotation-xml encoding="MathML-Content" id="S2.SS1.p2.3.m3.1b"><ci id="S2.SS1.p2.3.m3.1.1.cmml" xref="S2.SS1.p2.3.m3.1.1">ğ»</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p2.3.m3.1c">H</annotation><annotation encoding="application/x-llamapun" id="S2.SS1.p2.3.m3.1d">italic_H</annotation></semantics></math> is typically 500 to 1000 units. Moreover, the hidden layer is used to compute probability
distribution over all the words in the vocabulary, resulting in an output layer with dimensionality <math id="S2.SS1.p2.4.m4.1" class="ltx_Math" alttext="V" display="inline"><semantics id="S2.SS1.p2.4.m4.1a"><mi id="S2.SS1.p2.4.m4.1.1" xref="S2.SS1.p2.4.m4.1.1.cmml">V</mi><annotation-xml encoding="MathML-Content" id="S2.SS1.p2.4.m4.1b"><ci id="S2.SS1.p2.4.m4.1.1.cmml" xref="S2.SS1.p2.4.m4.1.1">ğ‘‰</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p2.4.m4.1c">V</annotation><annotation encoding="application/x-llamapun" id="S2.SS1.p2.4.m4.1d">italic_V</annotation></semantics></math>. Thus, the computational complexity per each training example is</p>
<table id="S2.E2" class="ltx_equation ltx_eqn_table">

<tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S2.E2.m1.1" class="ltx_Math" alttext="Q=N\times D+N\times D\times H+H\times V," display="block"><semantics id="S2.E2.m1.1a"><mrow id="S2.E2.m1.1.1.1" xref="S2.E2.m1.1.1.1.1.cmml"><mrow id="S2.E2.m1.1.1.1.1" xref="S2.E2.m1.1.1.1.1.cmml"><mi id="S2.E2.m1.1.1.1.1.2" xref="S2.E2.m1.1.1.1.1.2.cmml">Q</mi><mo id="S2.E2.m1.1.1.1.1.1" xref="S2.E2.m1.1.1.1.1.1.cmml">=</mo><mrow id="S2.E2.m1.1.1.1.1.3" xref="S2.E2.m1.1.1.1.1.3.cmml"><mrow id="S2.E2.m1.1.1.1.1.3.2" xref="S2.E2.m1.1.1.1.1.3.2.cmml"><mi id="S2.E2.m1.1.1.1.1.3.2.2" xref="S2.E2.m1.1.1.1.1.3.2.2.cmml">N</mi><mo id="S2.E2.m1.1.1.1.1.3.2.1" xref="S2.E2.m1.1.1.1.1.3.2.1.cmml">Ã—</mo><mi id="S2.E2.m1.1.1.1.1.3.2.3" xref="S2.E2.m1.1.1.1.1.3.2.3.cmml">D</mi></mrow><mo id="S2.E2.m1.1.1.1.1.3.1" xref="S2.E2.m1.1.1.1.1.3.1.cmml">+</mo><mrow id="S2.E2.m1.1.1.1.1.3.3" xref="S2.E2.m1.1.1.1.1.3.3.cmml"><mi id="S2.E2.m1.1.1.1.1.3.3.2" xref="S2.E2.m1.1.1.1.1.3.3.2.cmml">N</mi><mo id="S2.E2.m1.1.1.1.1.3.3.1" xref="S2.E2.m1.1.1.1.1.3.3.1.cmml">Ã—</mo><mi id="S2.E2.m1.1.1.1.1.3.3.3" xref="S2.E2.m1.1.1.1.1.3.3.3.cmml">D</mi><mo id="S2.E2.m1.1.1.1.1.3.3.1a" xref="S2.E2.m1.1.1.1.1.3.3.1.cmml">Ã—</mo><mi id="S2.E2.m1.1.1.1.1.3.3.4" xref="S2.E2.m1.1.1.1.1.3.3.4.cmml">H</mi></mrow><mo id="S2.E2.m1.1.1.1.1.3.1a" xref="S2.E2.m1.1.1.1.1.3.1.cmml">+</mo><mrow id="S2.E2.m1.1.1.1.1.3.4" xref="S2.E2.m1.1.1.1.1.3.4.cmml"><mi id="S2.E2.m1.1.1.1.1.3.4.2" xref="S2.E2.m1.1.1.1.1.3.4.2.cmml">H</mi><mo id="S2.E2.m1.1.1.1.1.3.4.1" xref="S2.E2.m1.1.1.1.1.3.4.1.cmml">Ã—</mo><mi id="S2.E2.m1.1.1.1.1.3.4.3" xref="S2.E2.m1.1.1.1.1.3.4.3.cmml">V</mi></mrow></mrow></mrow><mo id="S2.E2.m1.1.1.1.2" xref="S2.E2.m1.1.1.1.1.cmml">,</mo></mrow><annotation-xml encoding="MathML-Content" id="S2.E2.m1.1b"><apply id="S2.E2.m1.1.1.1.1.cmml" xref="S2.E2.m1.1.1.1"><eq id="S2.E2.m1.1.1.1.1.1.cmml" xref="S2.E2.m1.1.1.1.1.1"></eq><ci id="S2.E2.m1.1.1.1.1.2.cmml" xref="S2.E2.m1.1.1.1.1.2">ğ‘„</ci><apply id="S2.E2.m1.1.1.1.1.3.cmml" xref="S2.E2.m1.1.1.1.1.3"><plus id="S2.E2.m1.1.1.1.1.3.1.cmml" xref="S2.E2.m1.1.1.1.1.3.1"></plus><apply id="S2.E2.m1.1.1.1.1.3.2.cmml" xref="S2.E2.m1.1.1.1.1.3.2"><times id="S2.E2.m1.1.1.1.1.3.2.1.cmml" xref="S2.E2.m1.1.1.1.1.3.2.1"></times><ci id="S2.E2.m1.1.1.1.1.3.2.2.cmml" xref="S2.E2.m1.1.1.1.1.3.2.2">ğ‘</ci><ci id="S2.E2.m1.1.1.1.1.3.2.3.cmml" xref="S2.E2.m1.1.1.1.1.3.2.3">ğ·</ci></apply><apply id="S2.E2.m1.1.1.1.1.3.3.cmml" xref="S2.E2.m1.1.1.1.1.3.3"><times id="S2.E2.m1.1.1.1.1.3.3.1.cmml" xref="S2.E2.m1.1.1.1.1.3.3.1"></times><ci id="S2.E2.m1.1.1.1.1.3.3.2.cmml" xref="S2.E2.m1.1.1.1.1.3.3.2">ğ‘</ci><ci id="S2.E2.m1.1.1.1.1.3.3.3.cmml" xref="S2.E2.m1.1.1.1.1.3.3.3">ğ·</ci><ci id="S2.E2.m1.1.1.1.1.3.3.4.cmml" xref="S2.E2.m1.1.1.1.1.3.3.4">ğ»</ci></apply><apply id="S2.E2.m1.1.1.1.1.3.4.cmml" xref="S2.E2.m1.1.1.1.1.3.4"><times id="S2.E2.m1.1.1.1.1.3.4.1.cmml" xref="S2.E2.m1.1.1.1.1.3.4.1"></times><ci id="S2.E2.m1.1.1.1.1.3.4.2.cmml" xref="S2.E2.m1.1.1.1.1.3.4.2">ğ»</ci><ci id="S2.E2.m1.1.1.1.1.3.4.3.cmml" xref="S2.E2.m1.1.1.1.1.3.4.3">ğ‘‰</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.E2.m1.1c">Q=N\times D+N\times D\times H+H\times V,</annotation><annotation encoding="application/x-llamapun" id="S2.E2.m1.1d">italic_Q = italic_N Ã— italic_D + italic_N Ã— italic_D Ã— italic_H + italic_H Ã— italic_V ,</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(2)</span></td>
</tr>
</table>
<p id="S2.SS1.p2.7" class="ltx_p">where the dominating term is <math id="S2.SS1.p2.5.m1.1" class="ltx_Math" alttext="H\times V" display="inline"><semantics id="S2.SS1.p2.5.m1.1a"><mrow id="S2.SS1.p2.5.m1.1.1" xref="S2.SS1.p2.5.m1.1.1.cmml"><mi id="S2.SS1.p2.5.m1.1.1.2" xref="S2.SS1.p2.5.m1.1.1.2.cmml">H</mi><mo id="S2.SS1.p2.5.m1.1.1.1" xref="S2.SS1.p2.5.m1.1.1.1.cmml">Ã—</mo><mi id="S2.SS1.p2.5.m1.1.1.3" xref="S2.SS1.p2.5.m1.1.1.3.cmml">V</mi></mrow><annotation-xml encoding="MathML-Content" id="S2.SS1.p2.5.m1.1b"><apply id="S2.SS1.p2.5.m1.1.1.cmml" xref="S2.SS1.p2.5.m1.1.1"><times id="S2.SS1.p2.5.m1.1.1.1.cmml" xref="S2.SS1.p2.5.m1.1.1.1"></times><ci id="S2.SS1.p2.5.m1.1.1.2.cmml" xref="S2.SS1.p2.5.m1.1.1.2">ğ»</ci><ci id="S2.SS1.p2.5.m1.1.1.3.cmml" xref="S2.SS1.p2.5.m1.1.1.3">ğ‘‰</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p2.5.m1.1c">H\times V</annotation><annotation encoding="application/x-llamapun" id="S2.SS1.p2.5.m1.1d">italic_H Ã— italic_V</annotation></semantics></math>. However, several practical solutions were proposed for avoiding it; either using hierarchical versions of the softmaxÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib25" title="" class="ltx_ref">25</a>, <a href="#bib.bib23" title="" class="ltx_ref">23</a>, <a href="#bib.bib18" title="" class="ltx_ref">18</a>]</cite>, or avoiding
normalized models completely by using models that are not normalized during trainingÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>, <a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite>. With binary tree representations of the vocabulary, the number of output units that need to be evaluated
can go down to around <math id="S2.SS1.p2.6.m2.1" class="ltx_Math" alttext="log_{2}(V)" display="inline"><semantics id="S2.SS1.p2.6.m2.1a"><mrow id="S2.SS1.p2.6.m2.1.2" xref="S2.SS1.p2.6.m2.1.2.cmml"><mi id="S2.SS1.p2.6.m2.1.2.2" xref="S2.SS1.p2.6.m2.1.2.2.cmml">l</mi><mo id="S2.SS1.p2.6.m2.1.2.1" xref="S2.SS1.p2.6.m2.1.2.1.cmml">â¢</mo><mi id="S2.SS1.p2.6.m2.1.2.3" xref="S2.SS1.p2.6.m2.1.2.3.cmml">o</mi><mo id="S2.SS1.p2.6.m2.1.2.1a" xref="S2.SS1.p2.6.m2.1.2.1.cmml">â¢</mo><msub id="S2.SS1.p2.6.m2.1.2.4" xref="S2.SS1.p2.6.m2.1.2.4.cmml"><mi id="S2.SS1.p2.6.m2.1.2.4.2" xref="S2.SS1.p2.6.m2.1.2.4.2.cmml">g</mi><mn id="S2.SS1.p2.6.m2.1.2.4.3" xref="S2.SS1.p2.6.m2.1.2.4.3.cmml">2</mn></msub><mo id="S2.SS1.p2.6.m2.1.2.1b" xref="S2.SS1.p2.6.m2.1.2.1.cmml">â¢</mo><mrow id="S2.SS1.p2.6.m2.1.2.5.2" xref="S2.SS1.p2.6.m2.1.2.cmml"><mo stretchy="false" id="S2.SS1.p2.6.m2.1.2.5.2.1" xref="S2.SS1.p2.6.m2.1.2.cmml">(</mo><mi id="S2.SS1.p2.6.m2.1.1" xref="S2.SS1.p2.6.m2.1.1.cmml">V</mi><mo stretchy="false" id="S2.SS1.p2.6.m2.1.2.5.2.2" xref="S2.SS1.p2.6.m2.1.2.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.SS1.p2.6.m2.1b"><apply id="S2.SS1.p2.6.m2.1.2.cmml" xref="S2.SS1.p2.6.m2.1.2"><times id="S2.SS1.p2.6.m2.1.2.1.cmml" xref="S2.SS1.p2.6.m2.1.2.1"></times><ci id="S2.SS1.p2.6.m2.1.2.2.cmml" xref="S2.SS1.p2.6.m2.1.2.2">ğ‘™</ci><ci id="S2.SS1.p2.6.m2.1.2.3.cmml" xref="S2.SS1.p2.6.m2.1.2.3">ğ‘œ</ci><apply id="S2.SS1.p2.6.m2.1.2.4.cmml" xref="S2.SS1.p2.6.m2.1.2.4"><csymbol cd="ambiguous" id="S2.SS1.p2.6.m2.1.2.4.1.cmml" xref="S2.SS1.p2.6.m2.1.2.4">subscript</csymbol><ci id="S2.SS1.p2.6.m2.1.2.4.2.cmml" xref="S2.SS1.p2.6.m2.1.2.4.2">ğ‘”</ci><cn type="integer" id="S2.SS1.p2.6.m2.1.2.4.3.cmml" xref="S2.SS1.p2.6.m2.1.2.4.3">2</cn></apply><ci id="S2.SS1.p2.6.m2.1.1.cmml" xref="S2.SS1.p2.6.m2.1.1">ğ‘‰</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p2.6.m2.1c">log_{2}(V)</annotation><annotation encoding="application/x-llamapun" id="S2.SS1.p2.6.m2.1d">italic_l italic_o italic_g start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT ( italic_V )</annotation></semantics></math>. Thus, most of the complexity is caused by the term <math id="S2.SS1.p2.7.m3.1" class="ltx_Math" alttext="N\times D\times H" display="inline"><semantics id="S2.SS1.p2.7.m3.1a"><mrow id="S2.SS1.p2.7.m3.1.1" xref="S2.SS1.p2.7.m3.1.1.cmml"><mi id="S2.SS1.p2.7.m3.1.1.2" xref="S2.SS1.p2.7.m3.1.1.2.cmml">N</mi><mo id="S2.SS1.p2.7.m3.1.1.1" xref="S2.SS1.p2.7.m3.1.1.1.cmml">Ã—</mo><mi id="S2.SS1.p2.7.m3.1.1.3" xref="S2.SS1.p2.7.m3.1.1.3.cmml">D</mi><mo id="S2.SS1.p2.7.m3.1.1.1a" xref="S2.SS1.p2.7.m3.1.1.1.cmml">Ã—</mo><mi id="S2.SS1.p2.7.m3.1.1.4" xref="S2.SS1.p2.7.m3.1.1.4.cmml">H</mi></mrow><annotation-xml encoding="MathML-Content" id="S2.SS1.p2.7.m3.1b"><apply id="S2.SS1.p2.7.m3.1.1.cmml" xref="S2.SS1.p2.7.m3.1.1"><times id="S2.SS1.p2.7.m3.1.1.1.cmml" xref="S2.SS1.p2.7.m3.1.1.1"></times><ci id="S2.SS1.p2.7.m3.1.1.2.cmml" xref="S2.SS1.p2.7.m3.1.1.2">ğ‘</ci><ci id="S2.SS1.p2.7.m3.1.1.3.cmml" xref="S2.SS1.p2.7.m3.1.1.3">ğ·</ci><ci id="S2.SS1.p2.7.m3.1.1.4.cmml" xref="S2.SS1.p2.7.m3.1.1.4">ğ»</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p2.7.m3.1c">N\times D\times H</annotation><annotation encoding="application/x-llamapun" id="S2.SS1.p2.7.m3.1d">italic_N Ã— italic_D Ã— italic_H</annotation></semantics></math>.</p>
</div>
<div id="S2.SS1.p3" class="ltx_para ltx_noindent">
<p id="S2.SS1.p3.3" class="ltx_p">In our models, we use hierarchical softmax where the vocabulary is represented as a Huffman binary tree.
This follows previous observations that the frequency of words works well for obtaining classes in neural net language modelsÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref">16</a>]</cite>.
Huffman trees assign short binary codes to frequent words, and this further reduces the number of output units that need to be evaluated: while balanced binary tree would require <math id="S2.SS1.p3.1.m1.1" class="ltx_Math" alttext="log_{2}(V)" display="inline"><semantics id="S2.SS1.p3.1.m1.1a"><mrow id="S2.SS1.p3.1.m1.1.2" xref="S2.SS1.p3.1.m1.1.2.cmml"><mi id="S2.SS1.p3.1.m1.1.2.2" xref="S2.SS1.p3.1.m1.1.2.2.cmml">l</mi><mo id="S2.SS1.p3.1.m1.1.2.1" xref="S2.SS1.p3.1.m1.1.2.1.cmml">â¢</mo><mi id="S2.SS1.p3.1.m1.1.2.3" xref="S2.SS1.p3.1.m1.1.2.3.cmml">o</mi><mo id="S2.SS1.p3.1.m1.1.2.1a" xref="S2.SS1.p3.1.m1.1.2.1.cmml">â¢</mo><msub id="S2.SS1.p3.1.m1.1.2.4" xref="S2.SS1.p3.1.m1.1.2.4.cmml"><mi id="S2.SS1.p3.1.m1.1.2.4.2" xref="S2.SS1.p3.1.m1.1.2.4.2.cmml">g</mi><mn id="S2.SS1.p3.1.m1.1.2.4.3" xref="S2.SS1.p3.1.m1.1.2.4.3.cmml">2</mn></msub><mo id="S2.SS1.p3.1.m1.1.2.1b" xref="S2.SS1.p3.1.m1.1.2.1.cmml">â¢</mo><mrow id="S2.SS1.p3.1.m1.1.2.5.2" xref="S2.SS1.p3.1.m1.1.2.cmml"><mo stretchy="false" id="S2.SS1.p3.1.m1.1.2.5.2.1" xref="S2.SS1.p3.1.m1.1.2.cmml">(</mo><mi id="S2.SS1.p3.1.m1.1.1" xref="S2.SS1.p3.1.m1.1.1.cmml">V</mi><mo stretchy="false" id="S2.SS1.p3.1.m1.1.2.5.2.2" xref="S2.SS1.p3.1.m1.1.2.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.SS1.p3.1.m1.1b"><apply id="S2.SS1.p3.1.m1.1.2.cmml" xref="S2.SS1.p3.1.m1.1.2"><times id="S2.SS1.p3.1.m1.1.2.1.cmml" xref="S2.SS1.p3.1.m1.1.2.1"></times><ci id="S2.SS1.p3.1.m1.1.2.2.cmml" xref="S2.SS1.p3.1.m1.1.2.2">ğ‘™</ci><ci id="S2.SS1.p3.1.m1.1.2.3.cmml" xref="S2.SS1.p3.1.m1.1.2.3">ğ‘œ</ci><apply id="S2.SS1.p3.1.m1.1.2.4.cmml" xref="S2.SS1.p3.1.m1.1.2.4"><csymbol cd="ambiguous" id="S2.SS1.p3.1.m1.1.2.4.1.cmml" xref="S2.SS1.p3.1.m1.1.2.4">subscript</csymbol><ci id="S2.SS1.p3.1.m1.1.2.4.2.cmml" xref="S2.SS1.p3.1.m1.1.2.4.2">ğ‘”</ci><cn type="integer" id="S2.SS1.p3.1.m1.1.2.4.3.cmml" xref="S2.SS1.p3.1.m1.1.2.4.3">2</cn></apply><ci id="S2.SS1.p3.1.m1.1.1.cmml" xref="S2.SS1.p3.1.m1.1.1">ğ‘‰</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p3.1.m1.1c">log_{2}(V)</annotation><annotation encoding="application/x-llamapun" id="S2.SS1.p3.1.m1.1d">italic_l italic_o italic_g start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT ( italic_V )</annotation></semantics></math>
outputs to be evaluated, the Huffman tree based hierarchical softmax requires only about <math id="S2.SS1.p3.2.m2.2" class="ltx_Math" alttext="log_{2}(Unigram\_perplexity(V))" display="inline"><semantics id="S2.SS1.p3.2.m2.2a"><mrow id="S2.SS1.p3.2.m2.2.2" xref="S2.SS1.p3.2.m2.2.2.cmml"><mi id="S2.SS1.p3.2.m2.2.2.3" xref="S2.SS1.p3.2.m2.2.2.3.cmml">l</mi><mo id="S2.SS1.p3.2.m2.2.2.2" xref="S2.SS1.p3.2.m2.2.2.2.cmml">â¢</mo><mi id="S2.SS1.p3.2.m2.2.2.4" xref="S2.SS1.p3.2.m2.2.2.4.cmml">o</mi><mo id="S2.SS1.p3.2.m2.2.2.2a" xref="S2.SS1.p3.2.m2.2.2.2.cmml">â¢</mo><msub id="S2.SS1.p3.2.m2.2.2.5" xref="S2.SS1.p3.2.m2.2.2.5.cmml"><mi id="S2.SS1.p3.2.m2.2.2.5.2" xref="S2.SS1.p3.2.m2.2.2.5.2.cmml">g</mi><mn id="S2.SS1.p3.2.m2.2.2.5.3" xref="S2.SS1.p3.2.m2.2.2.5.3.cmml">2</mn></msub><mo id="S2.SS1.p3.2.m2.2.2.2b" xref="S2.SS1.p3.2.m2.2.2.2.cmml">â¢</mo><mrow id="S2.SS1.p3.2.m2.2.2.1.1" xref="S2.SS1.p3.2.m2.2.2.1.1.1.cmml"><mo stretchy="false" id="S2.SS1.p3.2.m2.2.2.1.1.2" xref="S2.SS1.p3.2.m2.2.2.1.1.1.cmml">(</mo><mrow id="S2.SS1.p3.2.m2.2.2.1.1.1" xref="S2.SS1.p3.2.m2.2.2.1.1.1.cmml"><mi id="S2.SS1.p3.2.m2.2.2.1.1.1.2" xref="S2.SS1.p3.2.m2.2.2.1.1.1.2.cmml">U</mi><mo id="S2.SS1.p3.2.m2.2.2.1.1.1.1" xref="S2.SS1.p3.2.m2.2.2.1.1.1.1.cmml">â¢</mo><mi id="S2.SS1.p3.2.m2.2.2.1.1.1.3" xref="S2.SS1.p3.2.m2.2.2.1.1.1.3.cmml">n</mi><mo id="S2.SS1.p3.2.m2.2.2.1.1.1.1a" xref="S2.SS1.p3.2.m2.2.2.1.1.1.1.cmml">â¢</mo><mi id="S2.SS1.p3.2.m2.2.2.1.1.1.4" xref="S2.SS1.p3.2.m2.2.2.1.1.1.4.cmml">i</mi><mo id="S2.SS1.p3.2.m2.2.2.1.1.1.1b" xref="S2.SS1.p3.2.m2.2.2.1.1.1.1.cmml">â¢</mo><mi id="S2.SS1.p3.2.m2.2.2.1.1.1.5" xref="S2.SS1.p3.2.m2.2.2.1.1.1.5.cmml">g</mi><mo id="S2.SS1.p3.2.m2.2.2.1.1.1.1c" xref="S2.SS1.p3.2.m2.2.2.1.1.1.1.cmml">â¢</mo><mi id="S2.SS1.p3.2.m2.2.2.1.1.1.6" xref="S2.SS1.p3.2.m2.2.2.1.1.1.6.cmml">r</mi><mo id="S2.SS1.p3.2.m2.2.2.1.1.1.1d" xref="S2.SS1.p3.2.m2.2.2.1.1.1.1.cmml">â¢</mo><mi id="S2.SS1.p3.2.m2.2.2.1.1.1.7" xref="S2.SS1.p3.2.m2.2.2.1.1.1.7.cmml">a</mi><mo id="S2.SS1.p3.2.m2.2.2.1.1.1.1e" xref="S2.SS1.p3.2.m2.2.2.1.1.1.1.cmml">â¢</mo><mi id="S2.SS1.p3.2.m2.2.2.1.1.1.8" xref="S2.SS1.p3.2.m2.2.2.1.1.1.8.cmml">m</mi><mo id="S2.SS1.p3.2.m2.2.2.1.1.1.1f" xref="S2.SS1.p3.2.m2.2.2.1.1.1.1.cmml">â¢</mo><mi mathvariant="normal" id="S2.SS1.p3.2.m2.2.2.1.1.1.9" xref="S2.SS1.p3.2.m2.2.2.1.1.1.9.cmml">_</mi><mo id="S2.SS1.p3.2.m2.2.2.1.1.1.1g" xref="S2.SS1.p3.2.m2.2.2.1.1.1.1.cmml">â¢</mo><mi id="S2.SS1.p3.2.m2.2.2.1.1.1.10" xref="S2.SS1.p3.2.m2.2.2.1.1.1.10.cmml">p</mi><mo id="S2.SS1.p3.2.m2.2.2.1.1.1.1h" xref="S2.SS1.p3.2.m2.2.2.1.1.1.1.cmml">â¢</mo><mi id="S2.SS1.p3.2.m2.2.2.1.1.1.11" xref="S2.SS1.p3.2.m2.2.2.1.1.1.11.cmml">e</mi><mo id="S2.SS1.p3.2.m2.2.2.1.1.1.1i" xref="S2.SS1.p3.2.m2.2.2.1.1.1.1.cmml">â¢</mo><mi id="S2.SS1.p3.2.m2.2.2.1.1.1.12" xref="S2.SS1.p3.2.m2.2.2.1.1.1.12.cmml">r</mi><mo id="S2.SS1.p3.2.m2.2.2.1.1.1.1j" xref="S2.SS1.p3.2.m2.2.2.1.1.1.1.cmml">â¢</mo><mi id="S2.SS1.p3.2.m2.2.2.1.1.1.13" xref="S2.SS1.p3.2.m2.2.2.1.1.1.13.cmml">p</mi><mo id="S2.SS1.p3.2.m2.2.2.1.1.1.1k" xref="S2.SS1.p3.2.m2.2.2.1.1.1.1.cmml">â¢</mo><mi id="S2.SS1.p3.2.m2.2.2.1.1.1.14" xref="S2.SS1.p3.2.m2.2.2.1.1.1.14.cmml">l</mi><mo id="S2.SS1.p3.2.m2.2.2.1.1.1.1l" xref="S2.SS1.p3.2.m2.2.2.1.1.1.1.cmml">â¢</mo><mi id="S2.SS1.p3.2.m2.2.2.1.1.1.15" xref="S2.SS1.p3.2.m2.2.2.1.1.1.15.cmml">e</mi><mo id="S2.SS1.p3.2.m2.2.2.1.1.1.1m" xref="S2.SS1.p3.2.m2.2.2.1.1.1.1.cmml">â¢</mo><mi id="S2.SS1.p3.2.m2.2.2.1.1.1.16" xref="S2.SS1.p3.2.m2.2.2.1.1.1.16.cmml">x</mi><mo id="S2.SS1.p3.2.m2.2.2.1.1.1.1n" xref="S2.SS1.p3.2.m2.2.2.1.1.1.1.cmml">â¢</mo><mi id="S2.SS1.p3.2.m2.2.2.1.1.1.17" xref="S2.SS1.p3.2.m2.2.2.1.1.1.17.cmml">i</mi><mo id="S2.SS1.p3.2.m2.2.2.1.1.1.1o" xref="S2.SS1.p3.2.m2.2.2.1.1.1.1.cmml">â¢</mo><mi id="S2.SS1.p3.2.m2.2.2.1.1.1.18" xref="S2.SS1.p3.2.m2.2.2.1.1.1.18.cmml">t</mi><mo id="S2.SS1.p3.2.m2.2.2.1.1.1.1p" xref="S2.SS1.p3.2.m2.2.2.1.1.1.1.cmml">â¢</mo><mi id="S2.SS1.p3.2.m2.2.2.1.1.1.19" xref="S2.SS1.p3.2.m2.2.2.1.1.1.19.cmml">y</mi><mo id="S2.SS1.p3.2.m2.2.2.1.1.1.1q" xref="S2.SS1.p3.2.m2.2.2.1.1.1.1.cmml">â¢</mo><mrow id="S2.SS1.p3.2.m2.2.2.1.1.1.20.2" xref="S2.SS1.p3.2.m2.2.2.1.1.1.cmml"><mo stretchy="false" id="S2.SS1.p3.2.m2.2.2.1.1.1.20.2.1" xref="S2.SS1.p3.2.m2.2.2.1.1.1.cmml">(</mo><mi id="S2.SS1.p3.2.m2.1.1" xref="S2.SS1.p3.2.m2.1.1.cmml">V</mi><mo stretchy="false" id="S2.SS1.p3.2.m2.2.2.1.1.1.20.2.2" xref="S2.SS1.p3.2.m2.2.2.1.1.1.cmml">)</mo></mrow></mrow><mo stretchy="false" id="S2.SS1.p3.2.m2.2.2.1.1.3" xref="S2.SS1.p3.2.m2.2.2.1.1.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.SS1.p3.2.m2.2b"><apply id="S2.SS1.p3.2.m2.2.2.cmml" xref="S2.SS1.p3.2.m2.2.2"><times id="S2.SS1.p3.2.m2.2.2.2.cmml" xref="S2.SS1.p3.2.m2.2.2.2"></times><ci id="S2.SS1.p3.2.m2.2.2.3.cmml" xref="S2.SS1.p3.2.m2.2.2.3">ğ‘™</ci><ci id="S2.SS1.p3.2.m2.2.2.4.cmml" xref="S2.SS1.p3.2.m2.2.2.4">ğ‘œ</ci><apply id="S2.SS1.p3.2.m2.2.2.5.cmml" xref="S2.SS1.p3.2.m2.2.2.5"><csymbol cd="ambiguous" id="S2.SS1.p3.2.m2.2.2.5.1.cmml" xref="S2.SS1.p3.2.m2.2.2.5">subscript</csymbol><ci id="S2.SS1.p3.2.m2.2.2.5.2.cmml" xref="S2.SS1.p3.2.m2.2.2.5.2">ğ‘”</ci><cn type="integer" id="S2.SS1.p3.2.m2.2.2.5.3.cmml" xref="S2.SS1.p3.2.m2.2.2.5.3">2</cn></apply><apply id="S2.SS1.p3.2.m2.2.2.1.1.1.cmml" xref="S2.SS1.p3.2.m2.2.2.1.1"><times id="S2.SS1.p3.2.m2.2.2.1.1.1.1.cmml" xref="S2.SS1.p3.2.m2.2.2.1.1.1.1"></times><ci id="S2.SS1.p3.2.m2.2.2.1.1.1.2.cmml" xref="S2.SS1.p3.2.m2.2.2.1.1.1.2">ğ‘ˆ</ci><ci id="S2.SS1.p3.2.m2.2.2.1.1.1.3.cmml" xref="S2.SS1.p3.2.m2.2.2.1.1.1.3">ğ‘›</ci><ci id="S2.SS1.p3.2.m2.2.2.1.1.1.4.cmml" xref="S2.SS1.p3.2.m2.2.2.1.1.1.4">ğ‘–</ci><ci id="S2.SS1.p3.2.m2.2.2.1.1.1.5.cmml" xref="S2.SS1.p3.2.m2.2.2.1.1.1.5">ğ‘”</ci><ci id="S2.SS1.p3.2.m2.2.2.1.1.1.6.cmml" xref="S2.SS1.p3.2.m2.2.2.1.1.1.6">ğ‘Ÿ</ci><ci id="S2.SS1.p3.2.m2.2.2.1.1.1.7.cmml" xref="S2.SS1.p3.2.m2.2.2.1.1.1.7">ğ‘</ci><ci id="S2.SS1.p3.2.m2.2.2.1.1.1.8.cmml" xref="S2.SS1.p3.2.m2.2.2.1.1.1.8">ğ‘š</ci><ci id="S2.SS1.p3.2.m2.2.2.1.1.1.9.cmml" xref="S2.SS1.p3.2.m2.2.2.1.1.1.9">_</ci><ci id="S2.SS1.p3.2.m2.2.2.1.1.1.10.cmml" xref="S2.SS1.p3.2.m2.2.2.1.1.1.10">ğ‘</ci><ci id="S2.SS1.p3.2.m2.2.2.1.1.1.11.cmml" xref="S2.SS1.p3.2.m2.2.2.1.1.1.11">ğ‘’</ci><ci id="S2.SS1.p3.2.m2.2.2.1.1.1.12.cmml" xref="S2.SS1.p3.2.m2.2.2.1.1.1.12">ğ‘Ÿ</ci><ci id="S2.SS1.p3.2.m2.2.2.1.1.1.13.cmml" xref="S2.SS1.p3.2.m2.2.2.1.1.1.13">ğ‘</ci><ci id="S2.SS1.p3.2.m2.2.2.1.1.1.14.cmml" xref="S2.SS1.p3.2.m2.2.2.1.1.1.14">ğ‘™</ci><ci id="S2.SS1.p3.2.m2.2.2.1.1.1.15.cmml" xref="S2.SS1.p3.2.m2.2.2.1.1.1.15">ğ‘’</ci><ci id="S2.SS1.p3.2.m2.2.2.1.1.1.16.cmml" xref="S2.SS1.p3.2.m2.2.2.1.1.1.16">ğ‘¥</ci><ci id="S2.SS1.p3.2.m2.2.2.1.1.1.17.cmml" xref="S2.SS1.p3.2.m2.2.2.1.1.1.17">ğ‘–</ci><ci id="S2.SS1.p3.2.m2.2.2.1.1.1.18.cmml" xref="S2.SS1.p3.2.m2.2.2.1.1.1.18">ğ‘¡</ci><ci id="S2.SS1.p3.2.m2.2.2.1.1.1.19.cmml" xref="S2.SS1.p3.2.m2.2.2.1.1.1.19">ğ‘¦</ci><ci id="S2.SS1.p3.2.m2.1.1.cmml" xref="S2.SS1.p3.2.m2.1.1">ğ‘‰</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p3.2.m2.2c">log_{2}(Unigram\_perplexity(V))</annotation><annotation encoding="application/x-llamapun" id="S2.SS1.p3.2.m2.2d">italic_l italic_o italic_g start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT ( italic_U italic_n italic_i italic_g italic_r italic_a italic_m _ italic_p italic_e italic_r italic_p italic_l italic_e italic_x italic_i italic_t italic_y ( italic_V ) )</annotation></semantics></math>. For example when the vocabulary size is one million words, this results in about two times speedup in evaluation.
While this is not crucial speedup for neural network LMs as the computational bottleneck is in the <math id="S2.SS1.p3.3.m3.1" class="ltx_Math" alttext="N\times D\times H" display="inline"><semantics id="S2.SS1.p3.3.m3.1a"><mrow id="S2.SS1.p3.3.m3.1.1" xref="S2.SS1.p3.3.m3.1.1.cmml"><mi id="S2.SS1.p3.3.m3.1.1.2" xref="S2.SS1.p3.3.m3.1.1.2.cmml">N</mi><mo id="S2.SS1.p3.3.m3.1.1.1" xref="S2.SS1.p3.3.m3.1.1.1.cmml">Ã—</mo><mi id="S2.SS1.p3.3.m3.1.1.3" xref="S2.SS1.p3.3.m3.1.1.3.cmml">D</mi><mo id="S2.SS1.p3.3.m3.1.1.1a" xref="S2.SS1.p3.3.m3.1.1.1.cmml">Ã—</mo><mi id="S2.SS1.p3.3.m3.1.1.4" xref="S2.SS1.p3.3.m3.1.1.4.cmml">H</mi></mrow><annotation-xml encoding="MathML-Content" id="S2.SS1.p3.3.m3.1b"><apply id="S2.SS1.p3.3.m3.1.1.cmml" xref="S2.SS1.p3.3.m3.1.1"><times id="S2.SS1.p3.3.m3.1.1.1.cmml" xref="S2.SS1.p3.3.m3.1.1.1"></times><ci id="S2.SS1.p3.3.m3.1.1.2.cmml" xref="S2.SS1.p3.3.m3.1.1.2">ğ‘</ci><ci id="S2.SS1.p3.3.m3.1.1.3.cmml" xref="S2.SS1.p3.3.m3.1.1.3">ğ·</ci><ci id="S2.SS1.p3.3.m3.1.1.4.cmml" xref="S2.SS1.p3.3.m3.1.1.4">ğ»</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p3.3.m3.1c">N\times D\times H</annotation><annotation encoding="application/x-llamapun" id="S2.SS1.p3.3.m3.1d">italic_N Ã— italic_D Ã— italic_H</annotation></semantics></math> term, we will later propose architectures that do not have hidden layers and thus depend
heavily on the efficiency of the softmax normalization.</p>
</div>
</section>
<section id="S2.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.2 </span>Recurrent Neural Net Language Model (RNNLM)</h3>

<div id="S2.SS2.p1" class="ltx_para ltx_noindent">
<p id="S2.SS2.p1.1" class="ltx_p">Recurrent neural network based language model has been proposed to overcome certain limitations of the feedforward NNLM, such as the need to specify the context length (the order of the model <math id="S2.SS2.p1.1.m1.1" class="ltx_Math" alttext="N" display="inline"><semantics id="S2.SS2.p1.1.m1.1a"><mi id="S2.SS2.p1.1.m1.1.1" xref="S2.SS2.p1.1.m1.1.1.cmml">N</mi><annotation-xml encoding="MathML-Content" id="S2.SS2.p1.1.m1.1b"><ci id="S2.SS2.p1.1.m1.1.1.cmml" xref="S2.SS2.p1.1.m1.1.1">ğ‘</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p1.1.m1.1c">N</annotation><annotation encoding="application/x-llamapun" id="S2.SS2.p1.1.m1.1d">italic_N</annotation></semantics></math>), and because
theoretically RNNs can efficiently represent more complex patterns than the shallow neural networksÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib15" title="" class="ltx_ref">15</a>, <a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite>. The RNN model does not have a projection layer; only input, hidden and output layer.
What is special for this type of model is the recurrent matrix that connects hidden layer to itself, using time-delayed connections. This allows the recurrent model to form some kind of short term memory, as information
from the past can be represented by the hidden layer state that gets updated based on the current input and the state of the hidden layer in the previous time step.</p>
</div>
<div id="S2.SS2.p2" class="ltx_para ltx_noindent">
<p id="S2.SS2.p2.6" class="ltx_p">The complexity per training example of the RNN model is</p>
<table id="S2.E3" class="ltx_equation ltx_eqn_table">

<tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S2.E3.m1.1" class="ltx_Math" alttext="Q=H\times H+H\times V," display="block"><semantics id="S2.E3.m1.1a"><mrow id="S2.E3.m1.1.1.1" xref="S2.E3.m1.1.1.1.1.cmml"><mrow id="S2.E3.m1.1.1.1.1" xref="S2.E3.m1.1.1.1.1.cmml"><mi id="S2.E3.m1.1.1.1.1.2" xref="S2.E3.m1.1.1.1.1.2.cmml">Q</mi><mo id="S2.E3.m1.1.1.1.1.1" xref="S2.E3.m1.1.1.1.1.1.cmml">=</mo><mrow id="S2.E3.m1.1.1.1.1.3" xref="S2.E3.m1.1.1.1.1.3.cmml"><mrow id="S2.E3.m1.1.1.1.1.3.2" xref="S2.E3.m1.1.1.1.1.3.2.cmml"><mi id="S2.E3.m1.1.1.1.1.3.2.2" xref="S2.E3.m1.1.1.1.1.3.2.2.cmml">H</mi><mo id="S2.E3.m1.1.1.1.1.3.2.1" xref="S2.E3.m1.1.1.1.1.3.2.1.cmml">Ã—</mo><mi id="S2.E3.m1.1.1.1.1.3.2.3" xref="S2.E3.m1.1.1.1.1.3.2.3.cmml">H</mi></mrow><mo id="S2.E3.m1.1.1.1.1.3.1" xref="S2.E3.m1.1.1.1.1.3.1.cmml">+</mo><mrow id="S2.E3.m1.1.1.1.1.3.3" xref="S2.E3.m1.1.1.1.1.3.3.cmml"><mi id="S2.E3.m1.1.1.1.1.3.3.2" xref="S2.E3.m1.1.1.1.1.3.3.2.cmml">H</mi><mo id="S2.E3.m1.1.1.1.1.3.3.1" xref="S2.E3.m1.1.1.1.1.3.3.1.cmml">Ã—</mo><mi id="S2.E3.m1.1.1.1.1.3.3.3" xref="S2.E3.m1.1.1.1.1.3.3.3.cmml">V</mi></mrow></mrow></mrow><mo id="S2.E3.m1.1.1.1.2" xref="S2.E3.m1.1.1.1.1.cmml">,</mo></mrow><annotation-xml encoding="MathML-Content" id="S2.E3.m1.1b"><apply id="S2.E3.m1.1.1.1.1.cmml" xref="S2.E3.m1.1.1.1"><eq id="S2.E3.m1.1.1.1.1.1.cmml" xref="S2.E3.m1.1.1.1.1.1"></eq><ci id="S2.E3.m1.1.1.1.1.2.cmml" xref="S2.E3.m1.1.1.1.1.2">ğ‘„</ci><apply id="S2.E3.m1.1.1.1.1.3.cmml" xref="S2.E3.m1.1.1.1.1.3"><plus id="S2.E3.m1.1.1.1.1.3.1.cmml" xref="S2.E3.m1.1.1.1.1.3.1"></plus><apply id="S2.E3.m1.1.1.1.1.3.2.cmml" xref="S2.E3.m1.1.1.1.1.3.2"><times id="S2.E3.m1.1.1.1.1.3.2.1.cmml" xref="S2.E3.m1.1.1.1.1.3.2.1"></times><ci id="S2.E3.m1.1.1.1.1.3.2.2.cmml" xref="S2.E3.m1.1.1.1.1.3.2.2">ğ»</ci><ci id="S2.E3.m1.1.1.1.1.3.2.3.cmml" xref="S2.E3.m1.1.1.1.1.3.2.3">ğ»</ci></apply><apply id="S2.E3.m1.1.1.1.1.3.3.cmml" xref="S2.E3.m1.1.1.1.1.3.3"><times id="S2.E3.m1.1.1.1.1.3.3.1.cmml" xref="S2.E3.m1.1.1.1.1.3.3.1"></times><ci id="S2.E3.m1.1.1.1.1.3.3.2.cmml" xref="S2.E3.m1.1.1.1.1.3.3.2">ğ»</ci><ci id="S2.E3.m1.1.1.1.1.3.3.3.cmml" xref="S2.E3.m1.1.1.1.1.3.3.3">ğ‘‰</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.E3.m1.1c">Q=H\times H+H\times V,</annotation><annotation encoding="application/x-llamapun" id="S2.E3.m1.1d">italic_Q = italic_H Ã— italic_H + italic_H Ã— italic_V ,</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(3)</span></td>
</tr>
</table>
<p id="S2.SS2.p2.5" class="ltx_p">where the word representations <math id="S2.SS2.p2.1.m1.1" class="ltx_Math" alttext="D" display="inline"><semantics id="S2.SS2.p2.1.m1.1a"><mi id="S2.SS2.p2.1.m1.1.1" xref="S2.SS2.p2.1.m1.1.1.cmml">D</mi><annotation-xml encoding="MathML-Content" id="S2.SS2.p2.1.m1.1b"><ci id="S2.SS2.p2.1.m1.1.1.cmml" xref="S2.SS2.p2.1.m1.1.1">ğ·</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p2.1.m1.1c">D</annotation><annotation encoding="application/x-llamapun" id="S2.SS2.p2.1.m1.1d">italic_D</annotation></semantics></math> have the same dimensionality as the hidden layer <math id="S2.SS2.p2.2.m2.1" class="ltx_Math" alttext="H" display="inline"><semantics id="S2.SS2.p2.2.m2.1a"><mi id="S2.SS2.p2.2.m2.1.1" xref="S2.SS2.p2.2.m2.1.1.cmml">H</mi><annotation-xml encoding="MathML-Content" id="S2.SS2.p2.2.m2.1b"><ci id="S2.SS2.p2.2.m2.1.1.cmml" xref="S2.SS2.p2.2.m2.1.1">ğ»</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p2.2.m2.1c">H</annotation><annotation encoding="application/x-llamapun" id="S2.SS2.p2.2.m2.1d">italic_H</annotation></semantics></math>. Again, the term <math id="S2.SS2.p2.3.m3.1" class="ltx_Math" alttext="H\times V" display="inline"><semantics id="S2.SS2.p2.3.m3.1a"><mrow id="S2.SS2.p2.3.m3.1.1" xref="S2.SS2.p2.3.m3.1.1.cmml"><mi id="S2.SS2.p2.3.m3.1.1.2" xref="S2.SS2.p2.3.m3.1.1.2.cmml">H</mi><mo id="S2.SS2.p2.3.m3.1.1.1" xref="S2.SS2.p2.3.m3.1.1.1.cmml">Ã—</mo><mi id="S2.SS2.p2.3.m3.1.1.3" xref="S2.SS2.p2.3.m3.1.1.3.cmml">V</mi></mrow><annotation-xml encoding="MathML-Content" id="S2.SS2.p2.3.m3.1b"><apply id="S2.SS2.p2.3.m3.1.1.cmml" xref="S2.SS2.p2.3.m3.1.1"><times id="S2.SS2.p2.3.m3.1.1.1.cmml" xref="S2.SS2.p2.3.m3.1.1.1"></times><ci id="S2.SS2.p2.3.m3.1.1.2.cmml" xref="S2.SS2.p2.3.m3.1.1.2">ğ»</ci><ci id="S2.SS2.p2.3.m3.1.1.3.cmml" xref="S2.SS2.p2.3.m3.1.1.3">ğ‘‰</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p2.3.m3.1c">H\times V</annotation><annotation encoding="application/x-llamapun" id="S2.SS2.p2.3.m3.1d">italic_H Ã— italic_V</annotation></semantics></math> can be efficiently reduced to <math id="S2.SS2.p2.4.m4.1" class="ltx_Math" alttext="H\times log_{2}(V)" display="inline"><semantics id="S2.SS2.p2.4.m4.1a"><mrow id="S2.SS2.p2.4.m4.1.2" xref="S2.SS2.p2.4.m4.1.2.cmml"><mrow id="S2.SS2.p2.4.m4.1.2.2" xref="S2.SS2.p2.4.m4.1.2.2.cmml"><mi id="S2.SS2.p2.4.m4.1.2.2.2" xref="S2.SS2.p2.4.m4.1.2.2.2.cmml">H</mi><mo id="S2.SS2.p2.4.m4.1.2.2.1" xref="S2.SS2.p2.4.m4.1.2.2.1.cmml">Ã—</mo><mi id="S2.SS2.p2.4.m4.1.2.2.3" xref="S2.SS2.p2.4.m4.1.2.2.3.cmml">l</mi></mrow><mo id="S2.SS2.p2.4.m4.1.2.1" xref="S2.SS2.p2.4.m4.1.2.1.cmml">â¢</mo><mi id="S2.SS2.p2.4.m4.1.2.3" xref="S2.SS2.p2.4.m4.1.2.3.cmml">o</mi><mo id="S2.SS2.p2.4.m4.1.2.1a" xref="S2.SS2.p2.4.m4.1.2.1.cmml">â¢</mo><msub id="S2.SS2.p2.4.m4.1.2.4" xref="S2.SS2.p2.4.m4.1.2.4.cmml"><mi id="S2.SS2.p2.4.m4.1.2.4.2" xref="S2.SS2.p2.4.m4.1.2.4.2.cmml">g</mi><mn id="S2.SS2.p2.4.m4.1.2.4.3" xref="S2.SS2.p2.4.m4.1.2.4.3.cmml">2</mn></msub><mo id="S2.SS2.p2.4.m4.1.2.1b" xref="S2.SS2.p2.4.m4.1.2.1.cmml">â¢</mo><mrow id="S2.SS2.p2.4.m4.1.2.5.2" xref="S2.SS2.p2.4.m4.1.2.cmml"><mo stretchy="false" id="S2.SS2.p2.4.m4.1.2.5.2.1" xref="S2.SS2.p2.4.m4.1.2.cmml">(</mo><mi id="S2.SS2.p2.4.m4.1.1" xref="S2.SS2.p2.4.m4.1.1.cmml">V</mi><mo stretchy="false" id="S2.SS2.p2.4.m4.1.2.5.2.2" xref="S2.SS2.p2.4.m4.1.2.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.SS2.p2.4.m4.1b"><apply id="S2.SS2.p2.4.m4.1.2.cmml" xref="S2.SS2.p2.4.m4.1.2"><times id="S2.SS2.p2.4.m4.1.2.1.cmml" xref="S2.SS2.p2.4.m4.1.2.1"></times><apply id="S2.SS2.p2.4.m4.1.2.2.cmml" xref="S2.SS2.p2.4.m4.1.2.2"><times id="S2.SS2.p2.4.m4.1.2.2.1.cmml" xref="S2.SS2.p2.4.m4.1.2.2.1"></times><ci id="S2.SS2.p2.4.m4.1.2.2.2.cmml" xref="S2.SS2.p2.4.m4.1.2.2.2">ğ»</ci><ci id="S2.SS2.p2.4.m4.1.2.2.3.cmml" xref="S2.SS2.p2.4.m4.1.2.2.3">ğ‘™</ci></apply><ci id="S2.SS2.p2.4.m4.1.2.3.cmml" xref="S2.SS2.p2.4.m4.1.2.3">ğ‘œ</ci><apply id="S2.SS2.p2.4.m4.1.2.4.cmml" xref="S2.SS2.p2.4.m4.1.2.4"><csymbol cd="ambiguous" id="S2.SS2.p2.4.m4.1.2.4.1.cmml" xref="S2.SS2.p2.4.m4.1.2.4">subscript</csymbol><ci id="S2.SS2.p2.4.m4.1.2.4.2.cmml" xref="S2.SS2.p2.4.m4.1.2.4.2">ğ‘”</ci><cn type="integer" id="S2.SS2.p2.4.m4.1.2.4.3.cmml" xref="S2.SS2.p2.4.m4.1.2.4.3">2</cn></apply><ci id="S2.SS2.p2.4.m4.1.1.cmml" xref="S2.SS2.p2.4.m4.1.1">ğ‘‰</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p2.4.m4.1c">H\times log_{2}(V)</annotation><annotation encoding="application/x-llamapun" id="S2.SS2.p2.4.m4.1d">italic_H Ã— italic_l italic_o italic_g start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT ( italic_V )</annotation></semantics></math> by using hierarchical softmax. Most of the complexity
then comes from <math id="S2.SS2.p2.5.m5.1" class="ltx_Math" alttext="H\times H" display="inline"><semantics id="S2.SS2.p2.5.m5.1a"><mrow id="S2.SS2.p2.5.m5.1.1" xref="S2.SS2.p2.5.m5.1.1.cmml"><mi id="S2.SS2.p2.5.m5.1.1.2" xref="S2.SS2.p2.5.m5.1.1.2.cmml">H</mi><mo id="S2.SS2.p2.5.m5.1.1.1" xref="S2.SS2.p2.5.m5.1.1.1.cmml">Ã—</mo><mi id="S2.SS2.p2.5.m5.1.1.3" xref="S2.SS2.p2.5.m5.1.1.3.cmml">H</mi></mrow><annotation-xml encoding="MathML-Content" id="S2.SS2.p2.5.m5.1b"><apply id="S2.SS2.p2.5.m5.1.1.cmml" xref="S2.SS2.p2.5.m5.1.1"><times id="S2.SS2.p2.5.m5.1.1.1.cmml" xref="S2.SS2.p2.5.m5.1.1.1"></times><ci id="S2.SS2.p2.5.m5.1.1.2.cmml" xref="S2.SS2.p2.5.m5.1.1.2">ğ»</ci><ci id="S2.SS2.p2.5.m5.1.1.3.cmml" xref="S2.SS2.p2.5.m5.1.1.3">ğ»</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p2.5.m5.1c">H\times H</annotation><annotation encoding="application/x-llamapun" id="S2.SS2.p2.5.m5.1d">italic_H Ã— italic_H</annotation></semantics></math>.</p>
</div>
</section>
<section id="S2.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.3 </span>Parallel Training of Neural Networks</h3>

<div id="S2.SS3.p1" class="ltx_para ltx_noindent">
<p id="S2.SS3.p1.1" class="ltx_p">To train models on huge data sets, we have implemented
several models on top of a large-scale distributed framework called
DistBeliefÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite>,
including the feedforward NNLM and the new models proposed in this
paper. The framework allows us to run multiple replicas of the same
model in parallel, and each
replica synchronizes its gradient updates through a centralized
server that keeps all the parameters. For this parallel training, we
use mini-batch asynchronous
gradient descent with an adaptive
learning rate procedure called AdagradÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite>. Under this framework, it is common to use one
hundred or more model replicas, each using many CPU cores at different machines in a data center.</p>
</div>
</section>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>New Log-linear Models</h2>

<div id="S3.p1" class="ltx_para ltx_noindent">
<p id="S3.p1.1" class="ltx_p">In this section, we propose two new model architectures for learning distributed representations of words that try to minimize computational complexity.
The main observation from the previous section was that most of the complexity is caused by the non-linear hidden layer in the model. While this is what makes neural networks so attractive, we decided to explore simpler models that might
not be able to represent the data as precisely as neural networks, but can possibly be trained on much more data efficiently. </p>
</div>
<div id="S3.p2" class="ltx_para ltx_noindent">
<p id="S3.p2.1" class="ltx_p">The new architectures directly follow those proposed in our earlier workÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>, <a href="#bib.bib14" title="" class="ltx_ref">14</a>]</cite>, where it was found that neural network language model can be successfully trained in two steps: first, continuous word vectors are learned using simple
model, and then the N-gram NNLM is trained on top of these distributed representations of words. While there has been later substantial amount of work that focuses on learning word vectors, we consider the approach proposed
inÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite> to be the simplest one. Note that related models have been proposed also much earlierÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib26" title="" class="ltx_ref">26</a>, <a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite>.</p>
</div>
<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Continuous Bag-of-Words Model</h3>

<div id="S3.SS1.p1" class="ltx_para ltx_noindent">
<p id="S3.SS1.p1.1" class="ltx_p">The first proposed architecture is similar to the feedforward NNLM, where the non-linear hidden layer is removed and the projection layer is shared for all words (not just the projection matrix); thus, all words get projected
into the same position (their vectors are averaged). We call this architecture a bag-of-words model as the order of words in the history does not influence the projection.
Furthermore, we also use words from the future; we have obtained the best performance on the task introduced in the next section by building a log-linear classifier with four future and four history words at the input,
where the training criterion is to correctly classify the current (middle) word. Training complexity is then</p>
<table id="S3.E4" class="ltx_equation ltx_eqn_table">

<tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S3.E4.m1.2" class="ltx_Math" alttext="Q=N\times D+D\times log_{2}(V)." display="block"><semantics id="S3.E4.m1.2a"><mrow id="S3.E4.m1.2.2.1" xref="S3.E4.m1.2.2.1.1.cmml"><mrow id="S3.E4.m1.2.2.1.1" xref="S3.E4.m1.2.2.1.1.cmml"><mi id="S3.E4.m1.2.2.1.1.2" xref="S3.E4.m1.2.2.1.1.2.cmml">Q</mi><mo id="S3.E4.m1.2.2.1.1.1" xref="S3.E4.m1.2.2.1.1.1.cmml">=</mo><mrow id="S3.E4.m1.2.2.1.1.3" xref="S3.E4.m1.2.2.1.1.3.cmml"><mrow id="S3.E4.m1.2.2.1.1.3.2" xref="S3.E4.m1.2.2.1.1.3.2.cmml"><mi id="S3.E4.m1.2.2.1.1.3.2.2" xref="S3.E4.m1.2.2.1.1.3.2.2.cmml">N</mi><mo id="S3.E4.m1.2.2.1.1.3.2.1" xref="S3.E4.m1.2.2.1.1.3.2.1.cmml">Ã—</mo><mi id="S3.E4.m1.2.2.1.1.3.2.3" xref="S3.E4.m1.2.2.1.1.3.2.3.cmml">D</mi></mrow><mo id="S3.E4.m1.2.2.1.1.3.1" xref="S3.E4.m1.2.2.1.1.3.1.cmml">+</mo><mrow id="S3.E4.m1.2.2.1.1.3.3" xref="S3.E4.m1.2.2.1.1.3.3.cmml"><mrow id="S3.E4.m1.2.2.1.1.3.3.2" xref="S3.E4.m1.2.2.1.1.3.3.2.cmml"><mi id="S3.E4.m1.2.2.1.1.3.3.2.2" xref="S3.E4.m1.2.2.1.1.3.3.2.2.cmml">D</mi><mo id="S3.E4.m1.2.2.1.1.3.3.2.1" xref="S3.E4.m1.2.2.1.1.3.3.2.1.cmml">Ã—</mo><mi id="S3.E4.m1.2.2.1.1.3.3.2.3" xref="S3.E4.m1.2.2.1.1.3.3.2.3.cmml">l</mi></mrow><mo id="S3.E4.m1.2.2.1.1.3.3.1" xref="S3.E4.m1.2.2.1.1.3.3.1.cmml">â¢</mo><mi id="S3.E4.m1.2.2.1.1.3.3.3" xref="S3.E4.m1.2.2.1.1.3.3.3.cmml">o</mi><mo id="S3.E4.m1.2.2.1.1.3.3.1a" xref="S3.E4.m1.2.2.1.1.3.3.1.cmml">â¢</mo><msub id="S3.E4.m1.2.2.1.1.3.3.4" xref="S3.E4.m1.2.2.1.1.3.3.4.cmml"><mi id="S3.E4.m1.2.2.1.1.3.3.4.2" xref="S3.E4.m1.2.2.1.1.3.3.4.2.cmml">g</mi><mn id="S3.E4.m1.2.2.1.1.3.3.4.3" xref="S3.E4.m1.2.2.1.1.3.3.4.3.cmml">2</mn></msub><mo id="S3.E4.m1.2.2.1.1.3.3.1b" xref="S3.E4.m1.2.2.1.1.3.3.1.cmml">â¢</mo><mrow id="S3.E4.m1.2.2.1.1.3.3.5.2" xref="S3.E4.m1.2.2.1.1.3.3.cmml"><mo stretchy="false" id="S3.E4.m1.2.2.1.1.3.3.5.2.1" xref="S3.E4.m1.2.2.1.1.3.3.cmml">(</mo><mi id="S3.E4.m1.1.1" xref="S3.E4.m1.1.1.cmml">V</mi><mo stretchy="false" id="S3.E4.m1.2.2.1.1.3.3.5.2.2" xref="S3.E4.m1.2.2.1.1.3.3.cmml">)</mo></mrow></mrow></mrow></mrow><mo id="S3.E4.m1.2.2.1.2" xref="S3.E4.m1.2.2.1.1.cmml">.</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.E4.m1.2b"><apply id="S3.E4.m1.2.2.1.1.cmml" xref="S3.E4.m1.2.2.1"><eq id="S3.E4.m1.2.2.1.1.1.cmml" xref="S3.E4.m1.2.2.1.1.1"></eq><ci id="S3.E4.m1.2.2.1.1.2.cmml" xref="S3.E4.m1.2.2.1.1.2">ğ‘„</ci><apply id="S3.E4.m1.2.2.1.1.3.cmml" xref="S3.E4.m1.2.2.1.1.3"><plus id="S3.E4.m1.2.2.1.1.3.1.cmml" xref="S3.E4.m1.2.2.1.1.3.1"></plus><apply id="S3.E4.m1.2.2.1.1.3.2.cmml" xref="S3.E4.m1.2.2.1.1.3.2"><times id="S3.E4.m1.2.2.1.1.3.2.1.cmml" xref="S3.E4.m1.2.2.1.1.3.2.1"></times><ci id="S3.E4.m1.2.2.1.1.3.2.2.cmml" xref="S3.E4.m1.2.2.1.1.3.2.2">ğ‘</ci><ci id="S3.E4.m1.2.2.1.1.3.2.3.cmml" xref="S3.E4.m1.2.2.1.1.3.2.3">ğ·</ci></apply><apply id="S3.E4.m1.2.2.1.1.3.3.cmml" xref="S3.E4.m1.2.2.1.1.3.3"><times id="S3.E4.m1.2.2.1.1.3.3.1.cmml" xref="S3.E4.m1.2.2.1.1.3.3.1"></times><apply id="S3.E4.m1.2.2.1.1.3.3.2.cmml" xref="S3.E4.m1.2.2.1.1.3.3.2"><times id="S3.E4.m1.2.2.1.1.3.3.2.1.cmml" xref="S3.E4.m1.2.2.1.1.3.3.2.1"></times><ci id="S3.E4.m1.2.2.1.1.3.3.2.2.cmml" xref="S3.E4.m1.2.2.1.1.3.3.2.2">ğ·</ci><ci id="S3.E4.m1.2.2.1.1.3.3.2.3.cmml" xref="S3.E4.m1.2.2.1.1.3.3.2.3">ğ‘™</ci></apply><ci id="S3.E4.m1.2.2.1.1.3.3.3.cmml" xref="S3.E4.m1.2.2.1.1.3.3.3">ğ‘œ</ci><apply id="S3.E4.m1.2.2.1.1.3.3.4.cmml" xref="S3.E4.m1.2.2.1.1.3.3.4"><csymbol cd="ambiguous" id="S3.E4.m1.2.2.1.1.3.3.4.1.cmml" xref="S3.E4.m1.2.2.1.1.3.3.4">subscript</csymbol><ci id="S3.E4.m1.2.2.1.1.3.3.4.2.cmml" xref="S3.E4.m1.2.2.1.1.3.3.4.2">ğ‘”</ci><cn type="integer" id="S3.E4.m1.2.2.1.1.3.3.4.3.cmml" xref="S3.E4.m1.2.2.1.1.3.3.4.3">2</cn></apply><ci id="S3.E4.m1.1.1.cmml" xref="S3.E4.m1.1.1">ğ‘‰</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E4.m1.2c">Q=N\times D+D\times log_{2}(V).</annotation><annotation encoding="application/x-llamapun" id="S3.E4.m1.2d">italic_Q = italic_N Ã— italic_D + italic_D Ã— italic_l italic_o italic_g start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT ( italic_V ) .</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(4)</span></td>
</tr>
</table>
<p id="S3.SS1.p1.2" class="ltx_p">We denote this model further as CBOW, as unlike standard bag-of-words model, it uses continuous distributed representation of the context. The model architecture is shown at FigureÂ <a href="#S3.F1" title="Figure 1 â€£ 3.1 Continuous Bag-of-Words Model â€£ 3 New Log-linear Models â€£ Efficient Estimation of Word Representations in Vector Space" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>. Note that the weight
matrix between the input and the projection layer is shared for all word positions in the same way as in the NNLM.</p>
</div>
<figure id="S3.F1" class="ltx_figure">
<p id="S3.F1.1" class="ltx_p ltx_align_center ltx_align_center"><span id="S3.F1.1.1" class="ltx_text"><img src="/html/1301.3781/assets/x1.png" id="S3.F1.1.1.g1" class="ltx_graphics ltx_img_landscape" width="708" height="500" alt="New model architectures. The CBOW architecture predicts the current word based on the context, and the Skip-gram predicts surrounding words given the current word."></span></p>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>New model architectures. The CBOW architecture predicts the current word based on the context, and the Skip-gram predicts surrounding words given the current word.</figcaption>
</figure>
</section>
<section id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>Continuous Skip-gram Model</h3>

<div id="S3.SS2.p1" class="ltx_para ltx_noindent">
<p id="S3.SS2.p1.1" class="ltx_p">The second architecture is similar to CBOW, but instead of predicting the current word based on the context, it tries to maximize classification of a word based on another word in the same sentence. More precisely, we use each current
word as an input to a log-linear classifier with continuous projection layer, and predict words within a certain range before and after the current word. We found that increasing the range improves quality of the resulting word vectors,
but it also increases the computational complexity. Since the more distant words are usually less related to the current word than those close to it, we give less weight to the distant words by sampling less from
those words in our training examples.</p>
</div>
<div id="S3.SS2.p2" class="ltx_para ltx_noindent">
<p id="S3.SS2.p2.10" class="ltx_p">The training complexity of this architecture is proportional to</p>
<table id="S3.E5" class="ltx_equation ltx_eqn_table">

<tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S3.E5.m1.2" class="ltx_Math" alttext="Q=C\times(D+D\times log_{2}(V))," display="block"><semantics id="S3.E5.m1.2a"><mrow id="S3.E5.m1.2.2.1" xref="S3.E5.m1.2.2.1.1.cmml"><mrow id="S3.E5.m1.2.2.1.1" xref="S3.E5.m1.2.2.1.1.cmml"><mi id="S3.E5.m1.2.2.1.1.3" xref="S3.E5.m1.2.2.1.1.3.cmml">Q</mi><mo id="S3.E5.m1.2.2.1.1.2" xref="S3.E5.m1.2.2.1.1.2.cmml">=</mo><mrow id="S3.E5.m1.2.2.1.1.1" xref="S3.E5.m1.2.2.1.1.1.cmml"><mi id="S3.E5.m1.2.2.1.1.1.3" xref="S3.E5.m1.2.2.1.1.1.3.cmml">C</mi><mo id="S3.E5.m1.2.2.1.1.1.2" xref="S3.E5.m1.2.2.1.1.1.2.cmml">Ã—</mo><mrow id="S3.E5.m1.2.2.1.1.1.1.1" xref="S3.E5.m1.2.2.1.1.1.1.1.1.cmml"><mo stretchy="false" id="S3.E5.m1.2.2.1.1.1.1.1.2" xref="S3.E5.m1.2.2.1.1.1.1.1.1.cmml">(</mo><mrow id="S3.E5.m1.2.2.1.1.1.1.1.1" xref="S3.E5.m1.2.2.1.1.1.1.1.1.cmml"><mi id="S3.E5.m1.2.2.1.1.1.1.1.1.2" xref="S3.E5.m1.2.2.1.1.1.1.1.1.2.cmml">D</mi><mo id="S3.E5.m1.2.2.1.1.1.1.1.1.1" xref="S3.E5.m1.2.2.1.1.1.1.1.1.1.cmml">+</mo><mrow id="S3.E5.m1.2.2.1.1.1.1.1.1.3" xref="S3.E5.m1.2.2.1.1.1.1.1.1.3.cmml"><mrow id="S3.E5.m1.2.2.1.1.1.1.1.1.3.2" xref="S3.E5.m1.2.2.1.1.1.1.1.1.3.2.cmml"><mi id="S3.E5.m1.2.2.1.1.1.1.1.1.3.2.2" xref="S3.E5.m1.2.2.1.1.1.1.1.1.3.2.2.cmml">D</mi><mo id="S3.E5.m1.2.2.1.1.1.1.1.1.3.2.1" xref="S3.E5.m1.2.2.1.1.1.1.1.1.3.2.1.cmml">Ã—</mo><mi id="S3.E5.m1.2.2.1.1.1.1.1.1.3.2.3" xref="S3.E5.m1.2.2.1.1.1.1.1.1.3.2.3.cmml">l</mi></mrow><mo id="S3.E5.m1.2.2.1.1.1.1.1.1.3.1" xref="S3.E5.m1.2.2.1.1.1.1.1.1.3.1.cmml">â¢</mo><mi id="S3.E5.m1.2.2.1.1.1.1.1.1.3.3" xref="S3.E5.m1.2.2.1.1.1.1.1.1.3.3.cmml">o</mi><mo id="S3.E5.m1.2.2.1.1.1.1.1.1.3.1a" xref="S3.E5.m1.2.2.1.1.1.1.1.1.3.1.cmml">â¢</mo><msub id="S3.E5.m1.2.2.1.1.1.1.1.1.3.4" xref="S3.E5.m1.2.2.1.1.1.1.1.1.3.4.cmml"><mi id="S3.E5.m1.2.2.1.1.1.1.1.1.3.4.2" xref="S3.E5.m1.2.2.1.1.1.1.1.1.3.4.2.cmml">g</mi><mn id="S3.E5.m1.2.2.1.1.1.1.1.1.3.4.3" xref="S3.E5.m1.2.2.1.1.1.1.1.1.3.4.3.cmml">2</mn></msub><mo id="S3.E5.m1.2.2.1.1.1.1.1.1.3.1b" xref="S3.E5.m1.2.2.1.1.1.1.1.1.3.1.cmml">â¢</mo><mrow id="S3.E5.m1.2.2.1.1.1.1.1.1.3.5.2" xref="S3.E5.m1.2.2.1.1.1.1.1.1.3.cmml"><mo stretchy="false" id="S3.E5.m1.2.2.1.1.1.1.1.1.3.5.2.1" xref="S3.E5.m1.2.2.1.1.1.1.1.1.3.cmml">(</mo><mi id="S3.E5.m1.1.1" xref="S3.E5.m1.1.1.cmml">V</mi><mo stretchy="false" id="S3.E5.m1.2.2.1.1.1.1.1.1.3.5.2.2" xref="S3.E5.m1.2.2.1.1.1.1.1.1.3.cmml">)</mo></mrow></mrow></mrow><mo stretchy="false" id="S3.E5.m1.2.2.1.1.1.1.1.3" xref="S3.E5.m1.2.2.1.1.1.1.1.1.cmml">)</mo></mrow></mrow></mrow><mo id="S3.E5.m1.2.2.1.2" xref="S3.E5.m1.2.2.1.1.cmml">,</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.E5.m1.2b"><apply id="S3.E5.m1.2.2.1.1.cmml" xref="S3.E5.m1.2.2.1"><eq id="S3.E5.m1.2.2.1.1.2.cmml" xref="S3.E5.m1.2.2.1.1.2"></eq><ci id="S3.E5.m1.2.2.1.1.3.cmml" xref="S3.E5.m1.2.2.1.1.3">ğ‘„</ci><apply id="S3.E5.m1.2.2.1.1.1.cmml" xref="S3.E5.m1.2.2.1.1.1"><times id="S3.E5.m1.2.2.1.1.1.2.cmml" xref="S3.E5.m1.2.2.1.1.1.2"></times><ci id="S3.E5.m1.2.2.1.1.1.3.cmml" xref="S3.E5.m1.2.2.1.1.1.3">ğ¶</ci><apply id="S3.E5.m1.2.2.1.1.1.1.1.1.cmml" xref="S3.E5.m1.2.2.1.1.1.1.1"><plus id="S3.E5.m1.2.2.1.1.1.1.1.1.1.cmml" xref="S3.E5.m1.2.2.1.1.1.1.1.1.1"></plus><ci id="S3.E5.m1.2.2.1.1.1.1.1.1.2.cmml" xref="S3.E5.m1.2.2.1.1.1.1.1.1.2">ğ·</ci><apply id="S3.E5.m1.2.2.1.1.1.1.1.1.3.cmml" xref="S3.E5.m1.2.2.1.1.1.1.1.1.3"><times id="S3.E5.m1.2.2.1.1.1.1.1.1.3.1.cmml" xref="S3.E5.m1.2.2.1.1.1.1.1.1.3.1"></times><apply id="S3.E5.m1.2.2.1.1.1.1.1.1.3.2.cmml" xref="S3.E5.m1.2.2.1.1.1.1.1.1.3.2"><times id="S3.E5.m1.2.2.1.1.1.1.1.1.3.2.1.cmml" xref="S3.E5.m1.2.2.1.1.1.1.1.1.3.2.1"></times><ci id="S3.E5.m1.2.2.1.1.1.1.1.1.3.2.2.cmml" xref="S3.E5.m1.2.2.1.1.1.1.1.1.3.2.2">ğ·</ci><ci id="S3.E5.m1.2.2.1.1.1.1.1.1.3.2.3.cmml" xref="S3.E5.m1.2.2.1.1.1.1.1.1.3.2.3">ğ‘™</ci></apply><ci id="S3.E5.m1.2.2.1.1.1.1.1.1.3.3.cmml" xref="S3.E5.m1.2.2.1.1.1.1.1.1.3.3">ğ‘œ</ci><apply id="S3.E5.m1.2.2.1.1.1.1.1.1.3.4.cmml" xref="S3.E5.m1.2.2.1.1.1.1.1.1.3.4"><csymbol cd="ambiguous" id="S3.E5.m1.2.2.1.1.1.1.1.1.3.4.1.cmml" xref="S3.E5.m1.2.2.1.1.1.1.1.1.3.4">subscript</csymbol><ci id="S3.E5.m1.2.2.1.1.1.1.1.1.3.4.2.cmml" xref="S3.E5.m1.2.2.1.1.1.1.1.1.3.4.2">ğ‘”</ci><cn type="integer" id="S3.E5.m1.2.2.1.1.1.1.1.1.3.4.3.cmml" xref="S3.E5.m1.2.2.1.1.1.1.1.1.3.4.3">2</cn></apply><ci id="S3.E5.m1.1.1.cmml" xref="S3.E5.m1.1.1">ğ‘‰</ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E5.m1.2c">Q=C\times(D+D\times log_{2}(V)),</annotation><annotation encoding="application/x-llamapun" id="S3.E5.m1.2d">italic_Q = italic_C Ã— ( italic_D + italic_D Ã— italic_l italic_o italic_g start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT ( italic_V ) ) ,</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(5)</span></td>
</tr>
</table>
<p id="S3.SS2.p2.9" class="ltx_p">where <math id="S3.SS2.p2.1.m1.1" class="ltx_Math" alttext="C" display="inline"><semantics id="S3.SS2.p2.1.m1.1a"><mi id="S3.SS2.p2.1.m1.1.1" xref="S3.SS2.p2.1.m1.1.1.cmml">C</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.1.m1.1b"><ci id="S3.SS2.p2.1.m1.1.1.cmml" xref="S3.SS2.p2.1.m1.1.1">ğ¶</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.1.m1.1c">C</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p2.1.m1.1d">italic_C</annotation></semantics></math> is the maximum distance of the words. Thus, if we choose <math id="S3.SS2.p2.2.m2.1" class="ltx_Math" alttext="C=5" display="inline"><semantics id="S3.SS2.p2.2.m2.1a"><mrow id="S3.SS2.p2.2.m2.1.1" xref="S3.SS2.p2.2.m2.1.1.cmml"><mi id="S3.SS2.p2.2.m2.1.1.2" xref="S3.SS2.p2.2.m2.1.1.2.cmml">C</mi><mo id="S3.SS2.p2.2.m2.1.1.1" xref="S3.SS2.p2.2.m2.1.1.1.cmml">=</mo><mn id="S3.SS2.p2.2.m2.1.1.3" xref="S3.SS2.p2.2.m2.1.1.3.cmml">5</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.2.m2.1b"><apply id="S3.SS2.p2.2.m2.1.1.cmml" xref="S3.SS2.p2.2.m2.1.1"><eq id="S3.SS2.p2.2.m2.1.1.1.cmml" xref="S3.SS2.p2.2.m2.1.1.1"></eq><ci id="S3.SS2.p2.2.m2.1.1.2.cmml" xref="S3.SS2.p2.2.m2.1.1.2">ğ¶</ci><cn type="integer" id="S3.SS2.p2.2.m2.1.1.3.cmml" xref="S3.SS2.p2.2.m2.1.1.3">5</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.2.m2.1c">C=5</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p2.2.m2.1d">italic_C = 5</annotation></semantics></math>, for each training word we will select randomly a number <math id="S3.SS2.p2.3.m3.1" class="ltx_Math" alttext="R" display="inline"><semantics id="S3.SS2.p2.3.m3.1a"><mi id="S3.SS2.p2.3.m3.1.1" xref="S3.SS2.p2.3.m3.1.1.cmml">R</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.3.m3.1b"><ci id="S3.SS2.p2.3.m3.1.1.cmml" xref="S3.SS2.p2.3.m3.1.1">ğ‘…</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.3.m3.1c">R</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p2.3.m3.1d">italic_R</annotation></semantics></math> in range <math id="S3.SS2.p2.4.m4.2" class="ltx_Math" alttext="&lt;1;C&gt;" display="inline"><semantics id="S3.SS2.p2.4.m4.2a"><mrow id="S3.SS2.p2.4.m4.2b"><mo id="S3.SS2.p2.4.m4.2.3" xref="S3.SS2.p2.4.m4.2.3.cmml">&lt;</mo><mn id="S3.SS2.p2.4.m4.1.1" xref="S3.SS2.p2.4.m4.1.1.cmml">1</mn><mo id="S3.SS2.p2.4.m4.2.4" xref="S3.SS2.p2.4.m4.2.4.cmml">;</mo><mi id="S3.SS2.p2.4.m4.2.2" xref="S3.SS2.p2.4.m4.2.2.cmml">C</mi><mo id="S3.SS2.p2.4.m4.2.5" xref="S3.SS2.p2.4.m4.2.5.cmml">&gt;</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.4.m4.2c"><cerror id="S3.SS2.p2.4.m4.2d"><csymbol cd="ambiguous" id="S3.SS2.p2.4.m4.2e">fragments</csymbol><lt id="S3.SS2.p2.4.m4.2.3.cmml" xref="S3.SS2.p2.4.m4.2.3"></lt><cn type="integer" id="S3.SS2.p2.4.m4.1.1.cmml" xref="S3.SS2.p2.4.m4.1.1">1</cn><ci id="S3.SS2.p2.4.m4.2.4.cmml" xref="S3.SS2.p2.4.m4.2.4">;</ci><csymbol cd="unknown" id="S3.SS2.p2.4.m4.2.2.cmml" xref="S3.SS2.p2.4.m4.2.2">C</csymbol><gt id="S3.SS2.p2.4.m4.2.5.cmml" xref="S3.SS2.p2.4.m4.2.5"></gt></cerror></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.4.m4.2f">&lt;1;C&gt;</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p2.4.m4.2g">&lt; 1 ; italic_C &gt;</annotation></semantics></math>, and then use <math id="S3.SS2.p2.5.m5.1" class="ltx_Math" alttext="R" display="inline"><semantics id="S3.SS2.p2.5.m5.1a"><mi id="S3.SS2.p2.5.m5.1.1" xref="S3.SS2.p2.5.m5.1.1.cmml">R</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.5.m5.1b"><ci id="S3.SS2.p2.5.m5.1.1.cmml" xref="S3.SS2.p2.5.m5.1.1">ğ‘…</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.5.m5.1c">R</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p2.5.m5.1d">italic_R</annotation></semantics></math> words from history and <math id="S3.SS2.p2.6.m6.1" class="ltx_Math" alttext="R" display="inline"><semantics id="S3.SS2.p2.6.m6.1a"><mi id="S3.SS2.p2.6.m6.1.1" xref="S3.SS2.p2.6.m6.1.1.cmml">R</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.6.m6.1b"><ci id="S3.SS2.p2.6.m6.1.1.cmml" xref="S3.SS2.p2.6.m6.1.1">ğ‘…</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.6.m6.1c">R</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p2.6.m6.1d">italic_R</annotation></semantics></math> words
from the future of the current word as correct labels. This will require us to do <math id="S3.SS2.p2.7.m7.1" class="ltx_Math" alttext="R\times 2" display="inline"><semantics id="S3.SS2.p2.7.m7.1a"><mrow id="S3.SS2.p2.7.m7.1.1" xref="S3.SS2.p2.7.m7.1.1.cmml"><mi id="S3.SS2.p2.7.m7.1.1.2" xref="S3.SS2.p2.7.m7.1.1.2.cmml">R</mi><mo id="S3.SS2.p2.7.m7.1.1.1" xref="S3.SS2.p2.7.m7.1.1.1.cmml">Ã—</mo><mn id="S3.SS2.p2.7.m7.1.1.3" xref="S3.SS2.p2.7.m7.1.1.3.cmml">2</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.7.m7.1b"><apply id="S3.SS2.p2.7.m7.1.1.cmml" xref="S3.SS2.p2.7.m7.1.1"><times id="S3.SS2.p2.7.m7.1.1.1.cmml" xref="S3.SS2.p2.7.m7.1.1.1"></times><ci id="S3.SS2.p2.7.m7.1.1.2.cmml" xref="S3.SS2.p2.7.m7.1.1.2">ğ‘…</ci><cn type="integer" id="S3.SS2.p2.7.m7.1.1.3.cmml" xref="S3.SS2.p2.7.m7.1.1.3">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.7.m7.1c">R\times 2</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p2.7.m7.1d">italic_R Ã— 2</annotation></semantics></math> word classifications, with the current word as input, and each of the <math id="S3.SS2.p2.8.m8.1" class="ltx_Math" alttext="R+R" display="inline"><semantics id="S3.SS2.p2.8.m8.1a"><mrow id="S3.SS2.p2.8.m8.1.1" xref="S3.SS2.p2.8.m8.1.1.cmml"><mi id="S3.SS2.p2.8.m8.1.1.2" xref="S3.SS2.p2.8.m8.1.1.2.cmml">R</mi><mo id="S3.SS2.p2.8.m8.1.1.1" xref="S3.SS2.p2.8.m8.1.1.1.cmml">+</mo><mi id="S3.SS2.p2.8.m8.1.1.3" xref="S3.SS2.p2.8.m8.1.1.3.cmml">R</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.8.m8.1b"><apply id="S3.SS2.p2.8.m8.1.1.cmml" xref="S3.SS2.p2.8.m8.1.1"><plus id="S3.SS2.p2.8.m8.1.1.1.cmml" xref="S3.SS2.p2.8.m8.1.1.1"></plus><ci id="S3.SS2.p2.8.m8.1.1.2.cmml" xref="S3.SS2.p2.8.m8.1.1.2">ğ‘…</ci><ci id="S3.SS2.p2.8.m8.1.1.3.cmml" xref="S3.SS2.p2.8.m8.1.1.3">ğ‘…</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.8.m8.1c">R+R</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p2.8.m8.1d">italic_R + italic_R</annotation></semantics></math> words as output.
In the following experiments, we use <math id="S3.SS2.p2.9.m9.1" class="ltx_Math" alttext="C=10" display="inline"><semantics id="S3.SS2.p2.9.m9.1a"><mrow id="S3.SS2.p2.9.m9.1.1" xref="S3.SS2.p2.9.m9.1.1.cmml"><mi id="S3.SS2.p2.9.m9.1.1.2" xref="S3.SS2.p2.9.m9.1.1.2.cmml">C</mi><mo id="S3.SS2.p2.9.m9.1.1.1" xref="S3.SS2.p2.9.m9.1.1.1.cmml">=</mo><mn id="S3.SS2.p2.9.m9.1.1.3" xref="S3.SS2.p2.9.m9.1.1.3.cmml">10</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.9.m9.1b"><apply id="S3.SS2.p2.9.m9.1.1.cmml" xref="S3.SS2.p2.9.m9.1.1"><eq id="S3.SS2.p2.9.m9.1.1.1.cmml" xref="S3.SS2.p2.9.m9.1.1.1"></eq><ci id="S3.SS2.p2.9.m9.1.1.2.cmml" xref="S3.SS2.p2.9.m9.1.1.2">ğ¶</ci><cn type="integer" id="S3.SS2.p2.9.m9.1.1.3.cmml" xref="S3.SS2.p2.9.m9.1.1.3">10</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.9.m9.1c">C=10</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p2.9.m9.1d">italic_C = 10</annotation></semantics></math>.</p>
</div>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Results</h2>

<div id="S4.p1" class="ltx_para ltx_noindent">
<p id="S4.p1.1" class="ltx_p">To compare the quality of different versions of word vectors, previous papers typically use a table showing example words and their most similar words, and understand them intuitively.
Although it is easy to show that word <span id="S4.p1.1.1" class="ltx_text ltx_font_italic">France</span> is similar to <span id="S4.p1.1.2" class="ltx_text ltx_font_italic">Italy</span> and perhaps some other countries, it is much more challenging when subjecting those vectors in a more complex similarity task, as follows.
We follow previous observation that there can be many different types of similarities between words, for example, word <span id="S4.p1.1.3" class="ltx_text ltx_font_italic">big</span> is similar to <span id="S4.p1.1.4" class="ltx_text ltx_font_italic">bigger</span> in the same sense that
<span id="S4.p1.1.5" class="ltx_text ltx_font_italic">small</span> is similar to <span id="S4.p1.1.6" class="ltx_text ltx_font_italic">smaller</span>. Example of another type of relationship can be word pairs <span id="S4.p1.1.7" class="ltx_text ltx_font_italic">big - biggest</span> and <span id="S4.p1.1.8" class="ltx_text ltx_font_italic">small - smallest</span>Â <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib20" title="" class="ltx_ref">20</a>]</cite>. We further denote two pairs of words with the same relationship
as a question, as we can ask: â€What is the word that is similar to <span id="S4.p1.1.9" class="ltx_text ltx_font_italic">small</span> in the same sense as <span id="S4.p1.1.10" class="ltx_text ltx_font_italic">biggest</span> is similar to <span id="S4.p1.1.11" class="ltx_text ltx_font_italic">big</span>?â€
</p>
</div>
<div id="S4.p2" class="ltx_para ltx_noindent">
<p id="S4.p2.3" class="ltx_p">Somewhat surprisingly, these questions can be answered
by performing simple algebraic operations with the vector representation of words. To find a word that is similar to <span id="S4.p2.3.1" class="ltx_text ltx_font_italic">small</span> in the same sense as <span id="S4.p2.3.2" class="ltx_text ltx_font_italic">biggest</span> is similar to <span id="S4.p2.3.3" class="ltx_text ltx_font_italic">big</span>, we can simply compute
vector <math id="S4.p2.1.m1.3" class="ltx_Math" alttext='X=vector("biggest")-vector("big")+vector("small")' display="inline"><semantics id="S4.p2.1.m1.3a"><mrow id="S4.p2.1.m1.3.3" xref="S4.p2.1.m1.3.3.cmml"><mi id="S4.p2.1.m1.3.3.5" xref="S4.p2.1.m1.3.3.5.cmml">X</mi><mo id="S4.p2.1.m1.3.3.4" xref="S4.p2.1.m1.3.3.4.cmml">=</mo><mrow id="S4.p2.1.m1.3.3.3" xref="S4.p2.1.m1.3.3.3.cmml"><mrow id="S4.p2.1.m1.2.2.2.2" xref="S4.p2.1.m1.2.2.2.2.cmml"><mrow id="S4.p2.1.m1.1.1.1.1.1" xref="S4.p2.1.m1.1.1.1.1.1.cmml"><mi id="S4.p2.1.m1.1.1.1.1.1.3" xref="S4.p2.1.m1.1.1.1.1.1.3.cmml">v</mi><mo id="S4.p2.1.m1.1.1.1.1.1.2" xref="S4.p2.1.m1.1.1.1.1.1.2.cmml">â¢</mo><mi id="S4.p2.1.m1.1.1.1.1.1.4" xref="S4.p2.1.m1.1.1.1.1.1.4.cmml">e</mi><mo id="S4.p2.1.m1.1.1.1.1.1.2a" xref="S4.p2.1.m1.1.1.1.1.1.2.cmml">â¢</mo><mi id="S4.p2.1.m1.1.1.1.1.1.5" xref="S4.p2.1.m1.1.1.1.1.1.5.cmml">c</mi><mo id="S4.p2.1.m1.1.1.1.1.1.2b" xref="S4.p2.1.m1.1.1.1.1.1.2.cmml">â¢</mo><mi id="S4.p2.1.m1.1.1.1.1.1.6" xref="S4.p2.1.m1.1.1.1.1.1.6.cmml">t</mi><mo id="S4.p2.1.m1.1.1.1.1.1.2c" xref="S4.p2.1.m1.1.1.1.1.1.2.cmml">â¢</mo><mi id="S4.p2.1.m1.1.1.1.1.1.7" xref="S4.p2.1.m1.1.1.1.1.1.7.cmml">o</mi><mo id="S4.p2.1.m1.1.1.1.1.1.2d" xref="S4.p2.1.m1.1.1.1.1.1.2.cmml">â¢</mo><mi id="S4.p2.1.m1.1.1.1.1.1.8" xref="S4.p2.1.m1.1.1.1.1.1.8.cmml">r</mi><mo id="S4.p2.1.m1.1.1.1.1.1.2e" xref="S4.p2.1.m1.1.1.1.1.1.2.cmml">â¢</mo><mrow id="S4.p2.1.m1.1.1.1.1.1.1.1" xref="S4.p2.1.m1.1.1.1.1.1.1.1.1.cmml"><mo stretchy="false" id="S4.p2.1.m1.1.1.1.1.1.1.1.2" xref="S4.p2.1.m1.1.1.1.1.1.1.1.1.cmml">(</mo><mrow id="S4.p2.1.m1.1.1.1.1.1.1.1.1" xref="S4.p2.1.m1.1.1.1.1.1.1.1.1.cmml"><mi mathvariant="normal" id="S4.p2.1.m1.1.1.1.1.1.1.1.1.2" xref="S4.p2.1.m1.1.1.1.1.1.1.1.1.2.cmml">"</mi><mo id="S4.p2.1.m1.1.1.1.1.1.1.1.1.1" xref="S4.p2.1.m1.1.1.1.1.1.1.1.1.1.cmml">â¢</mo><mi id="S4.p2.1.m1.1.1.1.1.1.1.1.1.3" xref="S4.p2.1.m1.1.1.1.1.1.1.1.1.3.cmml">b</mi><mo id="S4.p2.1.m1.1.1.1.1.1.1.1.1.1a" xref="S4.p2.1.m1.1.1.1.1.1.1.1.1.1.cmml">â¢</mo><mi id="S4.p2.1.m1.1.1.1.1.1.1.1.1.4" xref="S4.p2.1.m1.1.1.1.1.1.1.1.1.4.cmml">i</mi><mo id="S4.p2.1.m1.1.1.1.1.1.1.1.1.1b" xref="S4.p2.1.m1.1.1.1.1.1.1.1.1.1.cmml">â¢</mo><mi id="S4.p2.1.m1.1.1.1.1.1.1.1.1.5" xref="S4.p2.1.m1.1.1.1.1.1.1.1.1.5.cmml">g</mi><mo id="S4.p2.1.m1.1.1.1.1.1.1.1.1.1c" xref="S4.p2.1.m1.1.1.1.1.1.1.1.1.1.cmml">â¢</mo><mi id="S4.p2.1.m1.1.1.1.1.1.1.1.1.6" xref="S4.p2.1.m1.1.1.1.1.1.1.1.1.6.cmml">g</mi><mo id="S4.p2.1.m1.1.1.1.1.1.1.1.1.1d" xref="S4.p2.1.m1.1.1.1.1.1.1.1.1.1.cmml">â¢</mo><mi id="S4.p2.1.m1.1.1.1.1.1.1.1.1.7" xref="S4.p2.1.m1.1.1.1.1.1.1.1.1.7.cmml">e</mi><mo id="S4.p2.1.m1.1.1.1.1.1.1.1.1.1e" xref="S4.p2.1.m1.1.1.1.1.1.1.1.1.1.cmml">â¢</mo><mi id="S4.p2.1.m1.1.1.1.1.1.1.1.1.8" xref="S4.p2.1.m1.1.1.1.1.1.1.1.1.8.cmml">s</mi><mo id="S4.p2.1.m1.1.1.1.1.1.1.1.1.1f" xref="S4.p2.1.m1.1.1.1.1.1.1.1.1.1.cmml">â¢</mo><mi id="S4.p2.1.m1.1.1.1.1.1.1.1.1.9" xref="S4.p2.1.m1.1.1.1.1.1.1.1.1.9.cmml">t</mi><mo id="S4.p2.1.m1.1.1.1.1.1.1.1.1.1g" xref="S4.p2.1.m1.1.1.1.1.1.1.1.1.1.cmml">â¢</mo><mi mathvariant="normal" id="S4.p2.1.m1.1.1.1.1.1.1.1.1.10" xref="S4.p2.1.m1.1.1.1.1.1.1.1.1.10.cmml">"</mi></mrow><mo stretchy="false" id="S4.p2.1.m1.1.1.1.1.1.1.1.3" xref="S4.p2.1.m1.1.1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow><mo id="S4.p2.1.m1.2.2.2.2.3" xref="S4.p2.1.m1.2.2.2.2.3.cmml">-</mo><mrow id="S4.p2.1.m1.2.2.2.2.2" xref="S4.p2.1.m1.2.2.2.2.2.cmml"><mi id="S4.p2.1.m1.2.2.2.2.2.3" xref="S4.p2.1.m1.2.2.2.2.2.3.cmml">v</mi><mo id="S4.p2.1.m1.2.2.2.2.2.2" xref="S4.p2.1.m1.2.2.2.2.2.2.cmml">â¢</mo><mi id="S4.p2.1.m1.2.2.2.2.2.4" xref="S4.p2.1.m1.2.2.2.2.2.4.cmml">e</mi><mo id="S4.p2.1.m1.2.2.2.2.2.2a" xref="S4.p2.1.m1.2.2.2.2.2.2.cmml">â¢</mo><mi id="S4.p2.1.m1.2.2.2.2.2.5" xref="S4.p2.1.m1.2.2.2.2.2.5.cmml">c</mi><mo id="S4.p2.1.m1.2.2.2.2.2.2b" xref="S4.p2.1.m1.2.2.2.2.2.2.cmml">â¢</mo><mi id="S4.p2.1.m1.2.2.2.2.2.6" xref="S4.p2.1.m1.2.2.2.2.2.6.cmml">t</mi><mo id="S4.p2.1.m1.2.2.2.2.2.2c" xref="S4.p2.1.m1.2.2.2.2.2.2.cmml">â¢</mo><mi id="S4.p2.1.m1.2.2.2.2.2.7" xref="S4.p2.1.m1.2.2.2.2.2.7.cmml">o</mi><mo id="S4.p2.1.m1.2.2.2.2.2.2d" xref="S4.p2.1.m1.2.2.2.2.2.2.cmml">â¢</mo><mi id="S4.p2.1.m1.2.2.2.2.2.8" xref="S4.p2.1.m1.2.2.2.2.2.8.cmml">r</mi><mo id="S4.p2.1.m1.2.2.2.2.2.2e" xref="S4.p2.1.m1.2.2.2.2.2.2.cmml">â¢</mo><mrow id="S4.p2.1.m1.2.2.2.2.2.1.1" xref="S4.p2.1.m1.2.2.2.2.2.1.1.1.cmml"><mo stretchy="false" id="S4.p2.1.m1.2.2.2.2.2.1.1.2" xref="S4.p2.1.m1.2.2.2.2.2.1.1.1.cmml">(</mo><mrow id="S4.p2.1.m1.2.2.2.2.2.1.1.1" xref="S4.p2.1.m1.2.2.2.2.2.1.1.1.cmml"><mi mathvariant="normal" id="S4.p2.1.m1.2.2.2.2.2.1.1.1.2" xref="S4.p2.1.m1.2.2.2.2.2.1.1.1.2.cmml">"</mi><mo id="S4.p2.1.m1.2.2.2.2.2.1.1.1.1" xref="S4.p2.1.m1.2.2.2.2.2.1.1.1.1.cmml">â¢</mo><mi id="S4.p2.1.m1.2.2.2.2.2.1.1.1.3" xref="S4.p2.1.m1.2.2.2.2.2.1.1.1.3.cmml">b</mi><mo id="S4.p2.1.m1.2.2.2.2.2.1.1.1.1a" xref="S4.p2.1.m1.2.2.2.2.2.1.1.1.1.cmml">â¢</mo><mi id="S4.p2.1.m1.2.2.2.2.2.1.1.1.4" xref="S4.p2.1.m1.2.2.2.2.2.1.1.1.4.cmml">i</mi><mo id="S4.p2.1.m1.2.2.2.2.2.1.1.1.1b" xref="S4.p2.1.m1.2.2.2.2.2.1.1.1.1.cmml">â¢</mo><mi id="S4.p2.1.m1.2.2.2.2.2.1.1.1.5" xref="S4.p2.1.m1.2.2.2.2.2.1.1.1.5.cmml">g</mi><mo id="S4.p2.1.m1.2.2.2.2.2.1.1.1.1c" xref="S4.p2.1.m1.2.2.2.2.2.1.1.1.1.cmml">â¢</mo><mi mathvariant="normal" id="S4.p2.1.m1.2.2.2.2.2.1.1.1.6" xref="S4.p2.1.m1.2.2.2.2.2.1.1.1.6.cmml">"</mi></mrow><mo stretchy="false" id="S4.p2.1.m1.2.2.2.2.2.1.1.3" xref="S4.p2.1.m1.2.2.2.2.2.1.1.1.cmml">)</mo></mrow></mrow></mrow><mo id="S4.p2.1.m1.3.3.3.4" xref="S4.p2.1.m1.3.3.3.4.cmml">+</mo><mrow id="S4.p2.1.m1.3.3.3.3" xref="S4.p2.1.m1.3.3.3.3.cmml"><mi id="S4.p2.1.m1.3.3.3.3.3" xref="S4.p2.1.m1.3.3.3.3.3.cmml">v</mi><mo id="S4.p2.1.m1.3.3.3.3.2" xref="S4.p2.1.m1.3.3.3.3.2.cmml">â¢</mo><mi id="S4.p2.1.m1.3.3.3.3.4" xref="S4.p2.1.m1.3.3.3.3.4.cmml">e</mi><mo id="S4.p2.1.m1.3.3.3.3.2a" xref="S4.p2.1.m1.3.3.3.3.2.cmml">â¢</mo><mi id="S4.p2.1.m1.3.3.3.3.5" xref="S4.p2.1.m1.3.3.3.3.5.cmml">c</mi><mo id="S4.p2.1.m1.3.3.3.3.2b" xref="S4.p2.1.m1.3.3.3.3.2.cmml">â¢</mo><mi id="S4.p2.1.m1.3.3.3.3.6" xref="S4.p2.1.m1.3.3.3.3.6.cmml">t</mi><mo id="S4.p2.1.m1.3.3.3.3.2c" xref="S4.p2.1.m1.3.3.3.3.2.cmml">â¢</mo><mi id="S4.p2.1.m1.3.3.3.3.7" xref="S4.p2.1.m1.3.3.3.3.7.cmml">o</mi><mo id="S4.p2.1.m1.3.3.3.3.2d" xref="S4.p2.1.m1.3.3.3.3.2.cmml">â¢</mo><mi id="S4.p2.1.m1.3.3.3.3.8" xref="S4.p2.1.m1.3.3.3.3.8.cmml">r</mi><mo id="S4.p2.1.m1.3.3.3.3.2e" xref="S4.p2.1.m1.3.3.3.3.2.cmml">â¢</mo><mrow id="S4.p2.1.m1.3.3.3.3.1.1" xref="S4.p2.1.m1.3.3.3.3.1.1.1.cmml"><mo stretchy="false" id="S4.p2.1.m1.3.3.3.3.1.1.2" xref="S4.p2.1.m1.3.3.3.3.1.1.1.cmml">(</mo><mrow id="S4.p2.1.m1.3.3.3.3.1.1.1" xref="S4.p2.1.m1.3.3.3.3.1.1.1.cmml"><mi mathvariant="normal" id="S4.p2.1.m1.3.3.3.3.1.1.1.2" xref="S4.p2.1.m1.3.3.3.3.1.1.1.2.cmml">"</mi><mo id="S4.p2.1.m1.3.3.3.3.1.1.1.1" xref="S4.p2.1.m1.3.3.3.3.1.1.1.1.cmml">â¢</mo><mi id="S4.p2.1.m1.3.3.3.3.1.1.1.3" xref="S4.p2.1.m1.3.3.3.3.1.1.1.3.cmml">s</mi><mo id="S4.p2.1.m1.3.3.3.3.1.1.1.1a" xref="S4.p2.1.m1.3.3.3.3.1.1.1.1.cmml">â¢</mo><mi id="S4.p2.1.m1.3.3.3.3.1.1.1.4" xref="S4.p2.1.m1.3.3.3.3.1.1.1.4.cmml">m</mi><mo id="S4.p2.1.m1.3.3.3.3.1.1.1.1b" xref="S4.p2.1.m1.3.3.3.3.1.1.1.1.cmml">â¢</mo><mi id="S4.p2.1.m1.3.3.3.3.1.1.1.5" xref="S4.p2.1.m1.3.3.3.3.1.1.1.5.cmml">a</mi><mo id="S4.p2.1.m1.3.3.3.3.1.1.1.1c" xref="S4.p2.1.m1.3.3.3.3.1.1.1.1.cmml">â¢</mo><mi id="S4.p2.1.m1.3.3.3.3.1.1.1.6" xref="S4.p2.1.m1.3.3.3.3.1.1.1.6.cmml">l</mi><mo id="S4.p2.1.m1.3.3.3.3.1.1.1.1d" xref="S4.p2.1.m1.3.3.3.3.1.1.1.1.cmml">â¢</mo><mi id="S4.p2.1.m1.3.3.3.3.1.1.1.7" xref="S4.p2.1.m1.3.3.3.3.1.1.1.7.cmml">l</mi><mo id="S4.p2.1.m1.3.3.3.3.1.1.1.1e" xref="S4.p2.1.m1.3.3.3.3.1.1.1.1.cmml">â¢</mo><mi mathvariant="normal" id="S4.p2.1.m1.3.3.3.3.1.1.1.8" xref="S4.p2.1.m1.3.3.3.3.1.1.1.8.cmml">"</mi></mrow><mo stretchy="false" id="S4.p2.1.m1.3.3.3.3.1.1.3" xref="S4.p2.1.m1.3.3.3.3.1.1.1.cmml">)</mo></mrow></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.p2.1.m1.3b"><apply id="S4.p2.1.m1.3.3.cmml" xref="S4.p2.1.m1.3.3"><eq id="S4.p2.1.m1.3.3.4.cmml" xref="S4.p2.1.m1.3.3.4"></eq><ci id="S4.p2.1.m1.3.3.5.cmml" xref="S4.p2.1.m1.3.3.5">ğ‘‹</ci><apply id="S4.p2.1.m1.3.3.3.cmml" xref="S4.p2.1.m1.3.3.3"><plus id="S4.p2.1.m1.3.3.3.4.cmml" xref="S4.p2.1.m1.3.3.3.4"></plus><apply id="S4.p2.1.m1.2.2.2.2.cmml" xref="S4.p2.1.m1.2.2.2.2"><minus id="S4.p2.1.m1.2.2.2.2.3.cmml" xref="S4.p2.1.m1.2.2.2.2.3"></minus><apply id="S4.p2.1.m1.1.1.1.1.1.cmml" xref="S4.p2.1.m1.1.1.1.1.1"><times id="S4.p2.1.m1.1.1.1.1.1.2.cmml" xref="S4.p2.1.m1.1.1.1.1.1.2"></times><ci id="S4.p2.1.m1.1.1.1.1.1.3.cmml" xref="S4.p2.1.m1.1.1.1.1.1.3">ğ‘£</ci><ci id="S4.p2.1.m1.1.1.1.1.1.4.cmml" xref="S4.p2.1.m1.1.1.1.1.1.4">ğ‘’</ci><ci id="S4.p2.1.m1.1.1.1.1.1.5.cmml" xref="S4.p2.1.m1.1.1.1.1.1.5">ğ‘</ci><ci id="S4.p2.1.m1.1.1.1.1.1.6.cmml" xref="S4.p2.1.m1.1.1.1.1.1.6">ğ‘¡</ci><ci id="S4.p2.1.m1.1.1.1.1.1.7.cmml" xref="S4.p2.1.m1.1.1.1.1.1.7">ğ‘œ</ci><ci id="S4.p2.1.m1.1.1.1.1.1.8.cmml" xref="S4.p2.1.m1.1.1.1.1.1.8">ğ‘Ÿ</ci><apply id="S4.p2.1.m1.1.1.1.1.1.1.1.1.cmml" xref="S4.p2.1.m1.1.1.1.1.1.1.1"><times id="S4.p2.1.m1.1.1.1.1.1.1.1.1.1.cmml" xref="S4.p2.1.m1.1.1.1.1.1.1.1.1.1"></times><ci id="S4.p2.1.m1.1.1.1.1.1.1.1.1.2.cmml" xref="S4.p2.1.m1.1.1.1.1.1.1.1.1.2">"</ci><ci id="S4.p2.1.m1.1.1.1.1.1.1.1.1.3.cmml" xref="S4.p2.1.m1.1.1.1.1.1.1.1.1.3">ğ‘</ci><ci id="S4.p2.1.m1.1.1.1.1.1.1.1.1.4.cmml" xref="S4.p2.1.m1.1.1.1.1.1.1.1.1.4">ğ‘–</ci><ci id="S4.p2.1.m1.1.1.1.1.1.1.1.1.5.cmml" xref="S4.p2.1.m1.1.1.1.1.1.1.1.1.5">ğ‘”</ci><ci id="S4.p2.1.m1.1.1.1.1.1.1.1.1.6.cmml" xref="S4.p2.1.m1.1.1.1.1.1.1.1.1.6">ğ‘”</ci><ci id="S4.p2.1.m1.1.1.1.1.1.1.1.1.7.cmml" xref="S4.p2.1.m1.1.1.1.1.1.1.1.1.7">ğ‘’</ci><ci id="S4.p2.1.m1.1.1.1.1.1.1.1.1.8.cmml" xref="S4.p2.1.m1.1.1.1.1.1.1.1.1.8">ğ‘ </ci><ci id="S4.p2.1.m1.1.1.1.1.1.1.1.1.9.cmml" xref="S4.p2.1.m1.1.1.1.1.1.1.1.1.9">ğ‘¡</ci><ci id="S4.p2.1.m1.1.1.1.1.1.1.1.1.10.cmml" xref="S4.p2.1.m1.1.1.1.1.1.1.1.1.10">"</ci></apply></apply><apply id="S4.p2.1.m1.2.2.2.2.2.cmml" xref="S4.p2.1.m1.2.2.2.2.2"><times id="S4.p2.1.m1.2.2.2.2.2.2.cmml" xref="S4.p2.1.m1.2.2.2.2.2.2"></times><ci id="S4.p2.1.m1.2.2.2.2.2.3.cmml" xref="S4.p2.1.m1.2.2.2.2.2.3">ğ‘£</ci><ci id="S4.p2.1.m1.2.2.2.2.2.4.cmml" xref="S4.p2.1.m1.2.2.2.2.2.4">ğ‘’</ci><ci id="S4.p2.1.m1.2.2.2.2.2.5.cmml" xref="S4.p2.1.m1.2.2.2.2.2.5">ğ‘</ci><ci id="S4.p2.1.m1.2.2.2.2.2.6.cmml" xref="S4.p2.1.m1.2.2.2.2.2.6">ğ‘¡</ci><ci id="S4.p2.1.m1.2.2.2.2.2.7.cmml" xref="S4.p2.1.m1.2.2.2.2.2.7">ğ‘œ</ci><ci id="S4.p2.1.m1.2.2.2.2.2.8.cmml" xref="S4.p2.1.m1.2.2.2.2.2.8">ğ‘Ÿ</ci><apply id="S4.p2.1.m1.2.2.2.2.2.1.1.1.cmml" xref="S4.p2.1.m1.2.2.2.2.2.1.1"><times id="S4.p2.1.m1.2.2.2.2.2.1.1.1.1.cmml" xref="S4.p2.1.m1.2.2.2.2.2.1.1.1.1"></times><ci id="S4.p2.1.m1.2.2.2.2.2.1.1.1.2.cmml" xref="S4.p2.1.m1.2.2.2.2.2.1.1.1.2">"</ci><ci id="S4.p2.1.m1.2.2.2.2.2.1.1.1.3.cmml" xref="S4.p2.1.m1.2.2.2.2.2.1.1.1.3">ğ‘</ci><ci id="S4.p2.1.m1.2.2.2.2.2.1.1.1.4.cmml" xref="S4.p2.1.m1.2.2.2.2.2.1.1.1.4">ğ‘–</ci><ci id="S4.p2.1.m1.2.2.2.2.2.1.1.1.5.cmml" xref="S4.p2.1.m1.2.2.2.2.2.1.1.1.5">ğ‘”</ci><ci id="S4.p2.1.m1.2.2.2.2.2.1.1.1.6.cmml" xref="S4.p2.1.m1.2.2.2.2.2.1.1.1.6">"</ci></apply></apply></apply><apply id="S4.p2.1.m1.3.3.3.3.cmml" xref="S4.p2.1.m1.3.3.3.3"><times id="S4.p2.1.m1.3.3.3.3.2.cmml" xref="S4.p2.1.m1.3.3.3.3.2"></times><ci id="S4.p2.1.m1.3.3.3.3.3.cmml" xref="S4.p2.1.m1.3.3.3.3.3">ğ‘£</ci><ci id="S4.p2.1.m1.3.3.3.3.4.cmml" xref="S4.p2.1.m1.3.3.3.3.4">ğ‘’</ci><ci id="S4.p2.1.m1.3.3.3.3.5.cmml" xref="S4.p2.1.m1.3.3.3.3.5">ğ‘</ci><ci id="S4.p2.1.m1.3.3.3.3.6.cmml" xref="S4.p2.1.m1.3.3.3.3.6">ğ‘¡</ci><ci id="S4.p2.1.m1.3.3.3.3.7.cmml" xref="S4.p2.1.m1.3.3.3.3.7">ğ‘œ</ci><ci id="S4.p2.1.m1.3.3.3.3.8.cmml" xref="S4.p2.1.m1.3.3.3.3.8">ğ‘Ÿ</ci><apply id="S4.p2.1.m1.3.3.3.3.1.1.1.cmml" xref="S4.p2.1.m1.3.3.3.3.1.1"><times id="S4.p2.1.m1.3.3.3.3.1.1.1.1.cmml" xref="S4.p2.1.m1.3.3.3.3.1.1.1.1"></times><ci id="S4.p2.1.m1.3.3.3.3.1.1.1.2.cmml" xref="S4.p2.1.m1.3.3.3.3.1.1.1.2">"</ci><ci id="S4.p2.1.m1.3.3.3.3.1.1.1.3.cmml" xref="S4.p2.1.m1.3.3.3.3.1.1.1.3">ğ‘ </ci><ci id="S4.p2.1.m1.3.3.3.3.1.1.1.4.cmml" xref="S4.p2.1.m1.3.3.3.3.1.1.1.4">ğ‘š</ci><ci id="S4.p2.1.m1.3.3.3.3.1.1.1.5.cmml" xref="S4.p2.1.m1.3.3.3.3.1.1.1.5">ğ‘</ci><ci id="S4.p2.1.m1.3.3.3.3.1.1.1.6.cmml" xref="S4.p2.1.m1.3.3.3.3.1.1.1.6">ğ‘™</ci><ci id="S4.p2.1.m1.3.3.3.3.1.1.1.7.cmml" xref="S4.p2.1.m1.3.3.3.3.1.1.1.7">ğ‘™</ci><ci id="S4.p2.1.m1.3.3.3.3.1.1.1.8.cmml" xref="S4.p2.1.m1.3.3.3.3.1.1.1.8">"</ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.p2.1.m1.3c">X=vector("biggest")-vector("big")+vector("small")</annotation><annotation encoding="application/x-llamapun" id="S4.p2.1.m1.3d">italic_X = italic_v italic_e italic_c italic_t italic_o italic_r ( " italic_b italic_i italic_g italic_g italic_e italic_s italic_t " ) - italic_v italic_e italic_c italic_t italic_o italic_r ( " italic_b italic_i italic_g " ) + italic_v italic_e italic_c italic_t italic_o italic_r ( " italic_s italic_m italic_a italic_l italic_l " )</annotation></semantics></math>. Then, we search in the vector space for the word closest to <math id="S4.p2.2.m2.1" class="ltx_Math" alttext="X" display="inline"><semantics id="S4.p2.2.m2.1a"><mi id="S4.p2.2.m2.1.1" xref="S4.p2.2.m2.1.1.cmml">X</mi><annotation-xml encoding="MathML-Content" id="S4.p2.2.m2.1b"><ci id="S4.p2.2.m2.1.1.cmml" xref="S4.p2.2.m2.1.1">ğ‘‹</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.p2.2.m2.1c">X</annotation><annotation encoding="application/x-llamapun" id="S4.p2.2.m2.1d">italic_X</annotation></semantics></math> measured by cosine distance, and use it as the answer to the question (we discard the input
question words during this search).
When the word vectors are well trained, it is possible to find the correct answer (word <math id="S4.p2.3.m3.1" class="ltx_Math" alttext="smallest" display="inline"><semantics id="S4.p2.3.m3.1a"><mrow id="S4.p2.3.m3.1.1" xref="S4.p2.3.m3.1.1.cmml"><mi id="S4.p2.3.m3.1.1.2" xref="S4.p2.3.m3.1.1.2.cmml">s</mi><mo id="S4.p2.3.m3.1.1.1" xref="S4.p2.3.m3.1.1.1.cmml">â¢</mo><mi id="S4.p2.3.m3.1.1.3" xref="S4.p2.3.m3.1.1.3.cmml">m</mi><mo id="S4.p2.3.m3.1.1.1a" xref="S4.p2.3.m3.1.1.1.cmml">â¢</mo><mi id="S4.p2.3.m3.1.1.4" xref="S4.p2.3.m3.1.1.4.cmml">a</mi><mo id="S4.p2.3.m3.1.1.1b" xref="S4.p2.3.m3.1.1.1.cmml">â¢</mo><mi id="S4.p2.3.m3.1.1.5" xref="S4.p2.3.m3.1.1.5.cmml">l</mi><mo id="S4.p2.3.m3.1.1.1c" xref="S4.p2.3.m3.1.1.1.cmml">â¢</mo><mi id="S4.p2.3.m3.1.1.6" xref="S4.p2.3.m3.1.1.6.cmml">l</mi><mo id="S4.p2.3.m3.1.1.1d" xref="S4.p2.3.m3.1.1.1.cmml">â¢</mo><mi id="S4.p2.3.m3.1.1.7" xref="S4.p2.3.m3.1.1.7.cmml">e</mi><mo id="S4.p2.3.m3.1.1.1e" xref="S4.p2.3.m3.1.1.1.cmml">â¢</mo><mi id="S4.p2.3.m3.1.1.8" xref="S4.p2.3.m3.1.1.8.cmml">s</mi><mo id="S4.p2.3.m3.1.1.1f" xref="S4.p2.3.m3.1.1.1.cmml">â¢</mo><mi id="S4.p2.3.m3.1.1.9" xref="S4.p2.3.m3.1.1.9.cmml">t</mi></mrow><annotation-xml encoding="MathML-Content" id="S4.p2.3.m3.1b"><apply id="S4.p2.3.m3.1.1.cmml" xref="S4.p2.3.m3.1.1"><times id="S4.p2.3.m3.1.1.1.cmml" xref="S4.p2.3.m3.1.1.1"></times><ci id="S4.p2.3.m3.1.1.2.cmml" xref="S4.p2.3.m3.1.1.2">ğ‘ </ci><ci id="S4.p2.3.m3.1.1.3.cmml" xref="S4.p2.3.m3.1.1.3">ğ‘š</ci><ci id="S4.p2.3.m3.1.1.4.cmml" xref="S4.p2.3.m3.1.1.4">ğ‘</ci><ci id="S4.p2.3.m3.1.1.5.cmml" xref="S4.p2.3.m3.1.1.5">ğ‘™</ci><ci id="S4.p2.3.m3.1.1.6.cmml" xref="S4.p2.3.m3.1.1.6">ğ‘™</ci><ci id="S4.p2.3.m3.1.1.7.cmml" xref="S4.p2.3.m3.1.1.7">ğ‘’</ci><ci id="S4.p2.3.m3.1.1.8.cmml" xref="S4.p2.3.m3.1.1.8">ğ‘ </ci><ci id="S4.p2.3.m3.1.1.9.cmml" xref="S4.p2.3.m3.1.1.9">ğ‘¡</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.p2.3.m3.1c">smallest</annotation><annotation encoding="application/x-llamapun" id="S4.p2.3.m3.1d">italic_s italic_m italic_a italic_l italic_l italic_e italic_s italic_t</annotation></semantics></math>) using this method.</p>
</div>
<div id="S4.p3" class="ltx_para ltx_noindent">
<p id="S4.p3.1" class="ltx_p">Finally, we found that when we train high dimensional word vectors on a large amount of data, the resulting vectors can be used to answer very subtle semantic relationships between words, such as a city and the
country it belongs to, e.g. France is to Paris as Germany is to Berlin. Word vectors with such semantic relationships could be used to improve many existing NLP applications, such as machine translation, information retrieval
and question answering systems, and may enable other future applications yet to be invented. </p>
</div>
<section id="S4.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>Task Description</h3>

<div id="S4.SS1.p1" class="ltx_para ltx_noindent">
<p id="S4.SS1.p1.1" class="ltx_p">To measure quality of the word vectors, we define a comprehensive test set that contains five types of semantic questions, and nine types of syntactic questions. Two examples from each category are shown in TableÂ <a href="#S4.T1" title="Table 1 â€£ 4.1 Task Description â€£ 4 Results â€£ Efficient Estimation of Word Representations in Vector Space" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>. Overall,
there are 8869 semantic and 10675 syntactic questions. The questions in each category were created in two steps: first, a list of similar word pairs was created manually. Then, a large list of questions is formed by connecting
two word pairs. For example, we made a list of 68 large American cities and the states they belong to, and formed about 2.5K questions by picking two word pairs at random. We have included in our test set only single token words,
thus multi-word entities are not present (such as <span id="S4.SS1.p1.1.1" class="ltx_text ltx_font_italic">New York</span>). </p>
</div>
<div id="S4.SS1.p2" class="ltx_para ltx_noindent">
<p id="S4.SS1.p2.1" class="ltx_p">We evaluate the overall accuracy for all question types, and for each question type separately (semantic, syntactic). Question is assumed to be correctly answered only if the closest
word to the vector computed using the above method is exactly the same as the correct word in the question; synonyms are thus counted as mistakes. This also means that reaching 100% accuracy is likely to
be impossible, as the current models do not have any input information about word morphology.
However, we believe that usefulness of the word vectors for certain applications should be positively correlated with this accuracy metric. Further progress can be achieved by incorporating
information about structure of words, especially for the syntactic questions.</p>
</div>
<figure id="S4.T1" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 1: </span> <span id="S4.T1.2.1" class="ltx_text ltx_font_italic">Examples of five types of semantic and nine types of syntactic questions in the Semantic-Syntactic Word Relationship test set.</span></figcaption>
<p id="S4.T1.3" class="ltx_p ltx_align_center"><span id="S4.T1.3.1" class="ltx_text">

<span id="S4.T1.3.1.1" class="ltx_tabular ltx_align_middle">
<span class="ltx_tbody">
<span id="S4.T1.3.1.1.1.1" class="ltx_tr">
<span id="S4.T1.3.1.1.1.1.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_rr ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">Type of relationship</span>
<span id="S4.T1.3.1.1.1.1.2" class="ltx_td ltx_align_center ltx_border_rr ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;" colspan="2">Word Pair 1</span>
<span id="S4.T1.3.1.1.1.1.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;" colspan="2">Word Pair 2</span></span>
<span id="S4.T1.3.1.1.2.2" class="ltx_tr">
<span id="S4.T1.3.1.1.2.2.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_rr ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">Common capital city</span>
<span id="S4.T1.3.1.1.2.2.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">Athens</span>
<span id="S4.T1.3.1.1.2.2.3" class="ltx_td ltx_align_center ltx_border_rr ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">Greece</span>
<span id="S4.T1.3.1.1.2.2.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">Oslo</span>
<span id="S4.T1.3.1.1.2.2.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">Norway</span></span>
<span id="S4.T1.3.1.1.3.3" class="ltx_tr">
<span id="S4.T1.3.1.1.3.3.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_rr" style="padding-top:1.5pt;padding-bottom:1.5pt;">All capital cities</span>
<span id="S4.T1.3.1.1.3.3.2" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:1.5pt;padding-bottom:1.5pt;">Astana</span>
<span id="S4.T1.3.1.1.3.3.3" class="ltx_td ltx_align_center ltx_border_rr" style="padding-top:1.5pt;padding-bottom:1.5pt;">Kazakhstan</span>
<span id="S4.T1.3.1.1.3.3.4" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:1.5pt;padding-bottom:1.5pt;">Harare</span>
<span id="S4.T1.3.1.1.3.3.5" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:1.5pt;padding-bottom:1.5pt;">Zimbabwe</span></span>
<span id="S4.T1.3.1.1.4.4" class="ltx_tr">
<span id="S4.T1.3.1.1.4.4.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_rr" style="padding-top:1.5pt;padding-bottom:1.5pt;">Currency</span>
<span id="S4.T1.3.1.1.4.4.2" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:1.5pt;padding-bottom:1.5pt;">Angola</span>
<span id="S4.T1.3.1.1.4.4.3" class="ltx_td ltx_align_center ltx_border_rr" style="padding-top:1.5pt;padding-bottom:1.5pt;">kwanza</span>
<span id="S4.T1.3.1.1.4.4.4" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:1.5pt;padding-bottom:1.5pt;">Iran</span>
<span id="S4.T1.3.1.1.4.4.5" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:1.5pt;padding-bottom:1.5pt;">rial</span></span>
<span id="S4.T1.3.1.1.5.5" class="ltx_tr">
<span id="S4.T1.3.1.1.5.5.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_rr" style="padding-top:1.5pt;padding-bottom:1.5pt;">City-in-state</span>
<span id="S4.T1.3.1.1.5.5.2" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:1.5pt;padding-bottom:1.5pt;">Chicago</span>
<span id="S4.T1.3.1.1.5.5.3" class="ltx_td ltx_align_center ltx_border_rr" style="padding-top:1.5pt;padding-bottom:1.5pt;">Illinois</span>
<span id="S4.T1.3.1.1.5.5.4" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:1.5pt;padding-bottom:1.5pt;">Stockton</span>
<span id="S4.T1.3.1.1.5.5.5" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:1.5pt;padding-bottom:1.5pt;">California</span></span>
<span id="S4.T1.3.1.1.6.6" class="ltx_tr">
<span id="S4.T1.3.1.1.6.6.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_rr" style="padding-top:1.5pt;padding-bottom:1.5pt;">Man-Woman</span>
<span id="S4.T1.3.1.1.6.6.2" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:1.5pt;padding-bottom:1.5pt;">brother</span>
<span id="S4.T1.3.1.1.6.6.3" class="ltx_td ltx_align_center ltx_border_rr" style="padding-top:1.5pt;padding-bottom:1.5pt;">sister</span>
<span id="S4.T1.3.1.1.6.6.4" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:1.5pt;padding-bottom:1.5pt;">grandson</span>
<span id="S4.T1.3.1.1.6.6.5" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:1.5pt;padding-bottom:1.5pt;">granddaughter</span></span>
<span id="S4.T1.3.1.1.7.7" class="ltx_tr">
<span id="S4.T1.3.1.1.7.7.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_rr ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">Adjective to adverb</span>
<span id="S4.T1.3.1.1.7.7.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">apparent</span>
<span id="S4.T1.3.1.1.7.7.3" class="ltx_td ltx_align_center ltx_border_rr ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">apparently</span>
<span id="S4.T1.3.1.1.7.7.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">rapid</span>
<span id="S4.T1.3.1.1.7.7.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">rapidly</span></span>
<span id="S4.T1.3.1.1.8.8" class="ltx_tr">
<span id="S4.T1.3.1.1.8.8.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_rr" style="padding-top:1.5pt;padding-bottom:1.5pt;">Opposite</span>
<span id="S4.T1.3.1.1.8.8.2" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:1.5pt;padding-bottom:1.5pt;">possibly</span>
<span id="S4.T1.3.1.1.8.8.3" class="ltx_td ltx_align_center ltx_border_rr" style="padding-top:1.5pt;padding-bottom:1.5pt;">impossibly</span>
<span id="S4.T1.3.1.1.8.8.4" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:1.5pt;padding-bottom:1.5pt;">ethical</span>
<span id="S4.T1.3.1.1.8.8.5" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:1.5pt;padding-bottom:1.5pt;">unethical</span></span>
<span id="S4.T1.3.1.1.9.9" class="ltx_tr">
<span id="S4.T1.3.1.1.9.9.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_rr" style="padding-top:1.5pt;padding-bottom:1.5pt;">Comparative</span>
<span id="S4.T1.3.1.1.9.9.2" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:1.5pt;padding-bottom:1.5pt;">great</span>
<span id="S4.T1.3.1.1.9.9.3" class="ltx_td ltx_align_center ltx_border_rr" style="padding-top:1.5pt;padding-bottom:1.5pt;">greater</span>
<span id="S4.T1.3.1.1.9.9.4" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:1.5pt;padding-bottom:1.5pt;">tough</span>
<span id="S4.T1.3.1.1.9.9.5" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:1.5pt;padding-bottom:1.5pt;">tougher</span></span>
<span id="S4.T1.3.1.1.10.10" class="ltx_tr">
<span id="S4.T1.3.1.1.10.10.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_rr" style="padding-top:1.5pt;padding-bottom:1.5pt;">Superlative</span>
<span id="S4.T1.3.1.1.10.10.2" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:1.5pt;padding-bottom:1.5pt;">easy</span>
<span id="S4.T1.3.1.1.10.10.3" class="ltx_td ltx_align_center ltx_border_rr" style="padding-top:1.5pt;padding-bottom:1.5pt;">easiest</span>
<span id="S4.T1.3.1.1.10.10.4" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:1.5pt;padding-bottom:1.5pt;">lucky</span>
<span id="S4.T1.3.1.1.10.10.5" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:1.5pt;padding-bottom:1.5pt;">luckiest</span></span>
<span id="S4.T1.3.1.1.11.11" class="ltx_tr">
<span id="S4.T1.3.1.1.11.11.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_rr" style="padding-top:1.5pt;padding-bottom:1.5pt;">Present Participle</span>
<span id="S4.T1.3.1.1.11.11.2" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:1.5pt;padding-bottom:1.5pt;">think</span>
<span id="S4.T1.3.1.1.11.11.3" class="ltx_td ltx_align_center ltx_border_rr" style="padding-top:1.5pt;padding-bottom:1.5pt;">thinking</span>
<span id="S4.T1.3.1.1.11.11.4" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:1.5pt;padding-bottom:1.5pt;">read</span>
<span id="S4.T1.3.1.1.11.11.5" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:1.5pt;padding-bottom:1.5pt;">reading</span></span>
<span id="S4.T1.3.1.1.12.12" class="ltx_tr">
<span id="S4.T1.3.1.1.12.12.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_rr" style="padding-top:1.5pt;padding-bottom:1.5pt;">Nationality adjective</span>
<span id="S4.T1.3.1.1.12.12.2" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:1.5pt;padding-bottom:1.5pt;">Switzerland</span>
<span id="S4.T1.3.1.1.12.12.3" class="ltx_td ltx_align_center ltx_border_rr" style="padding-top:1.5pt;padding-bottom:1.5pt;">Swiss</span>
<span id="S4.T1.3.1.1.12.12.4" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:1.5pt;padding-bottom:1.5pt;">Cambodia</span>
<span id="S4.T1.3.1.1.12.12.5" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:1.5pt;padding-bottom:1.5pt;">Cambodian</span></span>
<span id="S4.T1.3.1.1.13.13" class="ltx_tr">
<span id="S4.T1.3.1.1.13.13.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_rr" style="padding-top:1.5pt;padding-bottom:1.5pt;">Past tense</span>
<span id="S4.T1.3.1.1.13.13.2" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:1.5pt;padding-bottom:1.5pt;">walking</span>
<span id="S4.T1.3.1.1.13.13.3" class="ltx_td ltx_align_center ltx_border_rr" style="padding-top:1.5pt;padding-bottom:1.5pt;">walked</span>
<span id="S4.T1.3.1.1.13.13.4" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:1.5pt;padding-bottom:1.5pt;">swimming</span>
<span id="S4.T1.3.1.1.13.13.5" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:1.5pt;padding-bottom:1.5pt;">swam</span></span>
<span id="S4.T1.3.1.1.14.14" class="ltx_tr">
<span id="S4.T1.3.1.1.14.14.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_rr" style="padding-top:1.5pt;padding-bottom:1.5pt;">Plural nouns</span>
<span id="S4.T1.3.1.1.14.14.2" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:1.5pt;padding-bottom:1.5pt;">mouse</span>
<span id="S4.T1.3.1.1.14.14.3" class="ltx_td ltx_align_center ltx_border_rr" style="padding-top:1.5pt;padding-bottom:1.5pt;">mice</span>
<span id="S4.T1.3.1.1.14.14.4" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:1.5pt;padding-bottom:1.5pt;">dollar</span>
<span id="S4.T1.3.1.1.14.14.5" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:1.5pt;padding-bottom:1.5pt;">dollars</span></span>
<span id="S4.T1.3.1.1.15.15" class="ltx_tr">
<span id="S4.T1.3.1.1.15.15.1" class="ltx_td ltx_align_left ltx_border_b ltx_border_l ltx_border_rr" style="padding-top:1.5pt;padding-bottom:1.5pt;">Plural verbs</span>
<span id="S4.T1.3.1.1.15.15.2" class="ltx_td ltx_align_center ltx_border_b ltx_border_r" style="padding-top:1.5pt;padding-bottom:1.5pt;">work</span>
<span id="S4.T1.3.1.1.15.15.3" class="ltx_td ltx_align_center ltx_border_b ltx_border_rr" style="padding-top:1.5pt;padding-bottom:1.5pt;">works</span>
<span id="S4.T1.3.1.1.15.15.4" class="ltx_td ltx_align_center ltx_border_b ltx_border_r" style="padding-top:1.5pt;padding-bottom:1.5pt;">speak</span>
<span id="S4.T1.3.1.1.15.15.5" class="ltx_td ltx_align_center ltx_border_b ltx_border_r" style="padding-top:1.5pt;padding-bottom:1.5pt;">speaks</span></span>
</span>
</span></span></p>
</figure>
</section>
<section id="S4.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2 </span>Maximization of Accuracy</h3>

<div id="S4.SS2.p1" class="ltx_para ltx_noindent">
<p id="S4.SS2.p1.1" class="ltx_p">We have used a Google News corpus for training the word vectors. This corpus contains about 6B tokens. We have restricted the vocabulary size to 1 million most frequent words. Clearly, we are facing
time constrained optimization problem, as it can be expected that both using more data and higher dimensional word vectors will improve the accuracy. To estimate the best choice of model architecture for obtaining
as good as possible results quickly, we have first evaluated models trained on subsets of the training data, with vocabulary restricted to the most frequent 30k words. The results using the CBOW
architecture with different choice of word vector dimensionality and increasing amount of the training data are shown in TableÂ <a href="#S4.T2" title="Table 2 â€£ 4.2 Maximization of Accuracy â€£ 4 Results â€£ Efficient Estimation of Word Representations in Vector Space" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>.</p>
</div>
<div id="S4.SS2.p2" class="ltx_para ltx_noindent">
<p id="S4.SS2.p2.1" class="ltx_p">It can be seen that after some point, adding more dimensions or adding more training data provides diminishing improvements. So, we have to increase both vector dimensionality and the amount of the training data together. While
this observation might seem trivial, it must be noted that it is currently popular to train word vectors on relatively large amounts of data, but with insufficient size (such as 50 - 100).
Given EquationÂ <a href="#S3.E4" title="(4) â€£ 3.1 Continuous Bag-of-Words Model â€£ 3 New Log-linear Models â€£ Efficient Estimation of Word Representations in Vector Space" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>, increasing amount of training data twice results in about the same increase of computational complexity as increasing vector size twice.</p>
</div>
<figure id="S4.T2" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 2: </span> <span id="S4.T2.2.1" class="ltx_text ltx_font_italic">Accuracy on subset of the Semantic-Syntactic Word Relationship test set, using word vectors from the CBOW architecture with limited vocabulary. Only questions containing words from the most frequent 30k words are used.</span></figcaption>
<p id="S4.T2.3" class="ltx_p ltx_align_center"><span id="S4.T2.3.1" class="ltx_text">

<span id="S4.T2.3.1.1" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<span class="ltx_thead">
<span id="S4.T2.3.1.1.1.1" class="ltx_tr">
<span id="S4.T2.3.1.1.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_l ltx_border_rr ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">Dimensionality / Training words</span>
<span id="S4.T2.3.1.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">24M</span>
<span id="S4.T2.3.1.1.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">49M</span>
<span id="S4.T2.3.1.1.1.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">98M</span>
<span id="S4.T2.3.1.1.1.1.5" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">196M</span>
<span id="S4.T2.3.1.1.1.1.6" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">391M</span>
<span id="S4.T2.3.1.1.1.1.7" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">783M</span></span>
</span>
<span class="ltx_tbody">
<span id="S4.T2.3.1.1.2.1" class="ltx_tr">
<span id="S4.T2.3.1.1.2.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_rr ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">50</span>
<span id="S4.T2.3.1.1.2.1.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">13.4</span>
<span id="S4.T2.3.1.1.2.1.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">15.7</span>
<span id="S4.T2.3.1.1.2.1.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">18.6</span>
<span id="S4.T2.3.1.1.2.1.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">19.1</span>
<span id="S4.T2.3.1.1.2.1.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">22.5</span>
<span id="S4.T2.3.1.1.2.1.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">23.2</span></span>
<span id="S4.T2.3.1.1.3.2" class="ltx_tr">
<span id="S4.T2.3.1.1.3.2.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_rr" style="padding-top:1.5pt;padding-bottom:1.5pt;">100</span>
<span id="S4.T2.3.1.1.3.2.2" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:1.5pt;padding-bottom:1.5pt;">19.4</span>
<span id="S4.T2.3.1.1.3.2.3" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:1.5pt;padding-bottom:1.5pt;">23.1</span>
<span id="S4.T2.3.1.1.3.2.4" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:1.5pt;padding-bottom:1.5pt;">27.8</span>
<span id="S4.T2.3.1.1.3.2.5" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:1.5pt;padding-bottom:1.5pt;">28.7</span>
<span id="S4.T2.3.1.1.3.2.6" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:1.5pt;padding-bottom:1.5pt;">33.4</span>
<span id="S4.T2.3.1.1.3.2.7" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:1.5pt;padding-bottom:1.5pt;">32.2</span></span>
<span id="S4.T2.3.1.1.4.3" class="ltx_tr">
<span id="S4.T2.3.1.1.4.3.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_rr" style="padding-top:1.5pt;padding-bottom:1.5pt;">300</span>
<span id="S4.T2.3.1.1.4.3.2" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:1.5pt;padding-bottom:1.5pt;">23.2</span>
<span id="S4.T2.3.1.1.4.3.3" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:1.5pt;padding-bottom:1.5pt;">29.2</span>
<span id="S4.T2.3.1.1.4.3.4" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:1.5pt;padding-bottom:1.5pt;">35.3</span>
<span id="S4.T2.3.1.1.4.3.5" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:1.5pt;padding-bottom:1.5pt;">38.6</span>
<span id="S4.T2.3.1.1.4.3.6" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:1.5pt;padding-bottom:1.5pt;">43.7</span>
<span id="S4.T2.3.1.1.4.3.7" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:1.5pt;padding-bottom:1.5pt;">45.9</span></span>
<span id="S4.T2.3.1.1.5.4" class="ltx_tr">
<span id="S4.T2.3.1.1.5.4.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_b ltx_border_l ltx_border_rr" style="padding-top:1.5pt;padding-bottom:1.5pt;">600</span>
<span id="S4.T2.3.1.1.5.4.2" class="ltx_td ltx_align_center ltx_border_b ltx_border_r" style="padding-top:1.5pt;padding-bottom:1.5pt;">24.0</span>
<span id="S4.T2.3.1.1.5.4.3" class="ltx_td ltx_align_center ltx_border_b ltx_border_r" style="padding-top:1.5pt;padding-bottom:1.5pt;">30.1</span>
<span id="S4.T2.3.1.1.5.4.4" class="ltx_td ltx_align_center ltx_border_b ltx_border_r" style="padding-top:1.5pt;padding-bottom:1.5pt;">36.5</span>
<span id="S4.T2.3.1.1.5.4.5" class="ltx_td ltx_align_center ltx_border_b ltx_border_r" style="padding-top:1.5pt;padding-bottom:1.5pt;">40.8</span>
<span id="S4.T2.3.1.1.5.4.6" class="ltx_td ltx_align_center ltx_border_b ltx_border_r" style="padding-top:1.5pt;padding-bottom:1.5pt;">46.6</span>
<span id="S4.T2.3.1.1.5.4.7" class="ltx_td ltx_align_center ltx_border_b ltx_border_r" style="padding-top:1.5pt;padding-bottom:1.5pt;">50.4</span></span>
</span>
</span></span></p>
</figure>
<div id="S4.SS2.p3" class="ltx_para ltx_noindent">
<p id="S4.SS2.p3.1" class="ltx_p">For the experiments reported in TablesÂ <a href="#S4.T2" title="Table 2 â€£ 4.2 Maximization of Accuracy â€£ 4 Results â€£ Efficient Estimation of Word Representations in Vector Space" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> andÂ <a href="#S4.T4" title="Table 4 â€£ 4.3 Comparison of Model Architectures â€£ 4 Results â€£ Efficient Estimation of Word Representations in Vector Space" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>, we used three training epochs with stochastic gradient descent and backpropagation. We chose starting learning rate 0.025 and decreased it linearly, so that it approaches zero
at the end of the last training epoch.</p>
</div>
</section>
<section id="S4.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.3 </span>Comparison of Model Architectures</h3>

<div id="S4.SS3.p1" class="ltx_para ltx_noindent">
<p id="S4.SS3.p1.1" class="ltx_p">First we compare different model architectures for deriving the word vectors using the same training data and using the same dimensionality of 640 of the word vectors.
In the further experiments, we use full set of questions in the new Semantic-Syntactic Word Relationship test set, i.e. unrestricted to the 30k vocabulary.
We also include results on a test set introduced inÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib20" title="" class="ltx_ref">20</a>]</cite> that focuses on syntactic
similarity between words<span id="footnote3" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">3</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">3</sup><span class="ltx_tag ltx_tag_note">3</span>We thank Geoff Zweig for providing us the test set.</span></span></span>.</p>
</div>
<div id="S4.SS3.p2" class="ltx_para ltx_noindent">
<p id="S4.SS3.p2.1" class="ltx_p">The training data consists of several LDC corpora and is described in detail inÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref">18</a>]</cite> (320M words, 82K vocabulary).
We used these data to provide a comparison to a previously trained recurrent neural network language model
that took about 8 weeks to train on a single CPU. We trained a feedforward NNLM with the same number of 640 hidden units using the DistBelief parallel trainingÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite>, using a history of 8 previous words (thus, the NNLM has
more parameters than the RNNLM, as the projection layer has size <math id="S4.SS3.p2.1.m1.1" class="ltx_Math" alttext="640\times 8" display="inline"><semantics id="S4.SS3.p2.1.m1.1a"><mrow id="S4.SS3.p2.1.m1.1.1" xref="S4.SS3.p2.1.m1.1.1.cmml"><mn id="S4.SS3.p2.1.m1.1.1.2" xref="S4.SS3.p2.1.m1.1.1.2.cmml">640</mn><mo id="S4.SS3.p2.1.m1.1.1.1" xref="S4.SS3.p2.1.m1.1.1.1.cmml">Ã—</mo><mn id="S4.SS3.p2.1.m1.1.1.3" xref="S4.SS3.p2.1.m1.1.1.3.cmml">8</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS3.p2.1.m1.1b"><apply id="S4.SS3.p2.1.m1.1.1.cmml" xref="S4.SS3.p2.1.m1.1.1"><times id="S4.SS3.p2.1.m1.1.1.1.cmml" xref="S4.SS3.p2.1.m1.1.1.1"></times><cn type="integer" id="S4.SS3.p2.1.m1.1.1.2.cmml" xref="S4.SS3.p2.1.m1.1.1.2">640</cn><cn type="integer" id="S4.SS3.p2.1.m1.1.1.3.cmml" xref="S4.SS3.p2.1.m1.1.1.3">8</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p2.1.m1.1c">640\times 8</annotation><annotation encoding="application/x-llamapun" id="S4.SS3.p2.1.m1.1d">640 Ã— 8</annotation></semantics></math>).</p>
</div>
<div id="S4.SS3.p3" class="ltx_para ltx_noindent">
<p id="S4.SS3.p3.1" class="ltx_p">In TableÂ <a href="#S4.T3" title="Table 3 â€£ 4.3 Comparison of Model Architectures â€£ 4 Results â€£ Efficient Estimation of Word Representations in Vector Space" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>, it can be seen that the word vectors from the RNN (as used inÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib20" title="" class="ltx_ref">20</a>]</cite>) perform well mostly on the syntactic questions. The NNLM vectors perform significantly better than the RNN - this is not surprising, as the
word vectors in the RNNLM are directly connected to a non-linear hidden layer. The CBOW architecture
works better than the NNLM on the syntactic tasks, and about the same on the semantic one. Finally, the Skip-gram architecture works slightly worse on the syntactic task than the CBOW model (but still better than the NNLM), and much better
on the semantic part of the test than all the other models.</p>
</div>
<figure id="S4.T3" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 3: </span><span id="S4.T3.2.1" class="ltx_text ltx_font_italic">Comparison of architectures using models trained on the same data, with 640-dimensional word vectors. The accuracies are reported on our Semantic-Syntactic Word Relationship test set, and on the syntactic relationship test set ofÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib20" title="" class="ltx_ref">20</a>]</cite> </span></figcaption>
<p id="S4.T3.3" class="ltx_p ltx_align_center"><span id="S4.T3.3.1" class="ltx_text">

<span id="S4.T3.3.1.1" class="ltx_tabular ltx_align_middle">
<span class="ltx_tbody">
<span id="S4.T3.3.1.1.1.1" class="ltx_tr">
<span id="S4.T3.3.1.1.1.1.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_rr ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">Model</span>
<span id="S4.T3.3.1.1.1.1.2" class="ltx_td ltx_align_center ltx_border_rr ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;" colspan="2">Semantic-Syntactic Word Relationship test set</span>
<span id="S4.T3.3.1.1.1.1.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">MSR Word Relatedness</span></span>
<span id="S4.T3.3.1.1.2.2" class="ltx_tr">
<span id="S4.T3.3.1.1.2.2.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_rr" style="padding-top:1.5pt;padding-bottom:1.5pt;">Architecture</span>
<span id="S4.T3.3.1.1.2.2.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">Semantic Accuracy [%]</span>
<span id="S4.T3.3.1.1.2.2.3" class="ltx_td ltx_align_center ltx_border_rr ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">Syntactic Accuracy [%]</span>
<span id="S4.T3.3.1.1.2.2.4" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:1.5pt;padding-bottom:1.5pt;">Test SetÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib20" title="" class="ltx_ref">20</a>]</cite></span></span>
<span id="S4.T3.3.1.1.3.3" class="ltx_tr">
<span id="S4.T3.3.1.1.3.3.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_rr ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">RNNLM</span>
<span id="S4.T3.3.1.1.3.3.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">9</span>
<span id="S4.T3.3.1.1.3.3.3" class="ltx_td ltx_align_center ltx_border_rr ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">36</span>
<span id="S4.T3.3.1.1.3.3.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">35</span></span>
<span id="S4.T3.3.1.1.4.4" class="ltx_tr">
<span id="S4.T3.3.1.1.4.4.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_rr" style="padding-top:1.5pt;padding-bottom:1.5pt;">NNLM</span>
<span id="S4.T3.3.1.1.4.4.2" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:1.5pt;padding-bottom:1.5pt;">23</span>
<span id="S4.T3.3.1.1.4.4.3" class="ltx_td ltx_align_center ltx_border_rr" style="padding-top:1.5pt;padding-bottom:1.5pt;">53</span>
<span id="S4.T3.3.1.1.4.4.4" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:1.5pt;padding-bottom:1.5pt;">47</span></span>
<span id="S4.T3.3.1.1.5.5" class="ltx_tr">
<span id="S4.T3.3.1.1.5.5.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_rr" style="padding-top:1.5pt;padding-bottom:1.5pt;">CBOW</span>
<span id="S4.T3.3.1.1.5.5.2" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:1.5pt;padding-bottom:1.5pt;">24</span>
<span id="S4.T3.3.1.1.5.5.3" class="ltx_td ltx_align_center ltx_border_rr" style="padding-top:1.5pt;padding-bottom:1.5pt;">64</span>
<span id="S4.T3.3.1.1.5.5.4" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:1.5pt;padding-bottom:1.5pt;">61</span></span>
<span id="S4.T3.3.1.1.6.6" class="ltx_tr">
<span id="S4.T3.3.1.1.6.6.1" class="ltx_td ltx_align_center ltx_border_b ltx_border_l ltx_border_rr" style="padding-top:1.5pt;padding-bottom:1.5pt;">Skip-gram</span>
<span id="S4.T3.3.1.1.6.6.2" class="ltx_td ltx_align_center ltx_border_b ltx_border_r" style="padding-top:1.5pt;padding-bottom:1.5pt;">55</span>
<span id="S4.T3.3.1.1.6.6.3" class="ltx_td ltx_align_center ltx_border_b ltx_border_rr" style="padding-top:1.5pt;padding-bottom:1.5pt;">59</span>
<span id="S4.T3.3.1.1.6.6.4" class="ltx_td ltx_align_center ltx_border_b ltx_border_r" style="padding-top:1.5pt;padding-bottom:1.5pt;">56</span></span>
</span>
</span></span></p>
</figure>
<div id="S4.SS3.p4" class="ltx_para ltx_noindent">
<p id="S4.SS3.p4.1" class="ltx_p">Next, we evaluated our models trained using one CPU only and compared the results against publicly available word vectors.
The comparison is given in TableÂ <a href="#S4.T4" title="Table 4 â€£ 4.3 Comparison of Model Architectures â€£ 4 Results â€£ Efficient Estimation of Word Representations in Vector Space" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>. The CBOW model was trained on subset of the Google News data in about a day, while training time for the Skip-gram model was about three days.</p>
</div>
<figure id="S4.T4" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 4: </span> <span id="S4.T4.2.1" class="ltx_text ltx_font_italic">Comparison of publicly available word vectors on the Semantic-Syntactic Word Relationship test set, and word vectors from our models. Full vocabularies are used.</span></figcaption>
<p id="S4.T4.3" class="ltx_p ltx_align_center"><span id="S4.T4.3.1" class="ltx_text">

<span id="S4.T4.3.1.1" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<span class="ltx_tbody">
<span id="S4.T4.3.1.1.1.1" class="ltx_tr">
<span id="S4.T4.3.1.1.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_rr ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">Model</span>
<span id="S4.T4.3.1.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">Vector</span>
<span id="S4.T4.3.1.1.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_rr ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">Training</span>
<span id="S4.T4.3.1.1.1.1.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;" colspan="3">Accuracy [%]</span></span>
<span id="S4.T4.3.1.1.2.2" class="ltx_tr">
<span id="S4.T4.3.1.1.2.2.1" class="ltx_td ltx_th ltx_th_row ltx_border_l ltx_border_rr" style="padding-top:1.5pt;padding-bottom:1.5pt;"></span>
<span id="S4.T4.3.1.1.2.2.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" style="padding-top:1.5pt;padding-bottom:1.5pt;">Dimensionality</span>
<span id="S4.T4.3.1.1.2.2.3" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_rr" style="padding-top:1.5pt;padding-bottom:1.5pt;">words</span>
<span id="S4.T4.3.1.1.2.2.4" class="ltx_td ltx_border_r" style="padding-top:1.5pt;padding-bottom:1.5pt;" colspan="3"></span></span>
<span id="S4.T4.3.1.1.3.3" class="ltx_tr">
<span id="S4.T4.3.1.1.3.3.1" class="ltx_td ltx_th ltx_th_row ltx_border_l ltx_border_rr ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;"></span>
<span id="S4.T4.3.1.1.3.3.2" class="ltx_td ltx_th ltx_th_row ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;"></span>
<span id="S4.T4.3.1.1.3.3.3" class="ltx_td ltx_th ltx_th_row ltx_border_rr ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;"></span>
<span id="S4.T4.3.1.1.3.3.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">Semantic</span>
<span id="S4.T4.3.1.1.3.3.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">Syntactic</span>
<span id="S4.T4.3.1.1.3.3.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">Total</span></span>
<span id="S4.T4.3.1.1.4.4" class="ltx_tr">
<span id="S4.T4.3.1.1.4.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_rr ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">Collobert-Weston NNLM</span>
<span id="S4.T4.3.1.1.4.4.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">50</span>
<span id="S4.T4.3.1.1.4.4.3" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_rr ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">660M</span>
<span id="S4.T4.3.1.1.4.4.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">9.3</span>
<span id="S4.T4.3.1.1.4.4.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">12.3</span>
<span id="S4.T4.3.1.1.4.4.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">11.0</span></span>
<span id="S4.T4.3.1.1.5.5" class="ltx_tr">
<span id="S4.T4.3.1.1.5.5.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_rr" style="padding-top:1.5pt;padding-bottom:1.5pt;">Turian NNLM</span>
<span id="S4.T4.3.1.1.5.5.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" style="padding-top:1.5pt;padding-bottom:1.5pt;">50</span>
<span id="S4.T4.3.1.1.5.5.3" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_rr" style="padding-top:1.5pt;padding-bottom:1.5pt;">37M</span>
<span id="S4.T4.3.1.1.5.5.4" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:1.5pt;padding-bottom:1.5pt;">1.4</span>
<span id="S4.T4.3.1.1.5.5.5" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:1.5pt;padding-bottom:1.5pt;">2.6</span>
<span id="S4.T4.3.1.1.5.5.6" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:1.5pt;padding-bottom:1.5pt;">2.1</span></span>
<span id="S4.T4.3.1.1.6.6" class="ltx_tr">
<span id="S4.T4.3.1.1.6.6.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_rr" style="padding-top:1.5pt;padding-bottom:1.5pt;">Turian NNLM</span>
<span id="S4.T4.3.1.1.6.6.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" style="padding-top:1.5pt;padding-bottom:1.5pt;">200</span>
<span id="S4.T4.3.1.1.6.6.3" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_rr" style="padding-top:1.5pt;padding-bottom:1.5pt;">37M</span>
<span id="S4.T4.3.1.1.6.6.4" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:1.5pt;padding-bottom:1.5pt;">1.4</span>
<span id="S4.T4.3.1.1.6.6.5" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:1.5pt;padding-bottom:1.5pt;">2.2</span>
<span id="S4.T4.3.1.1.6.6.6" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:1.5pt;padding-bottom:1.5pt;">1.8</span></span>
<span id="S4.T4.3.1.1.7.7" class="ltx_tr">
<span id="S4.T4.3.1.1.7.7.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_rr" style="padding-top:1.5pt;padding-bottom:1.5pt;">Mnih NNLM</span>
<span id="S4.T4.3.1.1.7.7.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" style="padding-top:1.5pt;padding-bottom:1.5pt;">50</span>
<span id="S4.T4.3.1.1.7.7.3" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_rr" style="padding-top:1.5pt;padding-bottom:1.5pt;">37M</span>
<span id="S4.T4.3.1.1.7.7.4" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:1.5pt;padding-bottom:1.5pt;">1.8</span>
<span id="S4.T4.3.1.1.7.7.5" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:1.5pt;padding-bottom:1.5pt;">9.1</span>
<span id="S4.T4.3.1.1.7.7.6" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:1.5pt;padding-bottom:1.5pt;">5.8</span></span>
<span id="S4.T4.3.1.1.8.8" class="ltx_tr">
<span id="S4.T4.3.1.1.8.8.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_rr" style="padding-top:1.5pt;padding-bottom:1.5pt;">Mnih NNLM</span>
<span id="S4.T4.3.1.1.8.8.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" style="padding-top:1.5pt;padding-bottom:1.5pt;">100</span>
<span id="S4.T4.3.1.1.8.8.3" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_rr" style="padding-top:1.5pt;padding-bottom:1.5pt;">37M</span>
<span id="S4.T4.3.1.1.8.8.4" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:1.5pt;padding-bottom:1.5pt;">3.3</span>
<span id="S4.T4.3.1.1.8.8.5" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:1.5pt;padding-bottom:1.5pt;">13.2</span>
<span id="S4.T4.3.1.1.8.8.6" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:1.5pt;padding-bottom:1.5pt;">8.8</span></span>
<span id="S4.T4.3.1.1.9.9" class="ltx_tr">
<span id="S4.T4.3.1.1.9.9.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_rr" style="padding-top:1.5pt;padding-bottom:1.5pt;">Mikolov RNNLM</span>
<span id="S4.T4.3.1.1.9.9.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" style="padding-top:1.5pt;padding-bottom:1.5pt;">80</span>
<span id="S4.T4.3.1.1.9.9.3" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_rr" style="padding-top:1.5pt;padding-bottom:1.5pt;">320M</span>
<span id="S4.T4.3.1.1.9.9.4" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:1.5pt;padding-bottom:1.5pt;">4.9</span>
<span id="S4.T4.3.1.1.9.9.5" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:1.5pt;padding-bottom:1.5pt;">18.4</span>
<span id="S4.T4.3.1.1.9.9.6" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:1.5pt;padding-bottom:1.5pt;">12.7</span></span>
<span id="S4.T4.3.1.1.10.10" class="ltx_tr">
<span id="S4.T4.3.1.1.10.10.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_rr" style="padding-top:1.5pt;padding-bottom:1.5pt;">Mikolov RNNLM</span>
<span id="S4.T4.3.1.1.10.10.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" style="padding-top:1.5pt;padding-bottom:1.5pt;">640</span>
<span id="S4.T4.3.1.1.10.10.3" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_rr" style="padding-top:1.5pt;padding-bottom:1.5pt;">320M</span>
<span id="S4.T4.3.1.1.10.10.4" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:1.5pt;padding-bottom:1.5pt;">8.6</span>
<span id="S4.T4.3.1.1.10.10.5" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:1.5pt;padding-bottom:1.5pt;">36.5</span>
<span id="S4.T4.3.1.1.10.10.6" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:1.5pt;padding-bottom:1.5pt;">24.6</span></span>
<span id="S4.T4.3.1.1.11.11" class="ltx_tr">
<span id="S4.T4.3.1.1.11.11.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_rr" style="padding-top:1.5pt;padding-bottom:1.5pt;">Huang NNLM</span>
<span id="S4.T4.3.1.1.11.11.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" style="padding-top:1.5pt;padding-bottom:1.5pt;">50</span>
<span id="S4.T4.3.1.1.11.11.3" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_rr" style="padding-top:1.5pt;padding-bottom:1.5pt;">990M</span>
<span id="S4.T4.3.1.1.11.11.4" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:1.5pt;padding-bottom:1.5pt;">13.3</span>
<span id="S4.T4.3.1.1.11.11.5" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:1.5pt;padding-bottom:1.5pt;">11.6</span>
<span id="S4.T4.3.1.1.11.11.6" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:1.5pt;padding-bottom:1.5pt;">12.3</span></span>
<span id="S4.T4.3.1.1.12.12" class="ltx_tr">
<span id="S4.T4.3.1.1.12.12.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_rr ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">Our NNLM</span>
<span id="S4.T4.3.1.1.12.12.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">20</span>
<span id="S4.T4.3.1.1.12.12.3" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_rr ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">6B</span>
<span id="S4.T4.3.1.1.12.12.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">12.9</span>
<span id="S4.T4.3.1.1.12.12.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">26.4</span>
<span id="S4.T4.3.1.1.12.12.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">20.3</span></span>
<span id="S4.T4.3.1.1.13.13" class="ltx_tr">
<span id="S4.T4.3.1.1.13.13.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_rr" style="padding-top:1.5pt;padding-bottom:1.5pt;">Our NNLM</span>
<span id="S4.T4.3.1.1.13.13.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" style="padding-top:1.5pt;padding-bottom:1.5pt;">50</span>
<span id="S4.T4.3.1.1.13.13.3" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_rr" style="padding-top:1.5pt;padding-bottom:1.5pt;">6B</span>
<span id="S4.T4.3.1.1.13.13.4" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:1.5pt;padding-bottom:1.5pt;">27.9</span>
<span id="S4.T4.3.1.1.13.13.5" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:1.5pt;padding-bottom:1.5pt;">55.8</span>
<span id="S4.T4.3.1.1.13.13.6" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:1.5pt;padding-bottom:1.5pt;">43.2</span></span>
<span id="S4.T4.3.1.1.14.14" class="ltx_tr">
<span id="S4.T4.3.1.1.14.14.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_rr" style="padding-top:1.5pt;padding-bottom:1.5pt;">Our NNLM</span>
<span id="S4.T4.3.1.1.14.14.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" style="padding-top:1.5pt;padding-bottom:1.5pt;">100</span>
<span id="S4.T4.3.1.1.14.14.3" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_rr" style="padding-top:1.5pt;padding-bottom:1.5pt;">6B</span>
<span id="S4.T4.3.1.1.14.14.4" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:1.5pt;padding-bottom:1.5pt;">34.2</span>
<span id="S4.T4.3.1.1.14.14.5" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:1.5pt;padding-bottom:1.5pt;"><span id="S4.T4.3.1.1.14.14.5.1" class="ltx_text ltx_font_bold">64.5</span></span>
<span id="S4.T4.3.1.1.14.14.6" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:1.5pt;padding-bottom:1.5pt;">50.8</span></span>
<span id="S4.T4.3.1.1.15.15" class="ltx_tr">
<span id="S4.T4.3.1.1.15.15.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_rr" style="padding-top:1.5pt;padding-bottom:1.5pt;">CBOW</span>
<span id="S4.T4.3.1.1.15.15.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" style="padding-top:1.5pt;padding-bottom:1.5pt;">300</span>
<span id="S4.T4.3.1.1.15.15.3" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_rr" style="padding-top:1.5pt;padding-bottom:1.5pt;">783M</span>
<span id="S4.T4.3.1.1.15.15.4" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:1.5pt;padding-bottom:1.5pt;">15.5</span>
<span id="S4.T4.3.1.1.15.15.5" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:1.5pt;padding-bottom:1.5pt;">53.1</span>
<span id="S4.T4.3.1.1.15.15.6" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:1.5pt;padding-bottom:1.5pt;">36.1</span></span>
<span id="S4.T4.3.1.1.16.16" class="ltx_tr">
<span id="S4.T4.3.1.1.16.16.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b ltx_border_l ltx_border_rr" style="padding-top:1.5pt;padding-bottom:1.5pt;">Skip-gram</span>
<span id="S4.T4.3.1.1.16.16.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_b ltx_border_r" style="padding-top:1.5pt;padding-bottom:1.5pt;">300</span>
<span id="S4.T4.3.1.1.16.16.3" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_b ltx_border_rr" style="padding-top:1.5pt;padding-bottom:1.5pt;">783M</span>
<span id="S4.T4.3.1.1.16.16.4" class="ltx_td ltx_align_center ltx_border_b ltx_border_r" style="padding-top:1.5pt;padding-bottom:1.5pt;"><span id="S4.T4.3.1.1.16.16.4.1" class="ltx_text ltx_font_bold">50.0</span></span>
<span id="S4.T4.3.1.1.16.16.5" class="ltx_td ltx_align_center ltx_border_b ltx_border_r" style="padding-top:1.5pt;padding-bottom:1.5pt;">55.9</span>
<span id="S4.T4.3.1.1.16.16.6" class="ltx_td ltx_align_center ltx_border_b ltx_border_r" style="padding-top:1.5pt;padding-bottom:1.5pt;"><span id="S4.T4.3.1.1.16.16.6.1" class="ltx_text ltx_font_bold">53.3</span></span></span>
</span>
</span></span></p>
</figure>
<div id="S4.SS3.p5" class="ltx_para ltx_noindent">
<p id="S4.SS3.p5.1" class="ltx_p">For experiments reported further, we used just one training epoch (again, we decrease the learning rate linearly so that it approaches zero at the end of training).
Training a model on twice as much data using one epoch gives comparable or better results than iterating over the same data for three epochs,
as is shown in TableÂ <a href="#S4.T5" title="Table 5 â€£ 4.3 Comparison of Model Architectures â€£ 4 Results â€£ Efficient Estimation of Word Representations in Vector Space" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>, and provides additional small speedup.</p>
</div>
<figure id="S4.T5" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 5: </span> <span id="S4.T5.2.1" class="ltx_text ltx_font_italic">Comparison of models trained for three epochs on the same data and models trained for one epoch. Accuracy is reported on the full Semantic-Syntactic data set.</span></figcaption>
<p id="S4.T5.3" class="ltx_p ltx_align_center"><span id="S4.T5.3.1" class="ltx_text">

<span id="S4.T5.3.1.1" class="ltx_tabular ltx_align_middle">
<span class="ltx_tbody">
<span id="S4.T5.3.1.1.1.1" class="ltx_tr">
<span id="S4.T5.3.1.1.1.1.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_rr ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">Model</span>
<span id="S4.T5.3.1.1.1.1.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">Vector</span>
<span id="S4.T5.3.1.1.1.1.3" class="ltx_td ltx_align_center ltx_border_rr ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">Training</span>
<span id="S4.T5.3.1.1.1.1.4" class="ltx_td ltx_align_center ltx_border_rr ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;" colspan="3">Accuracy [%]</span>
<span id="S4.T5.3.1.1.1.1.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">Training time</span></span>
<span id="S4.T5.3.1.1.2.2" class="ltx_tr">
<span id="S4.T5.3.1.1.2.2.1" class="ltx_td ltx_border_l ltx_border_rr" style="padding-top:1.5pt;padding-bottom:1.5pt;"></span>
<span id="S4.T5.3.1.1.2.2.2" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:1.5pt;padding-bottom:1.5pt;">Dimensionality</span>
<span id="S4.T5.3.1.1.2.2.3" class="ltx_td ltx_align_center ltx_border_rr" style="padding-top:1.5pt;padding-bottom:1.5pt;">words</span>
<span id="S4.T5.3.1.1.2.2.4" class="ltx_td ltx_border_rr" style="padding-top:1.5pt;padding-bottom:1.5pt;" colspan="3"></span>
<span id="S4.T5.3.1.1.2.2.5" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:1.5pt;padding-bottom:1.5pt;">[days]</span></span>
<span id="S4.T5.3.1.1.3.3" class="ltx_tr">
<span id="S4.T5.3.1.1.3.3.1" class="ltx_td ltx_border_l ltx_border_rr ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;"></span>
<span id="S4.T5.3.1.1.3.3.2" class="ltx_td ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;"></span>
<span id="S4.T5.3.1.1.3.3.3" class="ltx_td ltx_border_rr ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;"></span>
<span id="S4.T5.3.1.1.3.3.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">Semantic</span>
<span id="S4.T5.3.1.1.3.3.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">Syntactic</span>
<span id="S4.T5.3.1.1.3.3.6" class="ltx_td ltx_align_center ltx_border_rr ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">Total</span>
<span id="S4.T5.3.1.1.3.3.7" class="ltx_td ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;"></span></span>
<span id="S4.T5.3.1.1.4.4" class="ltx_tr">
<span id="S4.T5.3.1.1.4.4.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_rr ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">3 epoch CBOW</span>
<span id="S4.T5.3.1.1.4.4.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">300</span>
<span id="S4.T5.3.1.1.4.4.3" class="ltx_td ltx_align_center ltx_border_rr ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">783M</span>
<span id="S4.T5.3.1.1.4.4.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">15.5</span>
<span id="S4.T5.3.1.1.4.4.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">53.1</span>
<span id="S4.T5.3.1.1.4.4.6" class="ltx_td ltx_align_center ltx_border_rr ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">36.1</span>
<span id="S4.T5.3.1.1.4.4.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">1</span></span>
<span id="S4.T5.3.1.1.5.5" class="ltx_tr">
<span id="S4.T5.3.1.1.5.5.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_rr" style="padding-top:1.5pt;padding-bottom:1.5pt;">3 epoch Skip-gram</span>
<span id="S4.T5.3.1.1.5.5.2" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:1.5pt;padding-bottom:1.5pt;">300</span>
<span id="S4.T5.3.1.1.5.5.3" class="ltx_td ltx_align_center ltx_border_rr" style="padding-top:1.5pt;padding-bottom:1.5pt;">783M</span>
<span id="S4.T5.3.1.1.5.5.4" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:1.5pt;padding-bottom:1.5pt;">50.0</span>
<span id="S4.T5.3.1.1.5.5.5" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:1.5pt;padding-bottom:1.5pt;">55.9</span>
<span id="S4.T5.3.1.1.5.5.6" class="ltx_td ltx_align_center ltx_border_rr" style="padding-top:1.5pt;padding-bottom:1.5pt;">53.3</span>
<span id="S4.T5.3.1.1.5.5.7" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:1.5pt;padding-bottom:1.5pt;">3</span></span>
<span id="S4.T5.3.1.1.6.6" class="ltx_tr">
<span id="S4.T5.3.1.1.6.6.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_rr ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">1 epoch CBOW</span>
<span id="S4.T5.3.1.1.6.6.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">300</span>
<span id="S4.T5.3.1.1.6.6.3" class="ltx_td ltx_align_center ltx_border_rr ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">783M</span>
<span id="S4.T5.3.1.1.6.6.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">13.8</span>
<span id="S4.T5.3.1.1.6.6.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">49.9</span>
<span id="S4.T5.3.1.1.6.6.6" class="ltx_td ltx_align_center ltx_border_rr ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">33.6</span>
<span id="S4.T5.3.1.1.6.6.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">0.3</span></span>
<span id="S4.T5.3.1.1.7.7" class="ltx_tr">
<span id="S4.T5.3.1.1.7.7.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_rr" style="padding-top:1.5pt;padding-bottom:1.5pt;">1 epoch CBOW</span>
<span id="S4.T5.3.1.1.7.7.2" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:1.5pt;padding-bottom:1.5pt;">300</span>
<span id="S4.T5.3.1.1.7.7.3" class="ltx_td ltx_align_center ltx_border_rr" style="padding-top:1.5pt;padding-bottom:1.5pt;">1.6B</span>
<span id="S4.T5.3.1.1.7.7.4" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:1.5pt;padding-bottom:1.5pt;">16.1</span>
<span id="S4.T5.3.1.1.7.7.5" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:1.5pt;padding-bottom:1.5pt;">52.6</span>
<span id="S4.T5.3.1.1.7.7.6" class="ltx_td ltx_align_center ltx_border_rr" style="padding-top:1.5pt;padding-bottom:1.5pt;">36.1</span>
<span id="S4.T5.3.1.1.7.7.7" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:1.5pt;padding-bottom:1.5pt;">0.6</span></span>
<span id="S4.T5.3.1.1.8.8" class="ltx_tr">
<span id="S4.T5.3.1.1.8.8.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_rr" style="padding-top:1.5pt;padding-bottom:1.5pt;">1 epoch CBOW</span>
<span id="S4.T5.3.1.1.8.8.2" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:1.5pt;padding-bottom:1.5pt;">600</span>
<span id="S4.T5.3.1.1.8.8.3" class="ltx_td ltx_align_center ltx_border_rr" style="padding-top:1.5pt;padding-bottom:1.5pt;">783M</span>
<span id="S4.T5.3.1.1.8.8.4" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:1.5pt;padding-bottom:1.5pt;">15.4</span>
<span id="S4.T5.3.1.1.8.8.5" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:1.5pt;padding-bottom:1.5pt;">53.3</span>
<span id="S4.T5.3.1.1.8.8.6" class="ltx_td ltx_align_center ltx_border_rr" style="padding-top:1.5pt;padding-bottom:1.5pt;">36.2</span>
<span id="S4.T5.3.1.1.8.8.7" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:1.5pt;padding-bottom:1.5pt;">0.7</span></span>
<span id="S4.T5.3.1.1.9.9" class="ltx_tr">
<span id="S4.T5.3.1.1.9.9.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_rr ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">1 epoch Skip-gram</span>
<span id="S4.T5.3.1.1.9.9.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">300</span>
<span id="S4.T5.3.1.1.9.9.3" class="ltx_td ltx_align_center ltx_border_rr ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">783M</span>
<span id="S4.T5.3.1.1.9.9.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">45.6</span>
<span id="S4.T5.3.1.1.9.9.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">52.2</span>
<span id="S4.T5.3.1.1.9.9.6" class="ltx_td ltx_align_center ltx_border_rr ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">49.2</span>
<span id="S4.T5.3.1.1.9.9.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">1</span></span>
<span id="S4.T5.3.1.1.10.10" class="ltx_tr">
<span id="S4.T5.3.1.1.10.10.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_rr" style="padding-top:1.5pt;padding-bottom:1.5pt;">1 epoch Skip-gram</span>
<span id="S4.T5.3.1.1.10.10.2" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:1.5pt;padding-bottom:1.5pt;">300</span>
<span id="S4.T5.3.1.1.10.10.3" class="ltx_td ltx_align_center ltx_border_rr" style="padding-top:1.5pt;padding-bottom:1.5pt;">1.6B</span>
<span id="S4.T5.3.1.1.10.10.4" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:1.5pt;padding-bottom:1.5pt;">52.2</span>
<span id="S4.T5.3.1.1.10.10.5" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:1.5pt;padding-bottom:1.5pt;">55.1</span>
<span id="S4.T5.3.1.1.10.10.6" class="ltx_td ltx_align_center ltx_border_rr" style="padding-top:1.5pt;padding-bottom:1.5pt;">53.8</span>
<span id="S4.T5.3.1.1.10.10.7" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:1.5pt;padding-bottom:1.5pt;">2</span></span>
<span id="S4.T5.3.1.1.11.11" class="ltx_tr">
<span id="S4.T5.3.1.1.11.11.1" class="ltx_td ltx_align_left ltx_border_b ltx_border_l ltx_border_rr" style="padding-top:1.5pt;padding-bottom:1.5pt;">1 epoch Skip-gram</span>
<span id="S4.T5.3.1.1.11.11.2" class="ltx_td ltx_align_center ltx_border_b ltx_border_r" style="padding-top:1.5pt;padding-bottom:1.5pt;">600</span>
<span id="S4.T5.3.1.1.11.11.3" class="ltx_td ltx_align_center ltx_border_b ltx_border_rr" style="padding-top:1.5pt;padding-bottom:1.5pt;">783M</span>
<span id="S4.T5.3.1.1.11.11.4" class="ltx_td ltx_align_center ltx_border_b ltx_border_r" style="padding-top:1.5pt;padding-bottom:1.5pt;">56.7</span>
<span id="S4.T5.3.1.1.11.11.5" class="ltx_td ltx_align_center ltx_border_b ltx_border_r" style="padding-top:1.5pt;padding-bottom:1.5pt;">54.5</span>
<span id="S4.T5.3.1.1.11.11.6" class="ltx_td ltx_align_center ltx_border_b ltx_border_rr" style="padding-top:1.5pt;padding-bottom:1.5pt;">55.5</span>
<span id="S4.T5.3.1.1.11.11.7" class="ltx_td ltx_align_center ltx_border_b ltx_border_r" style="padding-top:1.5pt;padding-bottom:1.5pt;">2.5</span></span>
</span>
</span></span></p>
</figure>
</section>
<section id="S4.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.4 </span>Large Scale Parallel Training of Models</h3>

<div id="S4.SS4.p1" class="ltx_para ltx_noindent">
<p id="S4.SS4.p1.1" class="ltx_p">As mentioned earlier, we have implemented various models in a
distributed framework called DistBelief. Below we report the
results of several models trained on the Google News 6B data set, with
mini-batch asynchronous gradient descent and the adaptive
learning rate procedure called AdagradÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite>. We used 50 to 100 model
replicas during the training. The number
of CPU cores is an estimate since the data center machines
are shared with other production tasks, and the usage can fluctuate
quite a bit. Note that due to the overhead of the distributed
framework, the CPU usage of the CBOW model and the Skip-gram model are
much closer to each other than their single-machine
implementations. The result are reported in TableÂ <a href="#S4.T6" title="Table 6 â€£ 4.4 Large Scale Parallel Training of Models â€£ 4 Results â€£ Efficient Estimation of Word Representations in Vector Space" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a>.</p>
</div>
<figure id="S4.T6" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 6: </span> <span id="S4.T6.2.1" class="ltx_text ltx_font_italic">Comparison of models trained using the DistBelief
distributed framework.
Note that training of NNLM with 1000-dimensional vectors would take
too long to complete.</span></figcaption>
<p id="S4.T6.3" class="ltx_p ltx_align_center"><span id="S4.T6.3.1" class="ltx_text">

<span id="S4.T6.3.1.1" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<span class="ltx_thead">
<span id="S4.T6.3.1.1.1.1" class="ltx_tr">
<span id="S4.T6.3.1.1.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_l ltx_border_rr ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">Model</span>
<span id="S4.T6.3.1.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">Vector</span>
<span id="S4.T6.3.1.1.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_rr ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">Training</span>
<span id="S4.T6.3.1.1.1.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_rr ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;" colspan="3">Accuracy [%]</span>
<span id="S4.T6.3.1.1.1.1.5" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">Training time</span></span>
<span id="S4.T6.3.1.1.2.2" class="ltx_tr">
<span id="S4.T6.3.1.1.2.2.1" class="ltx_td ltx_th ltx_th_column ltx_border_l ltx_border_rr" style="padding-top:1.5pt;padding-bottom:1.5pt;"></span>
<span id="S4.T6.3.1.1.2.2.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r" style="padding-top:1.5pt;padding-bottom:1.5pt;">Dimensionality</span>
<span id="S4.T6.3.1.1.2.2.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_rr" style="padding-top:1.5pt;padding-bottom:1.5pt;">words</span>
<span id="S4.T6.3.1.1.2.2.4" class="ltx_td ltx_th ltx_th_column ltx_border_rr" style="padding-top:1.5pt;padding-bottom:1.5pt;" colspan="3"></span>
<span id="S4.T6.3.1.1.2.2.5" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r" style="padding-top:1.5pt;padding-bottom:1.5pt;">[days x CPU cores]</span></span>
<span id="S4.T6.3.1.1.3.3" class="ltx_tr">
<span id="S4.T6.3.1.1.3.3.1" class="ltx_td ltx_th ltx_th_column ltx_border_l ltx_border_rr ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;"></span>
<span id="S4.T6.3.1.1.3.3.2" class="ltx_td ltx_th ltx_th_column ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;"></span>
<span id="S4.T6.3.1.1.3.3.3" class="ltx_td ltx_th ltx_th_column ltx_border_rr ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;"></span>
<span id="S4.T6.3.1.1.3.3.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">Semantic</span>
<span id="S4.T6.3.1.1.3.3.5" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">Syntactic</span>
<span id="S4.T6.3.1.1.3.3.6" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_rr ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">Total</span>
<span id="S4.T6.3.1.1.3.3.7" class="ltx_td ltx_th ltx_th_column ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;"></span></span>
</span>
<span class="ltx_tbody">
<span id="S4.T6.3.1.1.4.1" class="ltx_tr">
<span id="S4.T6.3.1.1.4.1.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_rr ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">NNLM</span>
<span id="S4.T6.3.1.1.4.1.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">100</span>
<span id="S4.T6.3.1.1.4.1.3" class="ltx_td ltx_align_center ltx_border_rr ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">6B</span>
<span id="S4.T6.3.1.1.4.1.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">34.2</span>
<span id="S4.T6.3.1.1.4.1.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">64.5</span>
<span id="S4.T6.3.1.1.4.1.6" class="ltx_td ltx_align_center ltx_border_rr ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">50.8</span>
<span id="S4.T6.3.1.1.4.1.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">14 x 180</span></span>
<span id="S4.T6.3.1.1.5.2" class="ltx_tr">
<span id="S4.T6.3.1.1.5.2.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_rr" style="padding-top:1.5pt;padding-bottom:1.5pt;">CBOW</span>
<span id="S4.T6.3.1.1.5.2.2" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:1.5pt;padding-bottom:1.5pt;">1000</span>
<span id="S4.T6.3.1.1.5.2.3" class="ltx_td ltx_align_center ltx_border_rr" style="padding-top:1.5pt;padding-bottom:1.5pt;">6B</span>
<span id="S4.T6.3.1.1.5.2.4" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:1.5pt;padding-bottom:1.5pt;">57.3</span>
<span id="S4.T6.3.1.1.5.2.5" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:1.5pt;padding-bottom:1.5pt;">68.9</span>
<span id="S4.T6.3.1.1.5.2.6" class="ltx_td ltx_align_center ltx_border_rr" style="padding-top:1.5pt;padding-bottom:1.5pt;">63.7</span>
<span id="S4.T6.3.1.1.5.2.7" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:1.5pt;padding-bottom:1.5pt;">2 x 140</span></span>
<span id="S4.T6.3.1.1.6.3" class="ltx_tr">
<span id="S4.T6.3.1.1.6.3.1" class="ltx_td ltx_align_left ltx_border_b ltx_border_l ltx_border_rr" style="padding-top:1.5pt;padding-bottom:1.5pt;">Skip-gram</span>
<span id="S4.T6.3.1.1.6.3.2" class="ltx_td ltx_align_center ltx_border_b ltx_border_r" style="padding-top:1.5pt;padding-bottom:1.5pt;">1000</span>
<span id="S4.T6.3.1.1.6.3.3" class="ltx_td ltx_align_center ltx_border_b ltx_border_rr" style="padding-top:1.5pt;padding-bottom:1.5pt;">6B</span>
<span id="S4.T6.3.1.1.6.3.4" class="ltx_td ltx_align_center ltx_border_b ltx_border_r" style="padding-top:1.5pt;padding-bottom:1.5pt;">66.1</span>
<span id="S4.T6.3.1.1.6.3.5" class="ltx_td ltx_align_center ltx_border_b ltx_border_r" style="padding-top:1.5pt;padding-bottom:1.5pt;">65.1</span>
<span id="S4.T6.3.1.1.6.3.6" class="ltx_td ltx_align_center ltx_border_b ltx_border_rr" style="padding-top:1.5pt;padding-bottom:1.5pt;">65.6</span>
<span id="S4.T6.3.1.1.6.3.7" class="ltx_td ltx_align_center ltx_border_b ltx_border_r" style="padding-top:1.5pt;padding-bottom:1.5pt;">2.5 x 125</span></span>
</span>
</span></span></p>
</figure>
</section>
<section id="S4.SS5" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.5 </span>Microsoft Research Sentence Completion Challenge</h3>

<div id="S4.SS5.p1" class="ltx_para ltx_noindent">
<p id="S4.SS5.p1.1" class="ltx_p">The Microsoft Sentence Completion Challenge has been recently introduced as a task for advancing language modeling and other NLP techniquesÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib32" title="" class="ltx_ref">32</a>]</cite>. This task consists of 1040 sentences, where
one word is missing in each sentence and the goal is to select word that is the most coherent with the rest of the sentence, given a list of five reasonable choices. Performance of several techniques has been already
reported on this set, including N-gram models, LSA-based modelÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib32" title="" class="ltx_ref">32</a>]</cite>, log-bilinear modelÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib24" title="" class="ltx_ref">24</a>]</cite> and a combination of recurrent neural networks that currently holds the state of the art
performance of 55.4% accuracy on this benchmarkÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib19" title="" class="ltx_ref">19</a>]</cite>.</p>
</div>
<div id="S4.SS5.p2" class="ltx_para ltx_noindent">
<p id="S4.SS5.p2.1" class="ltx_p">We have explored the performance of Skip-gram architecture on this task. First, we train the 640-dimensional model on 50M words provided inÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib32" title="" class="ltx_ref">32</a>]</cite>. Then, we compute score of each sentence in the test set by
using the unknown word at the input, and predict all surrounding words in a sentence. The final sentence score is then the sum of these individual predictions. Using the sentence scores, we choose the most likely
sentence.</p>
</div>
<div id="S4.SS5.p3" class="ltx_para ltx_noindent">
<p id="S4.SS5.p3.1" class="ltx_p">A short summary of some previous results together with the new results is presented in TableÂ <a href="#S4.T7" title="Table 7 â€£ 4.5 Microsoft Research Sentence Completion Challenge â€£ 4 Results â€£ Efficient Estimation of Word Representations in Vector Space" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7</span></a>. While the Skip-gram model itself does not perform on this task better than LSA similarity,
the scores from this model are complementary to scores obtained with RNNLMs, and a weighted combination leads to a new state of the art result 58.9% accuracy (59.2% on the development part
of the set and 58.7% on the test part of the set).</p>
</div>
<figure id="S4.T7" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 7: </span><span id="S4.T7.2.1" class="ltx_text ltx_font_italic">Comparison and combination of models on the Microsoft Sentence Completion Challenge.</span></figcaption>
<p id="S4.T7.3" class="ltx_p ltx_align_center"><span id="S4.T7.3.1" class="ltx_text">

<span id="S4.T7.3.1.1" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<span class="ltx_thead">
<span id="S4.T7.3.1.1.1.1" class="ltx_tr">
<span id="S4.T7.3.1.1.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_l ltx_border_rr ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">Architecture</span>
<span id="S4.T7.3.1.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">Accuracy [%]</span></span>
</span>
<span class="ltx_tbody">
<span id="S4.T7.3.1.1.2.1" class="ltx_tr">
<span id="S4.T7.3.1.1.2.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_rr ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">4-gramÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib32" title="" class="ltx_ref">32</a>]</cite></span>
<span id="S4.T7.3.1.1.2.1.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">39</span></span>
<span id="S4.T7.3.1.1.3.2" class="ltx_tr">
<span id="S4.T7.3.1.1.3.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_rr" style="padding-top:1.5pt;padding-bottom:1.5pt;">Average LSA similarityÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib32" title="" class="ltx_ref">32</a>]</cite></span>
<span id="S4.T7.3.1.1.3.2.2" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:1.5pt;padding-bottom:1.5pt;">49</span></span>
<span id="S4.T7.3.1.1.4.3" class="ltx_tr">
<span id="S4.T7.3.1.1.4.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_rr" style="padding-top:1.5pt;padding-bottom:1.5pt;">Log-bilinear modelÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib24" title="" class="ltx_ref">24</a>]</cite></span>
<span id="S4.T7.3.1.1.4.3.2" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:1.5pt;padding-bottom:1.5pt;">54.8</span></span>
<span id="S4.T7.3.1.1.5.4" class="ltx_tr">
<span id="S4.T7.3.1.1.5.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_rr" style="padding-top:1.5pt;padding-bottom:1.5pt;">RNNLMsÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib19" title="" class="ltx_ref">19</a>]</cite></span>
<span id="S4.T7.3.1.1.5.4.2" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:1.5pt;padding-bottom:1.5pt;">55.4</span></span>
<span id="S4.T7.3.1.1.6.5" class="ltx_tr">
<span id="S4.T7.3.1.1.6.5.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_rr ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">Skip-gram</span>
<span id="S4.T7.3.1.1.6.5.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">48.0</span></span>
<span id="S4.T7.3.1.1.7.6" class="ltx_tr">
<span id="S4.T7.3.1.1.7.6.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b ltx_border_l ltx_border_rr" style="padding-top:1.5pt;padding-bottom:1.5pt;">Skip-gram + RNNLMs</span>
<span id="S4.T7.3.1.1.7.6.2" class="ltx_td ltx_align_center ltx_border_b ltx_border_r" style="padding-top:1.5pt;padding-bottom:1.5pt;"><span id="S4.T7.3.1.1.7.6.2.1" class="ltx_text ltx_font_bold">58.9</span></span></span>
</span>
</span></span></p>
</figure>
</section>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Examples of the Learned Relationships</h2>

<div id="S5.p1" class="ltx_para ltx_noindent">
<p id="S5.p1.1" class="ltx_p">TableÂ <a href="#S5.T8" title="Table 8 â€£ 5 Examples of the Learned Relationships â€£ Efficient Estimation of Word Representations in Vector Space" class="ltx_ref"><span class="ltx_text ltx_ref_tag">8</span></a> shows words that follow various relationships. We follow the approach described above: the relationship
is defined by subtracting two word vectors, and the result is added to another word. Thus for example, <span id="S5.p1.1.1" class="ltx_text ltx_font_italic">Paris - France + Italy = Rome</span>.
As it can be seen, accuracy is quite good, although there is clearly a lot of room for further improvements (note that using our accuracy metric that assumes exact match, the results in TableÂ <a href="#S5.T8" title="Table 8 â€£ 5 Examples of the Learned Relationships â€£ Efficient Estimation of Word Representations in Vector Space" class="ltx_ref"><span class="ltx_text ltx_ref_tag">8</span></a> would score only about 60%).
We believe that word vectors trained on even larger data sets with larger dimensionality will perform significantly better,
and will enable the development of new innovative applications.
Another way to improve accuracy is to provide more than one example of the relationship. By using ten examples instead of one to form the relationship vector (we average the individual vectors together),
we have observed improvement of accuracy of our best models by about 10% absolutely on the semantic-syntactic test.</p>
</div>
<div id="S5.p2" class="ltx_para ltx_noindent">
<p id="S5.p2.1" class="ltx_p">It is also possible to apply the vector operations to solve different tasks. For example, we have observed
good accuracy for selecting out-of-the-list words, by computing average vector for a list of words, and finding the most distant word vector.
This is a popular type of problems in certain human intelligence tests. Clearly, there is still a lot of discoveries to be made using these techniques.</p>
</div>
<figure id="S5.T8" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 8: </span> <span id="S5.T8.2.1" class="ltx_text ltx_font_italic">Examples of the word pair relationships, using the best word vectors from TableÂ <a href="#S4.T4" title="Table 4 â€£ 4.3 Comparison of Model Architectures â€£ 4 Results â€£ Efficient Estimation of Word Representations in Vector Space" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a> (Skip-gram model trained on 783M words with 300 dimensionality).</span></figcaption>
<p id="S5.T8.3" class="ltx_p ltx_align_center"><span id="S5.T8.3.1" class="ltx_text">

<span id="S5.T8.3.1.1" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<span class="ltx_thead">
<span id="S5.T8.3.1.1.1.1" class="ltx_tr">
<span id="S5.T8.3.1.1.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_l ltx_border_rr ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">Relationship</span>
<span id="S5.T8.3.1.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">Example 1</span>
<span id="S5.T8.3.1.1.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">Example 2</span>
<span id="S5.T8.3.1.1.1.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">Example 3</span></span>
</span>
<span class="ltx_tbody">
<span id="S5.T8.3.1.1.2.1" class="ltx_tr">
<span id="S5.T8.3.1.1.2.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_rr ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">France - Paris</span>
<span id="S5.T8.3.1.1.2.1.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">Italy: Rome</span>
<span id="S5.T8.3.1.1.2.1.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">Japan: Tokyo</span>
<span id="S5.T8.3.1.1.2.1.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">Florida: Tallahassee</span></span>
<span id="S5.T8.3.1.1.3.2" class="ltx_tr">
<span id="S5.T8.3.1.1.3.2.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_rr" style="padding-top:1.5pt;padding-bottom:1.5pt;">big - bigger</span>
<span id="S5.T8.3.1.1.3.2.2" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:1.5pt;padding-bottom:1.5pt;">small: larger</span>
<span id="S5.T8.3.1.1.3.2.3" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:1.5pt;padding-bottom:1.5pt;">cold: colder</span>
<span id="S5.T8.3.1.1.3.2.4" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:1.5pt;padding-bottom:1.5pt;">quick: quicker</span></span>
<span id="S5.T8.3.1.1.4.3" class="ltx_tr">
<span id="S5.T8.3.1.1.4.3.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_rr" style="padding-top:1.5pt;padding-bottom:1.5pt;">Miami - Florida</span>
<span id="S5.T8.3.1.1.4.3.2" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:1.5pt;padding-bottom:1.5pt;">Baltimore: Maryland</span>
<span id="S5.T8.3.1.1.4.3.3" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:1.5pt;padding-bottom:1.5pt;">Dallas: Texas</span>
<span id="S5.T8.3.1.1.4.3.4" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:1.5pt;padding-bottom:1.5pt;">Kona: Hawaii</span></span>
<span id="S5.T8.3.1.1.5.4" class="ltx_tr">
<span id="S5.T8.3.1.1.5.4.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_rr" style="padding-top:1.5pt;padding-bottom:1.5pt;">Einstein - scientist</span>
<span id="S5.T8.3.1.1.5.4.2" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:1.5pt;padding-bottom:1.5pt;">Messi: midfielder</span>
<span id="S5.T8.3.1.1.5.4.3" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:1.5pt;padding-bottom:1.5pt;">Mozart: violinist</span>
<span id="S5.T8.3.1.1.5.4.4" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:1.5pt;padding-bottom:1.5pt;">Picasso: painter</span></span>
<span id="S5.T8.3.1.1.6.5" class="ltx_tr">
<span id="S5.T8.3.1.1.6.5.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_rr" style="padding-top:1.5pt;padding-bottom:1.5pt;">Sarkozy - France</span>
<span id="S5.T8.3.1.1.6.5.2" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:1.5pt;padding-bottom:1.5pt;">Berlusconi: Italy</span>
<span id="S5.T8.3.1.1.6.5.3" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:1.5pt;padding-bottom:1.5pt;">Merkel: Germany</span>
<span id="S5.T8.3.1.1.6.5.4" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:1.5pt;padding-bottom:1.5pt;">Koizumi: Japan</span></span>
<span id="S5.T8.3.1.1.7.6" class="ltx_tr">
<span id="S5.T8.3.1.1.7.6.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_rr" style="padding-top:1.5pt;padding-bottom:1.5pt;">copper - Cu</span>
<span id="S5.T8.3.1.1.7.6.2" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:1.5pt;padding-bottom:1.5pt;">zinc: Zn</span>
<span id="S5.T8.3.1.1.7.6.3" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:1.5pt;padding-bottom:1.5pt;">gold: Au</span>
<span id="S5.T8.3.1.1.7.6.4" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:1.5pt;padding-bottom:1.5pt;">uranium: plutonium</span></span>
<span id="S5.T8.3.1.1.8.7" class="ltx_tr">
<span id="S5.T8.3.1.1.8.7.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_rr" style="padding-top:1.5pt;padding-bottom:1.5pt;">Berlusconi - Silvio</span>
<span id="S5.T8.3.1.1.8.7.2" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:1.5pt;padding-bottom:1.5pt;">Sarkozy: Nicolas</span>
<span id="S5.T8.3.1.1.8.7.3" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:1.5pt;padding-bottom:1.5pt;">Putin: Medvedev</span>
<span id="S5.T8.3.1.1.8.7.4" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:1.5pt;padding-bottom:1.5pt;">Obama: Barack</span></span>
<span id="S5.T8.3.1.1.9.8" class="ltx_tr">
<span id="S5.T8.3.1.1.9.8.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_rr" style="padding-top:1.5pt;padding-bottom:1.5pt;">Microsoft - Windows</span>
<span id="S5.T8.3.1.1.9.8.2" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:1.5pt;padding-bottom:1.5pt;">Google: Android</span>
<span id="S5.T8.3.1.1.9.8.3" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:1.5pt;padding-bottom:1.5pt;">IBM: Linux</span>
<span id="S5.T8.3.1.1.9.8.4" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:1.5pt;padding-bottom:1.5pt;">Apple: iPhone</span></span>
<span id="S5.T8.3.1.1.10.9" class="ltx_tr">
<span id="S5.T8.3.1.1.10.9.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_rr" style="padding-top:1.5pt;padding-bottom:1.5pt;">Microsoft - Ballmer</span>
<span id="S5.T8.3.1.1.10.9.2" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:1.5pt;padding-bottom:1.5pt;">Google: Yahoo</span>
<span id="S5.T8.3.1.1.10.9.3" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:1.5pt;padding-bottom:1.5pt;">IBM: McNealy</span>
<span id="S5.T8.3.1.1.10.9.4" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:1.5pt;padding-bottom:1.5pt;">Apple: Jobs</span></span>
<span id="S5.T8.3.1.1.11.10" class="ltx_tr">
<span id="S5.T8.3.1.1.11.10.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_b ltx_border_l ltx_border_rr" style="padding-top:1.5pt;padding-bottom:1.5pt;">Japan - sushi</span>
<span id="S5.T8.3.1.1.11.10.2" class="ltx_td ltx_align_center ltx_border_b ltx_border_r" style="padding-top:1.5pt;padding-bottom:1.5pt;">Germany: bratwurst</span>
<span id="S5.T8.3.1.1.11.10.3" class="ltx_td ltx_align_center ltx_border_b ltx_border_r" style="padding-top:1.5pt;padding-bottom:1.5pt;">France: tapas</span>
<span id="S5.T8.3.1.1.11.10.4" class="ltx_td ltx_align_center ltx_border_b ltx_border_r" style="padding-top:1.5pt;padding-bottom:1.5pt;">USA: pizza</span></span>
</span>
</span></span></p>
</figure>
</section>
<section id="S6" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6 </span>Conclusion</h2>

<div id="S6.p1" class="ltx_para ltx_noindent">
<p id="S6.p1.1" class="ltx_p">In this paper we studied the quality of vector representations of words derived by various models on a collection of syntactic and semantic language tasks.
We observed that it is possible to train high quality
word vectors using very simple model architectures, compared to the popular neural network models (both feedforward and recurrent). Because of the much lower computational complexity,
it is possible to compute very accurate high dimensional word vectors from a much larger data set. Using the DistBelief distributed framework, it should be possible to train
the CBOW and Skip-gram models even on corpora with one trillion words, for basically unlimited size of the vocabulary. That is several orders of magnitude larger than the best previously published results for similar models.</p>
</div>
<div id="S6.p2" class="ltx_para ltx_noindent">
<p id="S6.p2.1" class="ltx_p">An interesting task where the word vectors have recently been shown to significantly outperform the previous state of the art is the SemEval-2012 Task 2Â <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite>. The publicly available
RNN vectors were used together with other techniques to achieve over 50% increase in Spearmanâ€™s rank correlation over the previous best resultÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib31" title="" class="ltx_ref">31</a>]</cite>.
The neural network based word vectors were previously applied to many other NLP tasks, for example sentiment analysisÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>]</cite> and paraphrase detectionÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib28" title="" class="ltx_ref">28</a>]</cite>. It can be expected
that these applications can benefit from the model architectures described in this paper.</p>
</div>
<div id="S6.p3" class="ltx_para ltx_noindent">
<p id="S6.p3.1" class="ltx_p">Our ongoing work shows that the word vectors can be successfully applied to automatic extension of facts in Knowledge Bases, and also for verification of correctness of existing facts. Results from machine
translation experiments also look very promising.
In the future, it would be also interesting to compare our techniques to
Latent Relational AnalysisÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib30" title="" class="ltx_ref">30</a>]</cite> and others.
We believe that our comprehensive test set will help the research community to improve the existing techniques for estimating the word vectors.
We also expect that high quality word vectors will become an important building block for future NLP applications.</p>
</div>
</section>
<section id="S7" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">7 </span>Follow-Up Work</h2>

<div id="S7.p1" class="ltx_para ltx_noindent">
<p id="S7.p1.1" class="ltx_p">After the initial version of this paper was written, we published
single-machine multi-threaded C++ code for computing the word vectors,
using both the continuous bag-of-words and
skip-gram architectures<span id="footnote4" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">4</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">4</sup><span class="ltx_tag ltx_tag_note">4</span>The code
is available at <a target="_blank" href="https://code.google.com/p/word2vec/" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://code.google.com/p/word2vec/</a></span></span></span>.
The training speed is significantly higher
than reported earlier in this paper, i.e. it is in the order of
billions of words per hour for typical
hyperparameter choices. We also published more than 1.4 million
vectors that represent named entities, trained on more than 100
billion words.
Some of our follow-up work will be published in an upcoming NIPS 2013
paperÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib21" title="" class="ltx_ref">21</a>]</cite>.</p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock"> Y. Bengio, R. Ducharme, P.
Vincent. A neural probabilistic language model. Journal of
Machine Learning Research, 3:1137-1155, 2003.

</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock"> Y. Bengio, Y. LeCun. Scaling learning algorithms towards
AI. In: Large-Scale Kernel Machines, MIT Press, 2007.

</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock"> T. Brants, A. C. Popat, P. Xu, F. J. Och, and J. Dean.
Large language models in machine translation. In
Proceedings of the Joint Conference on Empirical
Methods in Natural Language Processing and
Computational Language Learning, 2007.

</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock"> R. Collobert and J. Weston.
A Unified Architecture for Natural Language Processing: Deep Neural Networks with Multitask Learning.
In International Conference on Machine Learning, ICML, 2008.

</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock"> R. Collobert, J. Weston, L. Bottou, M. Karlen, K. Kavukcuoglu and P. Kuksa.
Natural Language Processing (Almost) from Scratch. Journal of Machine Learning Research, 12:2493-2537, 2011.

</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock"> J. Dean, G.S. Corrado, R. Monga, K. Chen, M.
Devin, Q.V. Le, M.Z. Mao, M.A. Ranzato, A. Senior, P. Tucker, K. Yang,
A. Y. Ng., Large Scale Distributed Deep Networks, NIPS, 2012.

</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock"> J.C. Duchi, E. Hazan, and Y. Singer. Adaptive
subgradient methods for online learning and stochastic optimization. Journal of Machine Learning Research, 2011.

</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock"> J. Elman. Finding Structure in Time. Cognitive Science, 14, 179-211, 1990.

</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock"> Eric H. Huang, R. Socher, C. D. Manning and Andrew Y. Ng. Improving Word Representations via Global Context and Multiple Word Prototypes.
In: Proc. Association for Computational Linguistics, 2012.

</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock"> G.E. Hinton, J.L. McClelland, D.E. Rumelhart. Distributed representations. In: Parallel distributed processing: Explorations in the microstructure of cognition. Volume 1: Foundations, MIT Press, 1986.

</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock"> D.A. Jurgens, S.M. Mohammad, P.D. Turney, K.J. Holyoak. Semeval-2012 task 2: Measuring degrees of relational similarity. In: Proceedings of the 6th International Workshop on Semantic Evaluation (SemEval 2012), 2012.

</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock"> A.L. Maas, R.E. Daly, P.T. Pham, D. Huang, A.Y. Ng, and C. Potts. Learning word vectors for sentiment analysis. In Proceedings of ACL, 2011.

</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock"> T. Mikolov. Language Modeling for Speech Recognition in Czech, Masters thesis, Brno University of Technology, 2007.

</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock"> T. Mikolov, J. KopeckÃ½, L. Burget, O. Glembek and J. ÄŒernockÃ½. Neural
network based language models for higly inflective
languages, In: Proc. ICASSP 2009.

</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock"> T. Mikolov, M. KarafiÃ¡t, L. Burget, J. ÄŒernockÃ½, S. Khudanpur. Recurrent
neural network based language model, In: Proceedings of Interspeech, 2010.

</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock"> T. Mikolov, S. Kombrink, L. Burget, J. ÄŒernockÃ½, S. Khudanpur. Extensions
of recurrent neural network language model, In: Proceedings of ICASSP 2011.

</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock"> T. Mikolov, A. Deoras, S. Kombrink, L. Burget, J. ÄŒernockÃ½.
Empirical Evaluation and Combination of Advanced Language Modeling Techniques, In: Proceedings of Interspeech, 2011.

</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[18]</span>
<span class="ltx_bibblock"> T. Mikolov, A. Deoras, D. Povey, L. Burget, J. ÄŒernockÃ½. Strategies for Training Large Scale Neural Network Language Models,
In: Proc. Automatic Speech Recognition and Understanding, 2011.

</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[19]</span>
<span class="ltx_bibblock"> T. Mikolov. Statistical Language Models based on Neural Networks. PhD thesis, Brno University of Technology, 2012.

</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[20]</span>
<span class="ltx_bibblock"> T. Mikolov, W.T. Yih, G. Zweig. Linguistic Regularities in Continuous Space Word Representations.
NAACL HLT 2013.

</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[21]</span>
<span class="ltx_bibblock"> T. Mikolov, I. Sutskever, K. Chen, G. Corrado, and J. Dean. Distributed Representations of Words and Phrases and their Compositionality. Accepted to NIPS 2013.

</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[22]</span>
<span class="ltx_bibblock"> A. Mnih, G. Hinton. Three new graphical models for statistical language modelling. ICML, 2007.

</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[23]</span>
<span class="ltx_bibblock"> A. Mnih, G. Hinton. A Scalable Hierarchical Distributed Language Model. Advances in Neural Information Processing Systems 21, MIT Press, 2009.

</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[24]</span>
<span class="ltx_bibblock"> A. Mnih, Y.W. Teh. A fast and simple algorithm for training neural probabilistic language models. ICML, 2012.

</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[25]</span>
<span class="ltx_bibblock"> F. Morin, Y. Bengio. Hierarchical Probabilistic Neural Network Language Model. AISTATS, 2005.

</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[26]</span>
<span class="ltx_bibblock"> D. E. Rumelhart, G. E. Hinton, R. J. Williams. Learning internal representations
by back-propagating errors. Nature, 323:533.536, 1986.

</span>
</li>
<li id="bib.bib27" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[27]</span>
<span class="ltx_bibblock"> H. Schwenk. Continuous space language models. Computer Speech and Language, vol. 21, 2007.

</span>
</li>
<li id="bib.bib28" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[28]</span>
<span class="ltx_bibblock"> R. Socher, E.H. Huang, J. Pennington, A.Y. Ng, and C.D. Manning. Dynamic Pooling and Unfolding Recursive Autoencoders for Paraphrase Detection. In NIPS, 2011.

</span>
</li>
<li id="bib.bib29" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[29]</span>
<span class="ltx_bibblock"> J. Turian, L. Ratinov, Y. Bengio.
Word Representations: A Simple and General Method for Semi-Supervised Learning.
In: Proc. Association for Computational Linguistics, 2010.

</span>
</li>
<li id="bib.bib30" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[30]</span>
<span class="ltx_bibblock"> P. D. Turney. Measuring Semantic Similarity by Latent Relational Analysis. In: Proc. International Joint Conference on Artificial Intelligence, 2005.

</span>
</li>
<li id="bib.bib31" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[31]</span>
<span class="ltx_bibblock"> A. Zhila, W.T. Yih, C. Meek, G. Zweig, T. Mikolov. Combining Heterogeneous Models for Measuring Relational Similarity.
NAACL HLT 2013.

</span>
</li>
<li id="bib.bib32" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[32]</span>
<span class="ltx_bibblock"> G. Zweig, C.J.C. Burges. The Microsoft Research Sentence Completion Challenge, Microsoft Research Technical Report MSR-TR-2011-129, 2011.

</span>
</li>
</ul>
</section>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/1301.3780" class="ar5iv-nav-button ar5iv-nav-button-prev">â—„</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/1301.3781" class="ar5iv-text-button ar5iv-severity-error">Conversion<br>report</a>
    <a class="ar5iv-text-button" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+1301.3781">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/1301.3781" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/1301.3782" class="ar5iv-nav-button ar5iv-nav-button-next">â–º</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Fri Jan 28 20:46:44 2022 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/">LaTeXML <img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="[LOGO]"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }      
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })
      
      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "Ã—";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
